[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "deployment-strategy.html",
    "href": "deployment-strategy.html",
    "title": "Outlining the deployment strategy",
    "section": "",
    "text": "Deployment managers support the automated application rollout of the application binaries and of the application configurations across platforms (that is, managing distributed and mainframe systems). Typically, a deployment manager would perform the following tasks when deploying a package into an environment:\n\nVerify that the package is applicable to the environment\nSort and order the artifacts to process\nFor each artifact of a given type, run the appropriate process\nPerform post deployment activities\n\nApproval processes may be embedded. This approach will likely be necessary due to audit and legal requirements.\nDeployment managers are aware of the execution environments. They generally also provide an inventory of the packages and of the current deployed binaries in a given environment. The deployment process can be implemented with scripts or with other more sophisticated techniques. These more sophisticated processes manage return codes and facilitate the use of the APIs of the different middleware systems. However, in both cases, and in order to install the package, the deployment process will consume the manifest file, which contains metadata about contents of the package.\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Outlining the deployment strategy"
    ]
  },
  {
    "objectID": "deployment-strategy.html#introducing-deployment-managers",
    "href": "deployment-strategy.html#introducing-deployment-managers",
    "title": "Outlining the deployment strategy",
    "section": "",
    "text": "Deployment managers support the automated application rollout of the application binaries and of the application configurations across platforms (that is, managing distributed and mainframe systems). Typically, a deployment manager would perform the following tasks when deploying a package into an environment:\n\nVerify that the package is applicable to the environment\nSort and order the artifacts to process\nFor each artifact of a given type, run the appropriate process\nPerform post deployment activities\n\nApproval processes may be embedded. This approach will likely be necessary due to audit and legal requirements.\nDeployment managers are aware of the execution environments. They generally also provide an inventory of the packages and of the current deployed binaries in a given environment. The deployment process can be implemented with scripts or with other more sophisticated techniques. These more sophisticated processes manage return codes and facilitate the use of the APIs of the different middleware systems. However, in both cases, and in order to install the package, the deployment process will consume the manifest file, which contains metadata about contents of the package.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Outlining the deployment strategy"
    ]
  },
  {
    "objectID": "deployment-strategy.html#resources",
    "href": "deployment-strategy.html#resources",
    "title": "Outlining the deployment strategy",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Outlining the deployment strategy"
    ]
  },
  {
    "objectID": "build.html",
    "href": "build.html",
    "title": "Build",
    "section": "",
    "text": "The Build component of a continuous integration/continuous delivery (CI/CD) pipeline converts the source code into executable binaries. It supports multiple platforms and languages. In mainframe environments, it includes understanding dependencies, compile, link-edit, and unit test. The build can include the inspection of code quality to perform automated validation against a set of coding rules. In some cases, code quality inspection could also be a component of its own in the pipeline.\nWhile many of the steps in the DevOps flow for mainframe applications can be performed using the same tooling used by distributed development teams, the build step in particular needs to remain on z/OS. Therefore, in the IBM Z DevOps solution, this step is primarily handled by IBM Dependency Based Build (DBB). DBB has intelligent build capabilities where it can not only compile and link z/OS programs to produce executable binaries, but it can also perform different types of builds to support the various steps in an application development workflow. This includes the ability to perform an “impact build”, where DBB will only build programs that have changed since the last successful build and the files impacted by those changes, saving time and resources during the development process.\nDBB is a set of APIs based on open-source Groovy and adapted to z/OS. This enables you to easily incorporate your z/OS application builds into the same automated CI/CD pipeline used by distributed teams. It is possible to use DBB as a basis to write your own build scripts, but we recommend starting with the zAppBuild framework to provide a template for your build, and then customizing it as necessary for your enterprise and applications.\nThe zAppBuild framework helps facilitate the adoption of DBB APIs for your enterprise and applications. Rather than writing your own Groovy scripts to interact with the DBB APIs, you can fill in properties to define your build options for zAppBuild, and then let zAppBuild invoke DBB to perform your builds.\n\n\n\nPerform builds on z/OS and persist build results\nPersist metadata about the builds, which can then be used in subsequent automated CI/CD pipeline steps, as well as informing future DBB builds\nCan be run from the command line interface, making it easy to integrate into an automated CI/CD pipeline\n\n\n\n\n\nFramework template facilitates leveraging DBB APIs to build z/OS applications, letting you focus on defining the build’s properties separately from the logic to perform the build\nHigh-level enterprise-wide settings that can be set for all z/OS application builds\nApplication-level settings for any necessary overrides in individual application builds\nIncludes out-of-the-box support for the following languages:\n\nCOBOL\nPL/I\nBMS and MFS\nLink Cards\nPSB, DBD\nSee zAppBuild’s Supported Languages for a full list of out-of-the-box supported languages.\n\nSupported build actions:\n\nSingle program (“User Build”): Build a single program in the application\nList of programs: Build a list of programs provided by a text file\nFull build: Build all programs (or buildable files) of an application\nImpact build: Build only the programs impacted by source files that have changed since the last successful build\nScan only: Skip the actual building and only scan source files for dependency data\nAdditional supported build actions are listed in zAppBuild’s Build Scope documentation.\n\n\n\n\n\nzAppBuild is a free, generic mainframe application build framework that customers can extend to meet their DevOps needs. It is available under the Apache 2.0 license, and is a sample to get you started with building Git-based source code on z/OS UNIX System Services (z/OS UNIX). It is made up of property files to configure the build behavior, and Groovy scripts that invoke the DBB toolkit APIs.\nBuild properties can span across all applications (enterprise-level), one application (application-level), or individual programs. Properties that cross all applications are managed by administrators and define enterprise-wide settings such as the PDS name of the compiler, data set allocation attributes, and more. Application- and program-level properties are typically managed within the application repository itself.\nThe zAppBuild framework is invoked either by a developer using the “User Build” capability in their integrated development environment (IDE), or by an automated CI/CD pipeline. It supports different build types.\nThe main script of zAppBuild, build.groovy, initializes the build environment, identifies what to build, and invokes language scripts. This triggers the utilities and DBB APIs to then produce runtime artifacts. The build process also creates logs and an artifact manifest (BuildReport.json) for deployment processes coordinated by the deployment manager.\nThe following chart provides a high-level summary of the steps that zAppBuild performs during a build:\n\n\n\nzAppBuild workflow\n\n\n\n\nThe zAppBuild framework is split into two parts. The core build framework, called dbb-zappbuild, is a Git repository that contains the build scripts and stores enterprise-level settings. It resides in a permanent location on the z/OS UNIX file system (in addition to the central Git repository). It is typically owned and controlled by the central build team.\nThe other part of zAppBuild is the application-conf folder that resides within each application repository to provide application-level settings to the central build framework. These settings are owned, maintained, and updated by the application team.\n\n\n\nbuild-conf contains the following enterprise-level property files:\n\nbuild.properties defines DBB initilization properties, including location and of the DBB metadata store (for storing dependency information) and more.\ndataset.properties describes system datasets such as the PDS name of the COBOL compiler or libraries used for the subsystem. You must update this properties file with your site’s data set names.\nSeveral language-specific property files that define the compiler or link-editor/binder program names, system libraries, and general system-level properties for COBOL, Assembler, and other languages.\n\nlanguages contains Groovy scripts used to build programs. For example, Cobol.groovy is called by build.groovy to compile the COBOL source codes. The application source code is mapped by its file extension to the language script in application-conf/file.properties.\nsamples contains an application-conf template folder and a reference sample application, MortgageApplication.\nutilities contains helper scripts used by build.groovy and other scripts to calculate the build list.\nbuild.groovy is the main build script of zAppBuild. It takes several required command line parameters to customize the build process.\n\n\n\n\nThis folder is located within the application’s repository, and defines application-level properties such as the following:\n\napplication.properties defines various directory rules, default Git branch, impact resolution rules such as the copybook lookup rules, and more.\nfile.properties maps files to the language scripts in dbb-zappbuild/languages, and provides file-level property overrides.\nProperty files for further customization of the language script processing. For example, Cobol.properties is one of the language properties files to define compiler and link-edit options, among other properties.\n\nThe following diagram illustrates how zAppBuild’s application- and enterprise-level configurations feed into its build scripts to generate build artifacts from an application repository: \n\n\n\n\n\n\nIBM documentation for DBB\nIBM Dependency Based Build Fundamentals course\n\nThis page contains reformatted excerpts from the following documents:\n\nDBB zAppBuild Introduction and Custom Version Maintenance Strategy\nPackaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Build"
    ]
  },
  {
    "objectID": "build.html#dbb-features",
    "href": "build.html#dbb-features",
    "title": "Build",
    "section": "",
    "text": "Perform builds on z/OS and persist build results\nPersist metadata about the builds, which can then be used in subsequent automated CI/CD pipeline steps, as well as informing future DBB builds\nCan be run from the command line interface, making it easy to integrate into an automated CI/CD pipeline",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Build"
    ]
  },
  {
    "objectID": "build.html#zappbuild-features",
    "href": "build.html#zappbuild-features",
    "title": "Build",
    "section": "",
    "text": "Framework template facilitates leveraging DBB APIs to build z/OS applications, letting you focus on defining the build’s properties separately from the logic to perform the build\nHigh-level enterprise-wide settings that can be set for all z/OS application builds\nApplication-level settings for any necessary overrides in individual application builds\nIncludes out-of-the-box support for the following languages:\n\nCOBOL\nPL/I\nBMS and MFS\nLink Cards\nPSB, DBD\nSee zAppBuild’s Supported Languages for a full list of out-of-the-box supported languages.\n\nSupported build actions:\n\nSingle program (“User Build”): Build a single program in the application\nList of programs: Build a list of programs provided by a text file\nFull build: Build all programs (or buildable files) of an application\nImpact build: Build only the programs impacted by source files that have changed since the last successful build\nScan only: Skip the actual building and only scan source files for dependency data\nAdditional supported build actions are listed in zAppBuild’s Build Scope documentation.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Build"
    ]
  },
  {
    "objectID": "build.html#zappbuild-introduction",
    "href": "build.html#zappbuild-introduction",
    "title": "Build",
    "section": "",
    "text": "zAppBuild is a free, generic mainframe application build framework that customers can extend to meet their DevOps needs. It is available under the Apache 2.0 license, and is a sample to get you started with building Git-based source code on z/OS UNIX System Services (z/OS UNIX). It is made up of property files to configure the build behavior, and Groovy scripts that invoke the DBB toolkit APIs.\nBuild properties can span across all applications (enterprise-level), one application (application-level), or individual programs. Properties that cross all applications are managed by administrators and define enterprise-wide settings such as the PDS name of the compiler, data set allocation attributes, and more. Application- and program-level properties are typically managed within the application repository itself.\nThe zAppBuild framework is invoked either by a developer using the “User Build” capability in their integrated development environment (IDE), or by an automated CI/CD pipeline. It supports different build types.\nThe main script of zAppBuild, build.groovy, initializes the build environment, identifies what to build, and invokes language scripts. This triggers the utilities and DBB APIs to then produce runtime artifacts. The build process also creates logs and an artifact manifest (BuildReport.json) for deployment processes coordinated by the deployment manager.\nThe following chart provides a high-level summary of the steps that zAppBuild performs during a build:\n\n\n\nzAppBuild workflow\n\n\n\n\nThe zAppBuild framework is split into two parts. The core build framework, called dbb-zappbuild, is a Git repository that contains the build scripts and stores enterprise-level settings. It resides in a permanent location on the z/OS UNIX file system (in addition to the central Git repository). It is typically owned and controlled by the central build team.\nThe other part of zAppBuild is the application-conf folder that resides within each application repository to provide application-level settings to the central build framework. These settings are owned, maintained, and updated by the application team.\n\n\n\nbuild-conf contains the following enterprise-level property files:\n\nbuild.properties defines DBB initilization properties, including location and of the DBB metadata store (for storing dependency information) and more.\ndataset.properties describes system datasets such as the PDS name of the COBOL compiler or libraries used for the subsystem. You must update this properties file with your site’s data set names.\nSeveral language-specific property files that define the compiler or link-editor/binder program names, system libraries, and general system-level properties for COBOL, Assembler, and other languages.\n\nlanguages contains Groovy scripts used to build programs. For example, Cobol.groovy is called by build.groovy to compile the COBOL source codes. The application source code is mapped by its file extension to the language script in application-conf/file.properties.\nsamples contains an application-conf template folder and a reference sample application, MortgageApplication.\nutilities contains helper scripts used by build.groovy and other scripts to calculate the build list.\nbuild.groovy is the main build script of zAppBuild. It takes several required command line parameters to customize the build process.\n\n\n\n\nThis folder is located within the application’s repository, and defines application-level properties such as the following:\n\napplication.properties defines various directory rules, default Git branch, impact resolution rules such as the copybook lookup rules, and more.\nfile.properties maps files to the language scripts in dbb-zappbuild/languages, and provides file-level property overrides.\nProperty files for further customization of the language script processing. For example, Cobol.properties is one of the language properties files to define compiler and link-edit options, among other properties.\n\nThe following diagram illustrates how zAppBuild’s application- and enterprise-level configurations feed into its build scripts to generate build artifacts from an application repository:",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Build"
    ]
  },
  {
    "objectID": "build.html#resources",
    "href": "build.html#resources",
    "title": "Build",
    "section": "",
    "text": "IBM documentation for DBB\nIBM Dependency Based Build Fundamentals course\n\nThis page contains reformatted excerpts from the following documents:\n\nDBB zAppBuild Introduction and Custom Version Maintenance Strategy\nPackaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Build"
    ]
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorial",
    "section": "",
    "text": "Tutorial\nIf you are new to DevOps for z/OS applications, you might want to explore the workflow and tooling without having to first install and configure an entire technology stack on your own environment. IBM Z Systems software trials (also known as IBM Z Trial) allows you try out a variety of IBM Z software experiences in a provisioned environment for free, meaning you can get right to learning the important points. The following IBM Z Trial experiences are particularly relevant to DevOps and CI/CD pipelines for z/OS applications:\n\nBring Your Own (BYO) IDE for Cloud Native Development: Explore the integrated development environment (IDE) functionality in the Eclipse-based IBM Developer for z/OS® (IDz) and/or Microsoft® VS Code™ with IBM Z extensions, and learn how these capabilities fit into the DevOps workflow with other tools such as Git, DBB, and Jenkins.\nIBM Application Delivery Foundation for z/OS®: Explore the range of features in the IBM Application Delivery Foundation for z/OS (ADFz) suite of integrated tooling that can help you analyze, understand, debug, and modify your COBOL programs.",
    "crumbs": [
      "Tutorials",
      "Tutorial"
    ]
  },
  {
    "objectID": "install-configure.html",
    "href": "install-configure.html",
    "title": "Installing and configuring your tools",
    "section": "",
    "text": "import Tabs from ‘@theme/Tabs’; import TabItem from ‘@theme/TabItem’;",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#integrated-development-environment",
    "href": "install-configure.html#integrated-development-environment",
    "title": "Installing and configuring your tools",
    "section": "Integrated development environment",
    "text": "Integrated development environment\nWho needs to install the integrated development environment (IDE), and where?\n\nThe integrated development environment (IDE) will need to be installed by z/OS application developers on their local workstations.\nSome IDE options have host components that require a system programmer to configure.\n\n\nIDE options\n \nIBM Developer for z/OS (IDz): Eclipse-based IDE\nAdditional useful tools: Groovy development environment in IDz\n \nWazi for VS Code: Visual Studio Code IDE with IBM Z Open Editor and Debug extensions)\n \nWazi for Dev Spaces: Browser-based IDE\nNote: If you are using Wazi Dev Spaces (browser-based IDE), developers will not have to install the IDE.",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#source-code-management",
    "href": "install-configure.html#source-code-management",
    "title": "Installing and configuring your tools",
    "section": "Source code management",
    "text": "Source code management\nThe IBM Z DevOps solution supports Git as the SCM. Clients can pick from several Git providers, including those listed in the following Git provider options section.\nWho needs to install the source code management (SCM), and where?\n\nRegardless of the Git provider selected, Git will need to be installed on both developer machines, and on z/OS.\n\nDeveloper machines: Developers can obtain Git the same way they would in distributed development.\nz/OS: The Rocket distribution of Git is specifically ported for z/OS, and can be installed by a system programmer.\n\n\nIn addition to installing Git, the Git provider should also be configured. (Usually by the enterprise’s team that handles the Git provider for the distributed side will help provide the basic organization or repository setup for the z/OS application team(s), and then the z/OS/DevOps team will maintain these repositories.)\n\nGit provider options\n\nAzure Repos\nBitbucket\nGitHub\nGitLab",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#build",
    "href": "install-configure.html#build",
    "title": "Installing and configuring your tools",
    "section": "Build",
    "text": "Build\nIBM Dependency Based Build (DBB) is the build tool in the IBM Z DevOps solution. This is complemented by the zAppBuild framework, which helps facilitate your build process using DBB APIs. Many clients start by using zAppBuild and enhancing it to their needs, for example by adding new language scripts, or by modifying the existing build processing logic.\nThis section provides a set of instructions for how you can make zAppBuild available in your Git provider and how to synchronize new features of zAppBuild into your customized fork.\nNote: zAppBuild releases new versions through the main branch. New contributions are added first to the develop branch, which then will be reviewed and merged to the main branch.\nThe IBM DBB samples repository contains additional utilities that enhance or integrate with the other DBB build processes.\nWho needs to install DBB, and where?\n\nSystem programmers install DBB toolkit on z/OS.\n\nSet up Db2 for z/OS or Db2 for LUW (Linux, UNIX, and Windows) for the DBB metadata store.\nSee IBM Documentation on Installing and configuring DBB.\n\nDevelopers using IDz as their IDE must add the DBB integration to their installation of IDz in order to use DBB’s user build feature.\n\nWho needs to set up zAppBuild (and the IBM DBB samples repository), and where?\n\nThe build engineer and/or DevOps team (in DAT roles: Build specialist and/or Pipeline specialist) should set this up with the enterprise’s Git provider.\n\nSteps for making a copy of the zAppBuild repository available in your enterprise’s preferred Git provider are provided in the following section.\nIf the IBM DBB samples repository is needed, it can be copied from its IBM GitHub page to your Git provider in a similar manner to the zAppBuild repository.\n\n\n\nMaking zAppBuild available in your Git provider\nBefore you start your customization of zAppBuild, you must first create a clone of IBM’s zAppBuild repository and store the clone in your Git provider of choice. This could be any Git provider, such as GitHub, GitLab, Bitbucket or Azure Repos, and so on. If you have done this already, feel free to move to the next section.\nHere are the steps to make the zAppBuild repository available in a central repository on your Git provider:\n\nOn your local workstation, use your browser and log on to your Git provider. Follow the instructions in your Git provider to create a new repository, which will be the new “home” of your customized version of zAppBuild.\n\nWe suggest “dbb-zappbuild” as the new repository/project name, but you can use another name if you prefer.\nSet the repository’s visibility according to your needs.\nDo not initialize the repository yet.\n\nYour Git provider will create the repository, but it is not yet initialized. On most Git providers, the repository creation process will end on a page with information on how to share an existing Git repository. Leave the browser open.\n\n\nClone IBM’s public zAppBuild repository to your local workstation. You can use your local workstation’s terminal to complete this step (for example, Git Bash in Windows, or Terminal on MacOS).\n\nIf you are using IBM Developer for z/OS (IDz) as your IDE, you can use its Local Shell. Wazi for VS Code and Wazi for Dev Spaces also both have Terminal windows. (We documented the steps in this guide using a terminal.)\n\n\nIn the terminal, navigate to the folder where you would like to clone the repository.\nRetrieve the Git repository URL or SSH path from IBM’s public zAppBuild repository from https://github.com/IBM/dbb-zappbuild:\n\n\n\nRetrieving the Git repository URL from IBM’s public zAppBuild repository\n\n\nIn your terminal, enter the command for cloning the repository. (The following command uses the Git repository URL, but the SSH path can also be used if you have SSH keys set up.):\ngit clone https://github.com/IBM/dbb-zappbuild.git\n\nExample Git clone command with output in a terminal:\n\n\n\nExample Git clone command with output in a terminal\n\n\n\n\nFollow the instructions of your Git provider to push the contents of your newly-cloned local repository (from Step 2) to the central repository on your Git provider (created in Step 1). (Note: Exact instructions may vary from Git provider to Git provider.)\n\nWithin the terminal session, execute the following commands to push an existing Git repository:\n\nReplace &lt;existing_repo&gt; with the path to your newly-cloned local repository.\nReplace &lt;Your central Git repository&gt; with the URL to the new central repository on your Git provider. (For example, with GitLab as the Git provider, the URL might look similar to git@gitlab.dat.ibm.com:DAT/dbb-zappbuild.git.)\n\ncd &lt;existing_repo&gt;\ngit remote rename origin old-origin\ngit remote add origin &lt;Your central Git repository&gt;\ngit push -u origin –all\ngit push -u origin --tags\nOn the Git provider’s webpage for your new central repository in the browser, you will find that the repository is now populated with all of zAppBuild’s files and history, just like on IBM’s public zAppBuild repository.\n\nThe following screenshot shows an example of a populated central zAppBuild repository with GitLab as the Git provider:\n\n\n\nExample of a populated central zAppBuild repository in GitLab’s web UI\n\n\n\n\n\n\n\nUpdating your customized version of zAppBuild\nTo update your customized version of zAppBuild with latest official zAppBuild enhancements, you can integrate the latest official zAppBuild features into your version of zAppBuild. So, let’s get started.\n\nLocate the internal Git repository and create a new Git branch. This is a good practice to validate the changes first. In this example, the new branch is called update-zappbuild.\nAdd a new Git remote definition to connect to IBM’s official public zAppBuild GitHub repository. (Note: This step requires internet connectivity.)\n\nFirst, list the remotes by issuing git remote -v:\n\n\n\nOutput from initial listing of the remote tracked repositories\n\n\n\nFor more on Git remotes, see the git-remote documentation.\n\nAdd a new remote named zappbuild-official to connect to GitHub by issuing the following command:\ngit remote add zappbuild-official https://github.com/IBM/dbb-zappbuild.git\nVerify that the new remote is available by issuing the command to list the remotes again: git remote -v:\n\n\n\nOutput from listing the remote tracked repositories after adding the zappbuild-official remote\n\n\nFetch the latest information from the official repository, by executing a Git fetch for the official dbb-zappubuild repository:\ngit fetch zappbuild-official\n\n\n\nOutput from Git fetch of the zappbuild-official remote\n\n\nMake sure that your feature branch is checked out, before attempting to merge the changes from zappbuild-official. To merge the changes run into your branch update-zappbuild, run the following command:\ngit merge zappbuild-official/main\n\n\n\nMerge conflict when attempting to merge in changes from zappbuild-official\n\n\nPotentially, you face merge conflicts. In the above case, the merge processor could not automatically resolve the utilities/ImpactUtilities.groovy.\nRun the command git status to see which files changed:\n\n\n\nOutput of Git status to view files changed during the merge attempt\n\n\nOpen the unmerged files and resolve them manually. Either use the terminal, or an IDE for this task.\n:::tip\nThe Git integration in many modern IDEs (for example, VS Code) is able to provide a side-by-side comparison highlighting the diff between your feature branch and the incoming changes from the merge attempt (in this case, from zappbuild-official). This can help make manual resolution of any merge conflicts much easier.\n:::\nCommit the changes and verify them with a sample application before committing it (or opening a pull request to commit it) to your main branch that is used for all your production DBB builds.",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#artifact-repository",
    "href": "install-configure.html#artifact-repository",
    "title": "Installing and configuring your tools",
    "section": "Artifact repository",
    "text": "Artifact repository\nThe artifact repository is often already in-place as part of the enterprise’s distributed CI/CD pipeline.\nWho needs to set up the artifact repository, and where?\n\nGenerally, the DevOps team (pipeline specialist) will work to set this up for z/OS application teams, as well.\n\n\nArtifact repository options\n\nAzure Artifacts\nJFrog Artifactory\nSonatype Nexus\nUrbanCode Deploy (UCD) CodeStation",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#deployment-manager",
    "href": "install-configure.html#deployment-manager",
    "title": "Installing and configuring your tools",
    "section": "Deployment manager",
    "text": "Deployment manager\nWho needs to install the deployment manager, and where?\n\nDepending on the software selected, the deployment manager might require an agent on the z/OS side, which can be set up by a system programmer (infrastructure team). (Alternatively, the pipeline orchestrator could SSH into z/OS.)\n\n\nDeployment manager options\n\nUrbanCode Deploy (UCD)\nWazi Deploy",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#pipeline-orchestrator",
    "href": "install-configure.html#pipeline-orchestrator",
    "title": "Installing and configuring your tools",
    "section": "Pipeline orchestrator",
    "text": "Pipeline orchestrator\nThe artifact repository is often already in-place as part of the enterprise’s distributed CI/CD pipeline.\nWho needs to set up the pipeline orchestrator, and where?\n\nGenerally, the DevOps team (pipeline specialist) will work to set this up for z/OS application teams, as well.\nThe pipeline orchestrator often requires an agent or runner on the z/OS side, which can be set up by a system programmer (infrastructure team). (Alternatively, the pipeline orchestrator could SSH into z/OS.)\n\n\nPipeline orchestrator options\n\nAzure Pipeline\nGitHub Actions\nGitLab CI\nJenkins",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "install-configure.html#resources",
    "href": "install-configure.html#resources",
    "title": "Installing and configuring your tools",
    "section": "Resources",
    "text": "Resources\nThis page contains reformatted excerpts from the following documents:\n\nDBB zAppBuild Introduction and Custom Version Maintenance Strategy",
    "crumbs": [
      "Getting Started",
      "Installing and configuring your tools"
    ]
  },
  {
    "objectID": "sclm-git-migration-tool.html",
    "href": "sclm-git-migration-tool.html",
    "title": "SCLM-to-Git Migration Tool",
    "section": "",
    "text": "Software Configuration Library Manager (SCLM) is a primarily a library manager and configuration manager. Although it also has some change management and impact analysis capabilities, the functionality is basic by today’s standards. To help customers migrate from SCLM to the more modern Git software configuration management solution, IBM provides the SCLM-to-Git Migration Tool.\nThe SCLM-to-Git Migration Tool is a set of scripts that extract SCLM data and place it in the target Git repository. The migration is performed within z/OS UNIX System Services (z/OS UNIX), using Shell scripts that subsequently invoke Groovy scripts. The SCLM-to-Git Migration Tool also creates the .gitattributes file for code page conversion, although the tool’s generated sample build scripts are not currently compatible with the zAppBuild sample DBB build framework.\nBefore getting started with the migration, we recommend reviewing your current SCLM setup and removing any unused or redundant definitions to help make the migration process smoother.\n\n\n\nThe SCLM-to-Git Migration Tool migrates more than just source code from SCLM to Git. The following list details what is included in the migration:\n\nProject definition data\n\nThe project definitions define the steps a source member needs to go through to be built (for example, precompile, compile, and link edit).\nThis is mainly language definitions, but also includes allocation information of any files used in the language definitions.\n\nMember-level metadata\n\nThis includes member-level overrides, which are set by an SCLM construct called an ARCHDEF. This metadata provides the ability to override compile options, change output member names, and change output file destinations.\n\nGeneration of link decks from link-edit control (LEC) ARCHDEFs\n\nLEC ARCHDEFs are how SCLM controls the compile and link-edit prorcess. These LEC ARCHDEFs need to be converted to standard link-edit decks for use by DBB.\n\nCurrent version of the source\n\nThe zimport.sh script is used to migrate the current production version of the source to Git.\n\n(Optional) Previous version of the source\n\nPrevious versions can also be migrated by starting with the oldest version first and working backwards to the current version.\n\n\n\n\n\nThe SCLM-to-Git Migration Tool moves source members from SCLM to Git in three phases. The steps and outputs for each phase are detailed in the tool’s Migration Process documentation on GitHub. You can view a video guide for how to perform each phase in the following list.\nThe three-phase migration process used by the SCLM-to-Git Migration Tool:\n\nExtract the SCLM metadata and source: \nMigrate source members to a Git repository in z/OS UNIX: \nCreate sample DBB Groovy build scripts: :::note\n\nThe sample build scripts created in this step are not currently compatible with the zAppBuild sample DBB build framework. If you would like to use zAppBuild, you can skip this step and simply use the migrated code from the previous step (Step 2) with zAppBuild.\n::: \nThe above video guides are part of the IBM Dependency Based Build Foundation Course. More information on the context, configuration, and steps for the SCLM-to-Git migration tool can be found in Module 5 (Migrating Source Members to Git) of this course.\n\n\n\n\nSCLM-to-Git migration tool GitHub repository\nModule 5 - Migrating Source Members to Git, IBM Dependency Based Build Foundation Course\nMigration made easy - host SCLM to Git on Z (webinar)"
  },
  {
    "objectID": "sclm-git-migration-tool.html#introduction",
    "href": "sclm-git-migration-tool.html#introduction",
    "title": "SCLM-to-Git Migration Tool",
    "section": "",
    "text": "Software Configuration Library Manager (SCLM) is a primarily a library manager and configuration manager. Although it also has some change management and impact analysis capabilities, the functionality is basic by today’s standards. To help customers migrate from SCLM to the more modern Git software configuration management solution, IBM provides the SCLM-to-Git Migration Tool.\nThe SCLM-to-Git Migration Tool is a set of scripts that extract SCLM data and place it in the target Git repository. The migration is performed within z/OS UNIX System Services (z/OS UNIX), using Shell scripts that subsequently invoke Groovy scripts. The SCLM-to-Git Migration Tool also creates the .gitattributes file for code page conversion, although the tool’s generated sample build scripts are not currently compatible with the zAppBuild sample DBB build framework.\nBefore getting started with the migration, we recommend reviewing your current SCLM setup and removing any unused or redundant definitions to help make the migration process smoother."
  },
  {
    "objectID": "sclm-git-migration-tool.html#what-gets-migrated",
    "href": "sclm-git-migration-tool.html#what-gets-migrated",
    "title": "SCLM-to-Git Migration Tool",
    "section": "",
    "text": "The SCLM-to-Git Migration Tool migrates more than just source code from SCLM to Git. The following list details what is included in the migration:\n\nProject definition data\n\nThe project definitions define the steps a source member needs to go through to be built (for example, precompile, compile, and link edit).\nThis is mainly language definitions, but also includes allocation information of any files used in the language definitions.\n\nMember-level metadata\n\nThis includes member-level overrides, which are set by an SCLM construct called an ARCHDEF. This metadata provides the ability to override compile options, change output member names, and change output file destinations.\n\nGeneration of link decks from link-edit control (LEC) ARCHDEFs\n\nLEC ARCHDEFs are how SCLM controls the compile and link-edit prorcess. These LEC ARCHDEFs need to be converted to standard link-edit decks for use by DBB.\n\nCurrent version of the source\n\nThe zimport.sh script is used to migrate the current production version of the source to Git.\n\n(Optional) Previous version of the source\n\nPrevious versions can also be migrated by starting with the oldest version first and working backwards to the current version."
  },
  {
    "objectID": "sclm-git-migration-tool.html#migration-process",
    "href": "sclm-git-migration-tool.html#migration-process",
    "title": "SCLM-to-Git Migration Tool",
    "section": "",
    "text": "The SCLM-to-Git Migration Tool moves source members from SCLM to Git in three phases. The steps and outputs for each phase are detailed in the tool’s Migration Process documentation on GitHub. You can view a video guide for how to perform each phase in the following list.\nThe three-phase migration process used by the SCLM-to-Git Migration Tool:\n\nExtract the SCLM metadata and source: \nMigrate source members to a Git repository in z/OS UNIX: \nCreate sample DBB Groovy build scripts: :::note\n\nThe sample build scripts created in this step are not currently compatible with the zAppBuild sample DBB build framework. If you would like to use zAppBuild, you can skip this step and simply use the migrated code from the previous step (Step 2) with zAppBuild.\n::: \nThe above video guides are part of the IBM Dependency Based Build Foundation Course. More information on the context, configuration, and steps for the SCLM-to-Git migration tool can be found in Module 5 (Migrating Source Members to Git) of this course."
  },
  {
    "objectID": "sclm-git-migration-tool.html#resources",
    "href": "sclm-git-migration-tool.html#resources",
    "title": "SCLM-to-Git Migration Tool",
    "section": "",
    "text": "SCLM-to-Git migration tool GitHub repository\nModule 5 - Migrating Source Members to Git, IBM Dependency Based Build Foundation Course\nMigration made easy - host SCLM to Git on Z (webinar)"
  },
  {
    "objectID": "managing-code-page-conversion.html",
    "href": "managing-code-page-conversion.html",
    "title": "Managing code page conversion",
    "section": "",
    "text": "Choosing Git to host enterprise-level software assets implies a migration task of the z/OS artifacts. Whether the source code is under the control of a legacy SCM solution or not, these artifacts are typically stored as members within partitioned data sets (PDSs) with a specific encoding. Depending on locale, country languages, regulations or work habits, the encoding of the source code can differ by various specificities such as accents, locale typography and characters. To support these peculiarities, many different EBCDIC code pages were developed in z/OS and are now widely used.\nIn the Git world, the standard prevailing encoding is UTF-8, which supports most (if not all) of the characters used in printing worldwide. When migrating artifacts from z/OS to Git, a translation must occur to convert the EBCDIC-encoded content of source files to UTF-8. This process is transparent to the user, whereby the translation is performed by the Rocket Git client, as long as the necessary information on how to perform the conversion is provided. Git uses a specific file called .gitattributes to describe and dictate the translation behavior. This page contains details on the encoding that should be used when hosting the artifacts on z/OS or on Git repositories.\nThis page will highlight some specific migration situations and some cautionary commentary that must be taken into account in order to ensure a successful migration of z/OS artifacts to Git. The purpose of this document is not to cover all the country-related specificities, nor the complex encodings found worldwide. Rather it is aimed at presenting general use cases and how to prevent/circumvent potential pitfalls.\n\n\n\nCode page translation has always been a challenge since z/OS is exchanging data with the distributed systems. To completely understand the effect of encoding, it is important to take a step back and analyze the binary content of files. This analysis will help to better visualize the transformation process that is occurring behind the scenes\n\n\nA file itself can be summarized as being a series of bytes, often expressed in the hexadecimal notation (from x’00’ to x’FF’). This file can represent binary content which is not meant to be human-readable (like an executable file), or text content. For the latter and from a user standpoint, this series of bytes doesn’t mean anything if the content is not associated with a code page, which will link each byte to a printable character (or a control character). Printable characters are the formation of a character set.\nThe combination of the contents of a file and the code page used defines the human-readable content. That said, the contents of a file will be displayed slightly differently when combined with different code pages. Let’s explore a couple of examples using the ISPF Editor with the “HEX VERT” option enabled to show the hexadecimal values of a file.\nConsider the following example where a file containing the French sentence “Bonjour à tous !” (which translates to “Hello everyone!”) is displayed with two different code pages. The file which contains this sentence was saved using IBM-1147 encoding. When the file is then displayed using a different encoding of IBM-1047, although the contents of the file is the same from a binary standpoint, some characters are displayed in a different way:\n\nUsing the IBM-1147 code page:\n\n\n\nUsing the IBM-1147 code page\n\n\nUsing the IBM-1047 code page:\n\n\n\nUsing the IBM-1047 code page\n\n\n\nTo be correctly displayed with the IBM-1047 code page, the contents of this file must be transformed. Please note how the hexadecimal codes for the à and the ! characters changed, respectively from x’7C’ to x’44’ and from x’4F’ to x’5A’:\n\nTransforming to the IBM-1047 code page:\n\n\n\nTransforming to the IBM-1047 code page\n\n\n\nIt is important to understand that the code page plays a determining role not only when displaying a file, but also when editing it. To ensure the content of a file is consistent when used by different people, the code page used for editing and displaying will likely be the same for all the users. If Alice edits a file with the IBM-1147 code page and introduces characters (like accents) specific to that code page, then Bob will need to use the IBM-1147 code page to display the content of this file. Otherwise, he may experience the situation depicted earlier, where accents are not displayed correctly. If Bob uses the IBM-1047 code page to edit the file and change the content to replace the special characters by the corresponding accents, then Alice will not be able to display the content of this file like she entered it on her terminal.\nConsider this next example. A program file, such as C/370, employs the left ([) and right (]) brackets in some of the language statement constructs. In many US-based legacy applications, the original encoding used to create source files was IBM-037. If these files are then displayed using a different encoding of IBM-1047, again, although the content of the files is the same from a binary standpoint, the brackets are not displayed the correct way:\n\nUsing the IBM-037 code page:\n\n\n\nUsing the IBM-1147 code page\n\n\nUsing the IBM-1047 code page:\n\n\n\nUsing the IBM-1047 code page\n\n\n\nTo be correctly displayed with the IBM-1047 code page, the contents of this file must be transformed. The hexadecimal codes for the [ and the ] characters must be changed from x’BA’ and x’BB’ to x’AD’ and x’BD’, respectively:\n\nTransforming to the IBM-1047 code page:\n\n\n\nTransforming to the IBM-1047 code page\n\n\n\nAgain, it is very important that anyone and everyone who displays or edits the file uses a consistent code page. This can sometimes be a challenge, as the code page to be used is generally specified in the 3270 Emulator (TN3270) client session set-up. Another challenge is trying to determine the original encoding used to create the file.\nTo summarize, the binary content of a file must be transformed to ensure consistency when displayed with another code page. This process is known as the code page translation and is key when migrating your source from your z/OS platform to a distributed platform, which is using different code pages (and most likely today, UTF-8).\n\n\n\nIn this document, we will interchangeably use the coded character set ID (CCSID), its equivalent name, and the code page to designate an encoding. Although not strictly equivalent, they are all often interchanged for the sake of convenience. For instance, the IBM-1047 code page is equivalent to the 1047 coded character set (CCSID) and is usually named IBM-1047. The code page for IBM-037 is equivalent to coded character set (CCSID) 037 and is usually named IBM-037. The CCSID for UTF-8 is 1208 and is linked with many code pages.\nTo simplify, the code pages will be designated by their common name, for instance IBM-037, IBM-1047, IBM-1147, and UTF-8.\nFor reference, a list of code pages is available in the Personal Communications documentation site.\n\n\n\nIf you have ever tried transferring a z/OS data set to a distributed machine running a traditional operating system (like Microsoft Windows or Linux), you probably ended up with similar content when opening the file:\n\n\n\nUnreadable file due to transfer without code page conversion\n\n\nThe content is unreadable because the transfer was performed without a code page conversion. This generally happens when the file was transferred as binary, which leaves the content of the file untouched. Most of the transfer utilities offer the capability to transfer the files as binary or as text. In the latter option, code page conversion occurs, and the file should be readable on distributed platforms.\nThe same conversion must be performed for text files when migrating to Git. Generally, Git uses the UTF-8 code page and, as such, a translation must occur on the content of the transferred files. Likewise, when bringing the files back from Git to z/OS, a backward conversion must be performed to ensure the files are correctly read with the code page in use. Failure in performing this conversion may break the contents of the files, which could become damaged and unreadable. Recovering from this failure is difficult, and the best solution is usually to restart the conversion all over again from the beginning, which often means deleting the files in Git and transferring the same files from z/OS again.\n\n\n\nIn the traditional EBCDIC code pages, some characters exist to control the different devices themselves or the way text should be displayed. These characters are located in the first 64 positions of the EBCDIC characters tables and can be classified into 2 categories: the non-printable characters and the non-roundtripable characters.\n\nNon-printable characters: These characters can be translated from EBCDIC to UTF-8 and back to EBCDIC without breaking the content of the file. However, editing files which contain these characters with editing tools on a distributed platform, can cause display issues. Non-printable characters include all EBCDIC characters below 0x40.\nNon-roundtripable characters: The translation is still possible from EBCDIC to UTF-8, but the translation back to EBCDIC is not possible. In this case, the translated file is no longer identical to its original version. This specific situation must be considered and managed prior to the migration to Git. For these special characters, the conversion from EBCDIC to UTF-8 is generally feasible, but the resulting content of the files may be scrambled or displayed in a wrong way. Moreover, when transferring the files back to z/OS, the files could be damaged, and the transfer could even fail. Non-roundtripable characters include the following:\n\nEBCDIC\n\nNewline: 0x15\nCarriage return: 0x0D\nLine feed: 0x25\nSHIFT_IN/SHIFT_OUT: 0x0F & 0x0E\n\n\n\nIn a migration process, when either non-printable or non-roundtripable characters are found, two options are presented:\n\nKeep these characters “as-is” with the risk of damaging the source files.\nModify the characters to a conversion-friendly format.\n\nFor the first option, it is possible to keep the content of the files intact, but the cost of this is to manage the file as a binary artifact in Git. The benefit of this configuration is that the content of the file is not altered by Git when the file is transferred. Because it is flagged as binary, the contents are not converted to UTF-8 and remain the same across the entire workflow (that is, from z/OS PDS to Git to z/OS PDS). The major drawback of this method is that the modification of these binary files with a distributed platform tool, such as an integrated development environment (IDE) or Notepad, will be extremely difficult. As these files were not converted to UTF-8, their contents will still be in EBCDIC, which is rarely supported outside of the z/OS platform. Browsing or editing the content of these files would require to bring them back to the z/OS platform. Still, this option could be a valid strategy for files that rarely (or never) change, but this implies setting up a strategy when a modification is required (that is, these files will need to be modified on z/OS, not using a distributed solution - this strategy should remain as an exception).\nThe alternative is to opt for a transformation of the content that contains non-printable or non-roundtripable characters before migrating the files. Programmers have often employed elaborate strategies based on the use of these characters in literals or constants. Through the ISPF editor, developers have been able to insert hexadecimal values (when enabling the editing of raw data with the hex on feature) in their source code. This use case is very common and can easily be corrected by replacing the hexadecimal coded characters with their corresponding hexadecimal values as shown in the picture below (taken from a CICS COBOL copybook):\n\nInitial values:\n\n\n\nInitial values\n\n\nFinal values after transformation:\n\n\n\nFinal values after transformation\n\n\n\nThis transformation could even be automated (albeit cautiously) through scripting. Other use cases for these characters should be analyzed carefully, and developers should be encouraged to write their source code in a way that allows for code page conversion.\nFortunately, IBM Dependency Based Build (DBB) provides a feature in its migration script to detect these special characters and helps manage their code page conversion or their transfer as binary.\nIn any case, the decision about using binary-tagged files in Git, refactoring these files to transform the non-printable and non-roundtripable characters into their hex values, or not changing the content of the files should be taken prior to performing the final migration from datasets to Git repositories. If the migration is started with files that contain either non-printable or non-roundtripable characters, there is a high risk that files cannot be edited using distributed editors. Once the files are migrated, it is often very difficult to resolve the situation after the fact, as there can be information lost during the code page conversion. In that situation, the best option is to restart the migration from datasets, assuming the original members are still available until the migration is validated.\n\n\n\n\nSince Git is a distributed system, each member of this “network” should be able to work with the files stored in the Git repositories. Over the last decade, Git has become the de facto standard for the development community, being used in all-sized enterprises, internal, open-source and/or personal development projects. Because Git emerged in the distributed world, it was first designed to support technologies from this ecosystem, and it all started with the file encoding. Although it appeared in the 90’s, UTF-8 only gained notoriety in the 2010’s through the increased interconnectivity between heterogeneous systems. UTF-8 is now recognized as the universal, common language for exchanging data and files. Naturally, UTF-8 also became the standard for encoding source files for the distributed world.\nSince mainframe source resides on the host side under the control of a Source Code Management solution, this source is encoded with an EBCDIC code page and could potentially contain national characters. In order to store these source files in Git, a code page conversion must occur between the z/OS source file EBCDIC code page and UTF-8. This is the role of the Rocket Git client on z/OS to perform that conversion, which is transparent for the developers.\nHowever, the Rocket Git client on z/OS must be instructed on the conversion operation to perform. These instructions are defined in a specific file that belongs to the Git repository. The .gitattributes file is a standard file used by Git to declare specific attributes for the Git Repository that it belongs to. With the Rocket Git client on z/OS, it has been extended to describe the code page that should be used for files when they are stored in Git and on z/OS UNIX System Services (z/OS UNIX).\nThe structure of the file is straightforward. One or several attributes are associated to each path and file extension listed in the file. Wildcards can be used for generic definitions. This file must be encoded in ASCII (ISO-8859-1), as it is processed by Git on z/OS and other Git platforms on the distributed side. Here is an example of such a file, which leverages two parameters: zos-working-tree-encoding and git-encoding:\n# line endings\n* text=auto eol=lf\n\n# file encodings\n*.cpy zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.cbl zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.bms zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.pli zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.mfs zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.bnd zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.lnk zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.txt zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.groovy zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.sh zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.properties zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.asm zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.jcl zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.mac zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.json zos-working-tree-encoding=utf-8 git-encoding=utf-8\nThe zos-working-tree-encoding parameter specifies the code page used for encoding files on z/OS UNIX and in PDSs. It must be consistent with file tags for the files that are under the control of Git. The git-encoding parameter specifies the encoding for files stored in Git outside of the z/OS platform. The typical value for this parameter is UTF-8.\nThis file is used by the Rocket Git client to understand which conversion process must be performed when files are added to Git and when files are transferred between a Git repository and the z/OS platform. This file plays an important role in the migration process of source files from PDSs to Git repositories, and its content must be thoroughly planned.\nAn example for this file can be found in the dbb-zappbuild repository. This sample contains the major definitions that are typically found in the context of a migration project.\n\n\n\nDuring the migration of source code files from z/OS PDSs to Git, a transitional state exists as the source members are copied to z/OS Unix System Services (z/OS UNIX). This step is important and must be correctly performed to ensure the files are imported into Git in the right way.\nz/OS UNIX supports the use of different code pages for the files stored on zFS filesystems. As z/OS UNIX introduced the use of ASCII in the z/OS environment, there are some concepts to understand before diving into this section. These concepts are grouped under the term “Enhanced ASCII” in the z/OS documentation.\nFiles on z/OS UNIX are just a sequence of bytes, like members of PDSs. When members are copied from PDSs to files on z/OS UNIX, the raw content is not altered, and the byte sequence doesn’t change. However, since z/OS UNIX can deal with files coming from distributed systems, which are often encoded in ASCII, additional parameters are introduced to facilitate the management of the file encoding.\n\n\nOne of the additional parameters is the tagging of files to help determine the nature of the content. By default, files are not tagged on z/OS UNIX, and their content is assumed to be encoded in IBM-1047 by most of the z/OS utilities, unless stated otherwise. If the original content of those files is created with a different code page, some characters may not be rendered or read by programs properly.\nA required step to ensure that files are correctly read and processed is to tag them. The tagging of files on z/OS UNIX is controlled by the chtag command. With chtag, you can print the current tagging information for a file (-p option, or the ls -T command also displays the tag of listed files) or change this tagging (-t/-c/-b/-m options). It is important to understand that this tagging information is then leveraged during the migration process to Git by the Rocket Git client, which uses that information to correctly convert the file to the standard Git encoding of UTF-8. Having the correct tagging set for files on z/OS UNIX is a major step in the migration process to ensure a successful conversion.\nThe following example shows the output of the `ls -alT`` command for a folder that contains COBOL source code:\nls -alT\ntotal 448\n                    drwxr-xr-x   2 USER1    OMVS        8192 Feb 22 14:47 .\n                    drwxr-xr-x   7 USER1    OMVS        8192 Jan 12 14:06 ..\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        8930 Feb 22 13:04 epscmort.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS      132337 Jan 12 14:06 epscsmrd.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        7919 Feb 22 13:04 epsmlist.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        5854 Jan 12 14:06 epsmpmt.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        6882 Jan 12 14:06 epsnbrvl.cbl\n\n\n\nAlthough not directly involved in the migration process, the Enhanced ASCII feature introduces an in-flight automatic conversion for files stored in z/OS UNIX. This automatic conversion is mainly controlled by the _BPXK_AUTOCVT environment variable or by the AUTOCVT parameter defined in a BPXPRMxx PARMLIB member. By default, programs on z/OS UNIX are operating in EBCDIC. When the _BPXK_AUTOCVT parameter is activated to either ON or ALL, along with the correct tagging of files, programs executing in z/OS UNIX can transparently and seamlessly work with ASCII files without converting them to EBCDIC.\nDuring the migration process to Git, the .gitattributes file is used to describe the conversion format for all the files under the control of Git. To avoid any manual tasks, it is recommended to enable the automatic conversion for any thread working with Git on z/OS and the .gitattributes file. This file is discussed in further detail in the section Defining the code page of files in Git.\nOther options can interfere in the automatic conversion process. Without being exhaustive, this list provides some parameters which can also influence the global behavior of tools ported to z/OS, including the Rocket Git client for z/OS:\n\n\n\n\n\n\n\nParameter\nDocumentation\n\n\n\n\n_TAG_REDIR_IN _TAG_REDIR_OUT _TAG_REDIR_ERR\nhttps://www-40.ibm.com/servers/resourcelink/svc00100.nsf/pages/zosV2R4SA232280/$file/bpxa500_v2r4.pdf\n\n\nFILETAG\nhttps://www.ibm.com/docs/en/zos/2.3.0?topic=options-filetag-cc-only\n\n\n\nThese parameters can affect the overall behavior of the z/OS UNIX environment, so they should be configured with caution. Detailing the impacts of these parameters is outside the scope of this document. For more information on how they can affect your configuration, please reference the above documentation. Additionally, the recommended values for these parameters are described in the DBB configuration documentation page.\n\n\n\n\nIBM® Dependency Based Build (DBB) provides a migration tool to help facilitate the copying of source artifacts to Git from your z/OS development software configuration management (SCM). This tool can be used to copy the source code contained in your z/OS partitioned data sets (PDSs) to a local Git repository on z/OS UNIX that will later be committed and pushed to a distributed Git server. The DBB Migration Tool has built-in functionality that automates the detection of non-printable and non-roundtripable characters, copies the files to z/OS UNIX, tags them on z/OS UNIX, and generates a .gitattributes file. More information on the DBB Migration Tool and its usage in common migration scenarios can be found in DBB Migration Tool.\n\n\n\nThis document highlighted some pitfalls to avoid when migrating source code from z/OS PDSs to Git: correctly determining the original code page used when editing and reading source code members in z/OS is a key activity to ensure a smooth migration of these elements to Git. The second aspect is about managing the specific EBCDIC characters which are not easily converted to their UTF-8 counterparts. For these specific characters, known as non-printable and non-roundtripable characters, a decision must be taken to either refactor the source code to eliminate those characters, or transfer the files as binary. Both options have drawbacks that should be evaluated prior to the final migration to Git, as there is no easy way back.\nThe DBB Migration Tool shipped with IBM Dependency Based Build helps perform this migration activity by automating the detection of non-printable and non-roundtripable characters, copying the files to z/OS UNIX, tagging them on z/OS UNIX, and generating a .gitattributes file. You can learn more about this utility in DBB Migration Tool.\n\n\n\nThis page contains reformatted excerpts from Managing the code page conversion when migrating z/OS source files to Git.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#introduction",
    "href": "managing-code-page-conversion.html#introduction",
    "title": "Managing code page conversion",
    "section": "",
    "text": "Choosing Git to host enterprise-level software assets implies a migration task of the z/OS artifacts. Whether the source code is under the control of a legacy SCM solution or not, these artifacts are typically stored as members within partitioned data sets (PDSs) with a specific encoding. Depending on locale, country languages, regulations or work habits, the encoding of the source code can differ by various specificities such as accents, locale typography and characters. To support these peculiarities, many different EBCDIC code pages were developed in z/OS and are now widely used.\nIn the Git world, the standard prevailing encoding is UTF-8, which supports most (if not all) of the characters used in printing worldwide. When migrating artifacts from z/OS to Git, a translation must occur to convert the EBCDIC-encoded content of source files to UTF-8. This process is transparent to the user, whereby the translation is performed by the Rocket Git client, as long as the necessary information on how to perform the conversion is provided. Git uses a specific file called .gitattributes to describe and dictate the translation behavior. This page contains details on the encoding that should be used when hosting the artifacts on z/OS or on Git repositories.\nThis page will highlight some specific migration situations and some cautionary commentary that must be taken into account in order to ensure a successful migration of z/OS artifacts to Git. The purpose of this document is not to cover all the country-related specificities, nor the complex encodings found worldwide. Rather it is aimed at presenting general use cases and how to prevent/circumvent potential pitfalls.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#understanding-the-code-page-translation-process",
    "href": "managing-code-page-conversion.html#understanding-the-code-page-translation-process",
    "title": "Managing code page conversion",
    "section": "",
    "text": "Code page translation has always been a challenge since z/OS is exchanging data with the distributed systems. To completely understand the effect of encoding, it is important to take a step back and analyze the binary content of files. This analysis will help to better visualize the transformation process that is occurring behind the scenes\n\n\nA file itself can be summarized as being a series of bytes, often expressed in the hexadecimal notation (from x’00’ to x’FF’). This file can represent binary content which is not meant to be human-readable (like an executable file), or text content. For the latter and from a user standpoint, this series of bytes doesn’t mean anything if the content is not associated with a code page, which will link each byte to a printable character (or a control character). Printable characters are the formation of a character set.\nThe combination of the contents of a file and the code page used defines the human-readable content. That said, the contents of a file will be displayed slightly differently when combined with different code pages. Let’s explore a couple of examples using the ISPF Editor with the “HEX VERT” option enabled to show the hexadecimal values of a file.\nConsider the following example where a file containing the French sentence “Bonjour à tous !” (which translates to “Hello everyone!”) is displayed with two different code pages. The file which contains this sentence was saved using IBM-1147 encoding. When the file is then displayed using a different encoding of IBM-1047, although the contents of the file is the same from a binary standpoint, some characters are displayed in a different way:\n\nUsing the IBM-1147 code page:\n\n\n\nUsing the IBM-1147 code page\n\n\nUsing the IBM-1047 code page:\n\n\n\nUsing the IBM-1047 code page\n\n\n\nTo be correctly displayed with the IBM-1047 code page, the contents of this file must be transformed. Please note how the hexadecimal codes for the à and the ! characters changed, respectively from x’7C’ to x’44’ and from x’4F’ to x’5A’:\n\nTransforming to the IBM-1047 code page:\n\n\n\nTransforming to the IBM-1047 code page\n\n\n\nIt is important to understand that the code page plays a determining role not only when displaying a file, but also when editing it. To ensure the content of a file is consistent when used by different people, the code page used for editing and displaying will likely be the same for all the users. If Alice edits a file with the IBM-1147 code page and introduces characters (like accents) specific to that code page, then Bob will need to use the IBM-1147 code page to display the content of this file. Otherwise, he may experience the situation depicted earlier, where accents are not displayed correctly. If Bob uses the IBM-1047 code page to edit the file and change the content to replace the special characters by the corresponding accents, then Alice will not be able to display the content of this file like she entered it on her terminal.\nConsider this next example. A program file, such as C/370, employs the left ([) and right (]) brackets in some of the language statement constructs. In many US-based legacy applications, the original encoding used to create source files was IBM-037. If these files are then displayed using a different encoding of IBM-1047, again, although the content of the files is the same from a binary standpoint, the brackets are not displayed the correct way:\n\nUsing the IBM-037 code page:\n\n\n\nUsing the IBM-1147 code page\n\n\nUsing the IBM-1047 code page:\n\n\n\nUsing the IBM-1047 code page\n\n\n\nTo be correctly displayed with the IBM-1047 code page, the contents of this file must be transformed. The hexadecimal codes for the [ and the ] characters must be changed from x’BA’ and x’BB’ to x’AD’ and x’BD’, respectively:\n\nTransforming to the IBM-1047 code page:\n\n\n\nTransforming to the IBM-1047 code page\n\n\n\nAgain, it is very important that anyone and everyone who displays or edits the file uses a consistent code page. This can sometimes be a challenge, as the code page to be used is generally specified in the 3270 Emulator (TN3270) client session set-up. Another challenge is trying to determine the original encoding used to create the file.\nTo summarize, the binary content of a file must be transformed to ensure consistency when displayed with another code page. This process is known as the code page translation and is key when migrating your source from your z/OS platform to a distributed platform, which is using different code pages (and most likely today, UTF-8).\n\n\n\nIn this document, we will interchangeably use the coded character set ID (CCSID), its equivalent name, and the code page to designate an encoding. Although not strictly equivalent, they are all often interchanged for the sake of convenience. For instance, the IBM-1047 code page is equivalent to the 1047 coded character set (CCSID) and is usually named IBM-1047. The code page for IBM-037 is equivalent to coded character set (CCSID) 037 and is usually named IBM-037. The CCSID for UTF-8 is 1208 and is linked with many code pages.\nTo simplify, the code pages will be designated by their common name, for instance IBM-037, IBM-1047, IBM-1147, and UTF-8.\nFor reference, a list of code pages is available in the Personal Communications documentation site.\n\n\n\nIf you have ever tried transferring a z/OS data set to a distributed machine running a traditional operating system (like Microsoft Windows or Linux), you probably ended up with similar content when opening the file:\n\n\n\nUnreadable file due to transfer without code page conversion\n\n\nThe content is unreadable because the transfer was performed without a code page conversion. This generally happens when the file was transferred as binary, which leaves the content of the file untouched. Most of the transfer utilities offer the capability to transfer the files as binary or as text. In the latter option, code page conversion occurs, and the file should be readable on distributed platforms.\nThe same conversion must be performed for text files when migrating to Git. Generally, Git uses the UTF-8 code page and, as such, a translation must occur on the content of the transferred files. Likewise, when bringing the files back from Git to z/OS, a backward conversion must be performed to ensure the files are correctly read with the code page in use. Failure in performing this conversion may break the contents of the files, which could become damaged and unreadable. Recovering from this failure is difficult, and the best solution is usually to restart the conversion all over again from the beginning, which often means deleting the files in Git and transferring the same files from z/OS again.\n\n\n\nIn the traditional EBCDIC code pages, some characters exist to control the different devices themselves or the way text should be displayed. These characters are located in the first 64 positions of the EBCDIC characters tables and can be classified into 2 categories: the non-printable characters and the non-roundtripable characters.\n\nNon-printable characters: These characters can be translated from EBCDIC to UTF-8 and back to EBCDIC without breaking the content of the file. However, editing files which contain these characters with editing tools on a distributed platform, can cause display issues. Non-printable characters include all EBCDIC characters below 0x40.\nNon-roundtripable characters: The translation is still possible from EBCDIC to UTF-8, but the translation back to EBCDIC is not possible. In this case, the translated file is no longer identical to its original version. This specific situation must be considered and managed prior to the migration to Git. For these special characters, the conversion from EBCDIC to UTF-8 is generally feasible, but the resulting content of the files may be scrambled or displayed in a wrong way. Moreover, when transferring the files back to z/OS, the files could be damaged, and the transfer could even fail. Non-roundtripable characters include the following:\n\nEBCDIC\n\nNewline: 0x15\nCarriage return: 0x0D\nLine feed: 0x25\nSHIFT_IN/SHIFT_OUT: 0x0F & 0x0E\n\n\n\nIn a migration process, when either non-printable or non-roundtripable characters are found, two options are presented:\n\nKeep these characters “as-is” with the risk of damaging the source files.\nModify the characters to a conversion-friendly format.\n\nFor the first option, it is possible to keep the content of the files intact, but the cost of this is to manage the file as a binary artifact in Git. The benefit of this configuration is that the content of the file is not altered by Git when the file is transferred. Because it is flagged as binary, the contents are not converted to UTF-8 and remain the same across the entire workflow (that is, from z/OS PDS to Git to z/OS PDS). The major drawback of this method is that the modification of these binary files with a distributed platform tool, such as an integrated development environment (IDE) or Notepad, will be extremely difficult. As these files were not converted to UTF-8, their contents will still be in EBCDIC, which is rarely supported outside of the z/OS platform. Browsing or editing the content of these files would require to bring them back to the z/OS platform. Still, this option could be a valid strategy for files that rarely (or never) change, but this implies setting up a strategy when a modification is required (that is, these files will need to be modified on z/OS, not using a distributed solution - this strategy should remain as an exception).\nThe alternative is to opt for a transformation of the content that contains non-printable or non-roundtripable characters before migrating the files. Programmers have often employed elaborate strategies based on the use of these characters in literals or constants. Through the ISPF editor, developers have been able to insert hexadecimal values (when enabling the editing of raw data with the hex on feature) in their source code. This use case is very common and can easily be corrected by replacing the hexadecimal coded characters with their corresponding hexadecimal values as shown in the picture below (taken from a CICS COBOL copybook):\n\nInitial values:\n\n\n\nInitial values\n\n\nFinal values after transformation:\n\n\n\nFinal values after transformation\n\n\n\nThis transformation could even be automated (albeit cautiously) through scripting. Other use cases for these characters should be analyzed carefully, and developers should be encouraged to write their source code in a way that allows for code page conversion.\nFortunately, IBM Dependency Based Build (DBB) provides a feature in its migration script to detect these special characters and helps manage their code page conversion or their transfer as binary.\nIn any case, the decision about using binary-tagged files in Git, refactoring these files to transform the non-printable and non-roundtripable characters into their hex values, or not changing the content of the files should be taken prior to performing the final migration from datasets to Git repositories. If the migration is started with files that contain either non-printable or non-roundtripable characters, there is a high risk that files cannot be edited using distributed editors. Once the files are migrated, it is often very difficult to resolve the situation after the fact, as there can be information lost during the code page conversion. In that situation, the best option is to restart the migration from datasets, assuming the original members are still available until the migration is validated.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#defining-the-code-page-of-files-in-git",
    "href": "managing-code-page-conversion.html#defining-the-code-page-of-files-in-git",
    "title": "Managing code page conversion",
    "section": "",
    "text": "Since Git is a distributed system, each member of this “network” should be able to work with the files stored in the Git repositories. Over the last decade, Git has become the de facto standard for the development community, being used in all-sized enterprises, internal, open-source and/or personal development projects. Because Git emerged in the distributed world, it was first designed to support technologies from this ecosystem, and it all started with the file encoding. Although it appeared in the 90’s, UTF-8 only gained notoriety in the 2010’s through the increased interconnectivity between heterogeneous systems. UTF-8 is now recognized as the universal, common language for exchanging data and files. Naturally, UTF-8 also became the standard for encoding source files for the distributed world.\nSince mainframe source resides on the host side under the control of a Source Code Management solution, this source is encoded with an EBCDIC code page and could potentially contain national characters. In order to store these source files in Git, a code page conversion must occur between the z/OS source file EBCDIC code page and UTF-8. This is the role of the Rocket Git client on z/OS to perform that conversion, which is transparent for the developers.\nHowever, the Rocket Git client on z/OS must be instructed on the conversion operation to perform. These instructions are defined in a specific file that belongs to the Git repository. The .gitattributes file is a standard file used by Git to declare specific attributes for the Git Repository that it belongs to. With the Rocket Git client on z/OS, it has been extended to describe the code page that should be used for files when they are stored in Git and on z/OS UNIX System Services (z/OS UNIX).\nThe structure of the file is straightforward. One or several attributes are associated to each path and file extension listed in the file. Wildcards can be used for generic definitions. This file must be encoded in ASCII (ISO-8859-1), as it is processed by Git on z/OS and other Git platforms on the distributed side. Here is an example of such a file, which leverages two parameters: zos-working-tree-encoding and git-encoding:\n# line endings\n* text=auto eol=lf\n\n# file encodings\n*.cpy zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.cbl zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.bms zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.pli zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.mfs zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.bnd zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.lnk zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.txt zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.groovy zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.sh zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.properties zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.asm zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.jcl zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.mac zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\n*.json zos-working-tree-encoding=utf-8 git-encoding=utf-8\nThe zos-working-tree-encoding parameter specifies the code page used for encoding files on z/OS UNIX and in PDSs. It must be consistent with file tags for the files that are under the control of Git. The git-encoding parameter specifies the encoding for files stored in Git outside of the z/OS platform. The typical value for this parameter is UTF-8.\nThis file is used by the Rocket Git client to understand which conversion process must be performed when files are added to Git and when files are transferred between a Git repository and the z/OS platform. This file plays an important role in the migration process of source files from PDSs to Git repositories, and its content must be thoroughly planned.\nAn example for this file can be found in the dbb-zappbuild repository. This sample contains the major definitions that are typically found in the context of a migration project.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#managing-the-code-page-on-zos-unix-system-services",
    "href": "managing-code-page-conversion.html#managing-the-code-page-on-zos-unix-system-services",
    "title": "Managing code page conversion",
    "section": "",
    "text": "During the migration of source code files from z/OS PDSs to Git, a transitional state exists as the source members are copied to z/OS Unix System Services (z/OS UNIX). This step is important and must be correctly performed to ensure the files are imported into Git in the right way.\nz/OS UNIX supports the use of different code pages for the files stored on zFS filesystems. As z/OS UNIX introduced the use of ASCII in the z/OS environment, there are some concepts to understand before diving into this section. These concepts are grouped under the term “Enhanced ASCII” in the z/OS documentation.\nFiles on z/OS UNIX are just a sequence of bytes, like members of PDSs. When members are copied from PDSs to files on z/OS UNIX, the raw content is not altered, and the byte sequence doesn’t change. However, since z/OS UNIX can deal with files coming from distributed systems, which are often encoded in ASCII, additional parameters are introduced to facilitate the management of the file encoding.\n\n\nOne of the additional parameters is the tagging of files to help determine the nature of the content. By default, files are not tagged on z/OS UNIX, and their content is assumed to be encoded in IBM-1047 by most of the z/OS utilities, unless stated otherwise. If the original content of those files is created with a different code page, some characters may not be rendered or read by programs properly.\nA required step to ensure that files are correctly read and processed is to tag them. The tagging of files on z/OS UNIX is controlled by the chtag command. With chtag, you can print the current tagging information for a file (-p option, or the ls -T command also displays the tag of listed files) or change this tagging (-t/-c/-b/-m options). It is important to understand that this tagging information is then leveraged during the migration process to Git by the Rocket Git client, which uses that information to correctly convert the file to the standard Git encoding of UTF-8. Having the correct tagging set for files on z/OS UNIX is a major step in the migration process to ensure a successful conversion.\nThe following example shows the output of the `ls -alT`` command for a folder that contains COBOL source code:\nls -alT\ntotal 448\n                    drwxr-xr-x   2 USER1    OMVS        8192 Feb 22 14:47 .\n                    drwxr-xr-x   7 USER1    OMVS        8192 Jan 12 14:06 ..\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        8930 Feb 22 13:04 epscmort.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS      132337 Jan 12 14:06 epscsmrd.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        7919 Feb 22 13:04 epsmlist.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        5854 Jan 12 14:06 epsmpmt.cbl\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS        6882 Jan 12 14:06 epsnbrvl.cbl\n\n\n\nAlthough not directly involved in the migration process, the Enhanced ASCII feature introduces an in-flight automatic conversion for files stored in z/OS UNIX. This automatic conversion is mainly controlled by the _BPXK_AUTOCVT environment variable or by the AUTOCVT parameter defined in a BPXPRMxx PARMLIB member. By default, programs on z/OS UNIX are operating in EBCDIC. When the _BPXK_AUTOCVT parameter is activated to either ON or ALL, along with the correct tagging of files, programs executing in z/OS UNIX can transparently and seamlessly work with ASCII files without converting them to EBCDIC.\nDuring the migration process to Git, the .gitattributes file is used to describe the conversion format for all the files under the control of Git. To avoid any manual tasks, it is recommended to enable the automatic conversion for any thread working with Git on z/OS and the .gitattributes file. This file is discussed in further detail in the section Defining the code page of files in Git.\nOther options can interfere in the automatic conversion process. Without being exhaustive, this list provides some parameters which can also influence the global behavior of tools ported to z/OS, including the Rocket Git client for z/OS:\n\n\n\n\n\n\n\nParameter\nDocumentation\n\n\n\n\n_TAG_REDIR_IN _TAG_REDIR_OUT _TAG_REDIR_ERR\nhttps://www-40.ibm.com/servers/resourcelink/svc00100.nsf/pages/zosV2R4SA232280/$file/bpxa500_v2r4.pdf\n\n\nFILETAG\nhttps://www.ibm.com/docs/en/zos/2.3.0?topic=options-filetag-cc-only\n\n\n\nThese parameters can affect the overall behavior of the z/OS UNIX environment, so they should be configured with caution. Detailing the impacts of these parameters is outside the scope of this document. For more information on how they can affect your configuration, please reference the above documentation. Additionally, the recommended values for these parameters are described in the DBB configuration documentation page.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#using-the-dbb-migration-tool",
    "href": "managing-code-page-conversion.html#using-the-dbb-migration-tool",
    "title": "Managing code page conversion",
    "section": "",
    "text": "IBM® Dependency Based Build (DBB) provides a migration tool to help facilitate the copying of source artifacts to Git from your z/OS development software configuration management (SCM). This tool can be used to copy the source code contained in your z/OS partitioned data sets (PDSs) to a local Git repository on z/OS UNIX that will later be committed and pushed to a distributed Git server. The DBB Migration Tool has built-in functionality that automates the detection of non-printable and non-roundtripable characters, copies the files to z/OS UNIX, tags them on z/OS UNIX, and generates a .gitattributes file. More information on the DBB Migration Tool and its usage in common migration scenarios can be found in DBB Migration Tool.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#summary",
    "href": "managing-code-page-conversion.html#summary",
    "title": "Managing code page conversion",
    "section": "",
    "text": "This document highlighted some pitfalls to avoid when migrating source code from z/OS PDSs to Git: correctly determining the original code page used when editing and reading source code members in z/OS is a key activity to ensure a smooth migration of these elements to Git. The second aspect is about managing the specific EBCDIC characters which are not easily converted to their UTF-8 counterparts. For these specific characters, known as non-printable and non-roundtripable characters, a decision must be taken to either refactor the source code to eliminate those characters, or transfer the files as binary. Both options have drawbacks that should be evaluated prior to the final migration to Git, as there is no easy way back.\nThe DBB Migration Tool shipped with IBM Dependency Based Build helps perform this migration activity by automating the detection of non-printable and non-roundtripable characters, copying the files to z/OS UNIX, tagging them on z/OS UNIX, and generating a .gitattributes file. You can learn more about this utility in DBB Migration Tool.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "managing-code-page-conversion.html#resources",
    "href": "managing-code-page-conversion.html#resources",
    "title": "Managing code page conversion",
    "section": "",
    "text": "This page contains reformatted excerpts from Managing the code page conversion when migrating z/OS source files to Git.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Managing code page conversion"
    ]
  },
  {
    "objectID": "branching-appendix.html",
    "href": "branching-appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "In traditional mainframe development workflows, teams follow a practice of overlapping releases. In that situation, the team leverages the main and epic branches following a rolling wave pattern: The team decides which commit/tag of the main codeline will be used to baseline the overlapping release – most likely when the previous release moves into its release stabilization phase. The development phase of the subsequent release then occurs on the epic branch and is merged back into main when entering the stabilization phase. This leads to the below composition of the main and epic workflow:\n\n\n\nGit-based development process with overlapping release development",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Appendix"
    ]
  },
  {
    "objectID": "branching-appendix.html#development-process-variations",
    "href": "branching-appendix.html#development-process-variations",
    "title": "Appendix",
    "section": "",
    "text": "In traditional mainframe development workflows, teams follow a practice of overlapping releases. In that situation, the team leverages the main and epic branches following a rolling wave pattern: The team decides which commit/tag of the main codeline will be used to baseline the overlapping release – most likely when the previous release moves into its release stabilization phase. The development phase of the subsequent release then occurs on the epic branch and is merged back into main when entering the stabilization phase. This leads to the below composition of the main and epic workflow:\n\n\n\nGit-based development process with overlapping release development",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Appendix"
    ]
  },
  {
    "objectID": "scm.html",
    "href": "scm.html",
    "title": "Source code management",
    "section": "",
    "text": "A source code management (SCM) tool manages and stores different versions of your application configuration such as source code files, application-specific configuration data, test cases, and more. It provides capabilities to isolate different development activities and enables parallel development.\nGit is the SCM for the IBM Z DevOps solution.\n\n\nGit is the de facto industry standard SCM for the open source community and is growing in popularity among major organizations. It is a central part of the modern developer’s toolkit, and provides a common SCM tool for hybrid application architectures that can span across components ranging from those implemented in traditional mainframe languages such as COBOL, PL/I, or Assembler, to components for the service layer such as z/OS Connect, and components in Java and Go, to reflect the architecture of the business application.\nGit integrates with most modern DevOps tools and pipeline processes to support the full development lifecycle from continuous integration (CI) to continuous delivery (CD). By migrating to Git as the enterprise SCM, mainframe application development teams can then take advantage of the open source community’s modern tooling.\n\n\n\nThe following diagram draws analogies between legacy mainframe SCM processes and DevOps with Git on z/OS.\nIt shows the key features of an SCM process, starting with source code storage, and ending with the deployment to the development or other upper environments such as Q/A and Production.\n\n\n\nAnalogies between legacy mainframe SCM and DevOps with Git on z/OS\n\n\n\n\n\nGit is a distributed “version control system” for source code. It provides many features to allow developers to check in and check out code with a full history and audit trail for all changes.\nSource is stored in repositories (also known as “repos”) on distributed file systems. An application repo can be copied (or “cloned”) between the central Git server (known as the “remote”) and any machine that has Git, including a developer’s local computer using popular integrated development environments (IDEs) such as IBM Developer for z/OS (IDz) and Microsoft’s Visual Studio Code (VS Code). This “clone” contains all the files and folders in the repository, as well as their complete version histories.\nThe developer can then create working copies of the repository code, which Git calls “branches”. For each task the developer has (such as a bug fix or feature), the developer would generally do their development work on a personal branch dedicated to that task. When they are ready to promote their changes, they can create what Git calls a “pull request” (also known as a “merge request” by some Git providers), which is a request to integrate (or “merge”) those changes back into the team’s common, shared branch of code.\nWith Git’s branching and merging features, changes can be performed in isolation and in parallel with other developer changes. Git is typically hosted by service providers such as GitHub, GitLab, Bitbucket, or Azure Repos. Git providers add valuable features on top of the base Git functionality, such as repository hosting, data storage, and security.\nIn Git, all changes are committed (saved) in a repo using a commit hash (unique identifier) and a descriptive comment. Most IDEs provide a Git history tool to navigate changes and drill down to line-by-line details in Git diff reports. The following image of an Azure Repos example setup shows the Git history on the right panel, and a Git diff report on the left.\n\n\n\nGit history and Git diff in Azure Repos\n\n\n\n\nA Git “branch” is a reference to all the files in a repo at a certain point in time, as well as their history. A normal practice is to create multiple branches in a repo, each for a different purpose. Typically, there will be a “main” branch, which is shared by the development team. The team’s repository administrator(s) will usually set up protections for this branch, requiring approval for any change to be merged into it. The team might also have additional shared branches for different purposes, depending on their branching strategy. The repository administrator(s) can also set up branch protections for these branches, as well as any other branch in the repository.\nAll Git actions are performed on a branch, and a key advantage of Git is that it allows developers to clone a repo and create (check out) a new branch (sometimes called a “feature branch”) to work on their own changes in isolation from the main source branch. This lets each developer focus on their task without having to worry about other developers’ activities disturbing their work or vice versa.\nWhen a developer wants to save their code changes onto a branch in Git, they perform a Git “commit”, which creates a snapshot of the branch with their changes. Git uniquely identifies this snapshot with a commit hash, and attaches a short commit message from the developer describing the changes. The developer (and any other teammates with access to the branch) can then use this commit hash as a point-in-time reference for the set of committed changes. They can later check out the commit hash to view the code at that commit point. Additionally, the code can also be rolled back (or “reverted”, in Git terminology) to any prior commit hash.\n\n\n\nFeature branching allows developers to check out the same code, and work in parallel and in isolation. Git merge is how all the code changes from one branch get integrated into another branch. Once developers complete their feature development, they initiate a pull request asking to integrate their feature changes into the team’s shared branch of code.\nThe pull request process is where development teams can implement peer reviews, allowing team leads or other developers to approve or reject changes. They can also set up other quality gates such as automated testing and code scanning to run on the PR. Git will automatically perform merge conflict detection to prevent the accidental overlaying of changes when the pull request is merged in. Development teams often have a CI pipeline that is triggered to run upon pull request approval/merge for the integration test phase.\n\n\nOne of the biggest benefits of using Git is its merge conflict detection. This is Git’s ability to detect when there are overlaps in the code changes during a merge process, so that developers can stop the merge and resolve the merge conflict. This merge conflict detection means that team members can merge their changes to the same program while avoiding unintentionally overlaying each other’s code.\nTo illustrate this example of parallel development, in the following diagram, Developer 1 (Dev1) and Developer 2 (Dev2) have each created their own feature branch from the same version of their team’s shared branch of code. Note that there are no commits (indicated by purple dots) on the team’s shared branch between when Dev2 and Dev1 created their respective feature branches. Now, each developer can work on their own feature in isolation: Dev1 has his feature1 branch where he is working on his copy of hte code, and Dev2 has her feature2 branch where she is working on her copy of the code.\n\n\n\nDiagram illustrating parallel development use case\n\n\nDoing this kind of parallel development is complicated on legacy systems, especially with PDSs, because developers have to figure out how to merge the code at the end, especially when working on the same files. Additionally, legacy SCMs typically lock files that are being worked on. In contrast, Git branching allows the developers to work on the files at the same time, in parallel.\nIn the Git example illustrated above, Dev1 and Dev2 agreed to work on different parts of the same program, and they then each make their own pull request to integrate their respective chanages back into the team’s shared branch of code when they are ready. Dev1 has done this before Dev2, so his changes have been approved and merged in first. When Dev2 later makes her request to merge her code changes into the team’s shared branch of code, Git does a line-by-line check to make sure the changes proposed in Dev2’s pull request do not conflict with any of the changes in the shared branch of code (which now include Dev1’s changes). If any issues are found, Git will stop the merge and alert the developers of the merge conflict. Git will also highlight the conflicting code so that the developers know where to look and can resolve the conflict, most likely via another commit in Dev2’s branch.\n\n\n\n\nA Git tag is similar to a Git branch, as it references the repo by a specific commit point. However, unlike a branch, a tag can reference only one commit point. Tags are used to label and track releases. They are optional but recommended.\n\n\n\n\n\n\nIt is a common practice that mainframe applications share common code. For example, COBOL copybooks are typically shared across applications that process similar data.\nThe following diagram illustrates how teams can define repos to securely share common code. In this example, App Team 1 has common code that App Team 2 can clone and use in their build.\nAnother example (also illustrated in the following diagram) is that an enterprise-wide team can maintain source that is common across many applications.\n\n\n\nBest practices for sharing code in Git\n\n\n\n\n\n\n\nGitLab\nGitHub\nBitbucket\nAzure Repos\n\n\n\n\nThis page contains reformatted excerpts from Git training for Mainframers.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "scm.html#why-move-to-git",
    "href": "scm.html#why-move-to-git",
    "title": "Source code management",
    "section": "",
    "text": "Git is the de facto industry standard SCM for the open source community and is growing in popularity among major organizations. It is a central part of the modern developer’s toolkit, and provides a common SCM tool for hybrid application architectures that can span across components ranging from those implemented in traditional mainframe languages such as COBOL, PL/I, or Assembler, to components for the service layer such as z/OS Connect, and components in Java and Go, to reflect the architecture of the business application.\nGit integrates with most modern DevOps tools and pipeline processes to support the full development lifecycle from continuous integration (CI) to continuous delivery (CD). By migrating to Git as the enterprise SCM, mainframe application development teams can then take advantage of the open source community’s modern tooling.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "scm.html#git-in-the-software-development-lifecycle",
    "href": "scm.html#git-in-the-software-development-lifecycle",
    "title": "Source code management",
    "section": "",
    "text": "The following diagram draws analogies between legacy mainframe SCM processes and DevOps with Git on z/OS.\nIt shows the key features of an SCM process, starting with source code storage, and ending with the deployment to the development or other upper environments such as Q/A and Production.\n\n\n\nAnalogies between legacy mainframe SCM and DevOps with Git on z/OS",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "scm.html#git-basics",
    "href": "scm.html#git-basics",
    "title": "Source code management",
    "section": "",
    "text": "Git is a distributed “version control system” for source code. It provides many features to allow developers to check in and check out code with a full history and audit trail for all changes.\nSource is stored in repositories (also known as “repos”) on distributed file systems. An application repo can be copied (or “cloned”) between the central Git server (known as the “remote”) and any machine that has Git, including a developer’s local computer using popular integrated development environments (IDEs) such as IBM Developer for z/OS (IDz) and Microsoft’s Visual Studio Code (VS Code). This “clone” contains all the files and folders in the repository, as well as their complete version histories.\nThe developer can then create working copies of the repository code, which Git calls “branches”. For each task the developer has (such as a bug fix or feature), the developer would generally do their development work on a personal branch dedicated to that task. When they are ready to promote their changes, they can create what Git calls a “pull request” (also known as a “merge request” by some Git providers), which is a request to integrate (or “merge”) those changes back into the team’s common, shared branch of code.\nWith Git’s branching and merging features, changes can be performed in isolation and in parallel with other developer changes. Git is typically hosted by service providers such as GitHub, GitLab, Bitbucket, or Azure Repos. Git providers add valuable features on top of the base Git functionality, such as repository hosting, data storage, and security.\nIn Git, all changes are committed (saved) in a repo using a commit hash (unique identifier) and a descriptive comment. Most IDEs provide a Git history tool to navigate changes and drill down to line-by-line details in Git diff reports. The following image of an Azure Repos example setup shows the Git history on the right panel, and a Git diff report on the left.\n\n\n\nGit history and Git diff in Azure Repos\n\n\n\n\nA Git “branch” is a reference to all the files in a repo at a certain point in time, as well as their history. A normal practice is to create multiple branches in a repo, each for a different purpose. Typically, there will be a “main” branch, which is shared by the development team. The team’s repository administrator(s) will usually set up protections for this branch, requiring approval for any change to be merged into it. The team might also have additional shared branches for different purposes, depending on their branching strategy. The repository administrator(s) can also set up branch protections for these branches, as well as any other branch in the repository.\nAll Git actions are performed on a branch, and a key advantage of Git is that it allows developers to clone a repo and create (check out) a new branch (sometimes called a “feature branch”) to work on their own changes in isolation from the main source branch. This lets each developer focus on their task without having to worry about other developers’ activities disturbing their work or vice versa.\nWhen a developer wants to save their code changes onto a branch in Git, they perform a Git “commit”, which creates a snapshot of the branch with their changes. Git uniquely identifies this snapshot with a commit hash, and attaches a short commit message from the developer describing the changes. The developer (and any other teammates with access to the branch) can then use this commit hash as a point-in-time reference for the set of committed changes. They can later check out the commit hash to view the code at that commit point. Additionally, the code can also be rolled back (or “reverted”, in Git terminology) to any prior commit hash.\n\n\n\nFeature branching allows developers to check out the same code, and work in parallel and in isolation. Git merge is how all the code changes from one branch get integrated into another branch. Once developers complete their feature development, they initiate a pull request asking to integrate their feature changes into the team’s shared branch of code.\nThe pull request process is where development teams can implement peer reviews, allowing team leads or other developers to approve or reject changes. They can also set up other quality gates such as automated testing and code scanning to run on the PR. Git will automatically perform merge conflict detection to prevent the accidental overlaying of changes when the pull request is merged in. Development teams often have a CI pipeline that is triggered to run upon pull request approval/merge for the integration test phase.\n\n\nOne of the biggest benefits of using Git is its merge conflict detection. This is Git’s ability to detect when there are overlaps in the code changes during a merge process, so that developers can stop the merge and resolve the merge conflict. This merge conflict detection means that team members can merge their changes to the same program while avoiding unintentionally overlaying each other’s code.\nTo illustrate this example of parallel development, in the following diagram, Developer 1 (Dev1) and Developer 2 (Dev2) have each created their own feature branch from the same version of their team’s shared branch of code. Note that there are no commits (indicated by purple dots) on the team’s shared branch between when Dev2 and Dev1 created their respective feature branches. Now, each developer can work on their own feature in isolation: Dev1 has his feature1 branch where he is working on his copy of hte code, and Dev2 has her feature2 branch where she is working on her copy of the code.\n\n\n\nDiagram illustrating parallel development use case\n\n\nDoing this kind of parallel development is complicated on legacy systems, especially with PDSs, because developers have to figure out how to merge the code at the end, especially when working on the same files. Additionally, legacy SCMs typically lock files that are being worked on. In contrast, Git branching allows the developers to work on the files at the same time, in parallel.\nIn the Git example illustrated above, Dev1 and Dev2 agreed to work on different parts of the same program, and they then each make their own pull request to integrate their respective chanages back into the team’s shared branch of code when they are ready. Dev1 has done this before Dev2, so his changes have been approved and merged in first. When Dev2 later makes her request to merge her code changes into the team’s shared branch of code, Git does a line-by-line check to make sure the changes proposed in Dev2’s pull request do not conflict with any of the changes in the shared branch of code (which now include Dev1’s changes). If any issues are found, Git will stop the merge and alert the developers of the merge conflict. Git will also highlight the conflicting code so that the developers know where to look and can resolve the conflict, most likely via another commit in Dev2’s branch.\n\n\n\n\nA Git tag is similar to a Git branch, as it references the repo by a specific commit point. However, unlike a branch, a tag can reference only one commit point. Tags are used to label and track releases. They are optional but recommended.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "scm.html#best-practices",
    "href": "scm.html#best-practices",
    "title": "Source code management",
    "section": "",
    "text": "It is a common practice that mainframe applications share common code. For example, COBOL copybooks are typically shared across applications that process similar data.\nThe following diagram illustrates how teams can define repos to securely share common code. In this example, App Team 1 has common code that App Team 2 can clone and use in their build.\nAnother example (also illustrated in the following diagram) is that an enterprise-wide team can maintain source that is common across many applications.\n\n\n\nBest practices for sharing code in Git",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "scm.html#common-git-provider-options",
    "href": "scm.html#common-git-provider-options",
    "title": "Source code management",
    "section": "",
    "text": "GitLab\nGitHub\nBitbucket\nAzure Repos",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "scm.html#resources",
    "href": "scm.html#resources",
    "title": "Source code management",
    "section": "",
    "text": "This page contains reformatted excerpts from Git training for Mainframers.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Source code management"
    ]
  },
  {
    "objectID": "repository-layouts.html",
    "href": "repository-layouts.html",
    "title": "Planning repository layouts",
    "section": "",
    "text": "Because the traditionally large and monolithic nature of enterprise applications can slow down development both at the organizational level and the individual developer level, many enterprises take the migration to Git as an opportunity to examine how they can break their monolithic applications down into more agile functional Git repositories. The repository layout in the source code management (SCM) component of the continuous integration/continuous delivery (CI/CD) pipeline affects other pipeline components in several ways:\n\nRepository layout affects the developer experience, as it impacts the level of isolation for development activities - in other words, how much of your development can happen independently of code or components managed in other repositories.\nThe scope of the build is typically defined by what is included inside a repository versus outside of it. For shared resources such as copybooks, other repositories might need to be pulled in.\nThe scope of the package sent to the artifact repository will depend on the outputs produced by the build step.\n\n\n\nAt a high level, the Git SCM can be thought of in two components: the central Git provider, which is the source from which developers will be pulling code from and pushing code to when they do their development work, and the clone on the developer workstation.\nWithin the central Git provider, there can be multiple organizations:\n\nEach organization can have multiple repositories inside it.\nIn turn, each repository can have multiple branches (or versions) of the code in the repository.\nEach branch has a full copy of the files and folders in the codebase, allowing each developer to focus on their development task in isolation on a dedicated branch, only needing to worry about upstream and downstream impacts when they are ready to integrate those with their work.\n\nOn the developer’s workstation, the developer will have however many repositories they have decided to clone down onto their local computer to work with. Each local repository clone has one or more branches that they have checked out (or in other words, selected to work with). And once again, each branch will have a full copy of the files and folders in the codebase.\n\n\n\nGit is a distributed version control system, so rather than the files all staying in the mainframe during development, developers will clone the repository they are interested in down to their local workstation. Your organization’s administrator(s) can control who has what permissions, or which repositories each developer has access to.\nWorking with a distributed version control system like Git also means that when planning the SCM layout, one factor to consider is size. Generally, you want to avoid doing something like migrating the entire codebase of a large application into one repository, because that will mean that every time your developers want to clone or update the repository, it will take a long time to transfer all the code, which will make development feel slow and cumbersome rather than light and agile.\n\n\n\nTraditionally, mainframe applications are often seen as a monolith, but as mentioned previously, it is generally not preferable to have all of your codebase in one giant repository. To begin thinking about how to separate an application “monolith” into different repositories and designing your SCM layout, you can start to think about boundaries within your application, which are formed by things such as responsibilities and ownership.\nTo use an analogy, you can think of the application like a large capital city. On the surface, it might seem like this city is one big entity - a big monolith. However, upon closer inspection, the city is broken up into districts. And while each district has its own workings (for example, each district has its own school system), all of these districts are connected by a broader infrastructure in a controlled way. There might be public transport, such as a metro or buses, which run on predefined paths and schedules. We can think of this transport infrastructure like the data flowing between application boundaries within a larger system.\nIn a similar sense, the districts within the city could be likened to groupings of related files and programs, which can be formed into their own smaller applications (in a loose sense of the term) with data flowing between them.\nThe following diagram illustrates the point more literally. In this example, there are three applications, which we can again think of like the city districts:\n\n\n\nThree applications with interfaces\n\n\n\nEach application has its own inner workings in the private implementation (represented by the tan parts of the above diagram). In the city analogy, an example of this could be the school system each district has. In the mainframe context, an example of these inner workings could be the COBOL programs for that application.\nAdditionally, each application needs to be able to communicate with other applications, which is accomplished via its public interface (shown in blue on the diagram). Using the city analogy again, the bus stop for each district could be considered as the district’s “public interface”. In the mainframe context, the public interface could be the copybooks defining data structures that are passed between programs.\nBecause each application needs to know how to interact with the other applications, the application also includes usage of interfaces from the other applications (shown in red on the diagram). This can be likened to the application needing to know the bus stops and schedule from the other applications.\n\nTypically, with this layout, each application would have its own Git repository with its own scope. Compared to putting the whole system in a single large Git repository, having these smaller repositories enables more agile development practices. Companies choose a central, web-based repository server where they can manage their repositories, and from which their developers will clone from (for example, Git providers such as GitHub, GitLab, Bitbucket, Azure Repos, and so on). The following section provides some guidance to consider when breaking down a monolithic codebase to achieve this kind of setup.\n\n\n\nThe guidelines in this section can be used when considering how to cluster your codebase into Git repositories in a way that makes sense and encourages a streamlined workflow for your organization. There are several factors to balance when scoping repositories, including the following:\n\nAccess control\n\nConsider who (or which teams) need access to which files. Read/write, read-only, and restricted permissions need to be preserved, and this can be achieved by separating the files into different Git repositories and then managing access accordingly through your Git provider.\n\nFunctional areas of the code\n\nIn your current system, if different teams work on different components of your application, it might make sense to make boundaries based on that. An individual can be part of one or several teams.\n\nDecoupling\n\nConsider decoupling applications where it makes sense to do so. This allows different functional areas to have separate release cycles, which can help promote more agility in the overall organization.\n\nInterfaces\n\nWhen scoping for Git repository layouts, consider if or how changes performed by a team in one repository might impact other teams. In other words, within a team, if a developer on that team makes a breaking change to their repository (for example, to the application’s private implementation), they can just work with their team to fix it. However, if a developer makes breaking changes impacting another team (for example, to the application’s public interface), then resolving it can become more complicated.\n\nSize\n\nConsider the scope of changes for a typical change request (or pull request/merge request). For example, it is typically preferable to avoid change requests spanning across multiple repositories. Git providers usually have built-in review and approval mechanisms, so it is recommended to set up the repository layout in a way where these approval and review systems will make sense.\n\nPerformance\n\nGit performance can be a consideration, but it should not be the primary driver to make decisions when organizing repository layouts.\n\n\n\n\n\nOne of the challenges in designing the SCM layout is managing dependencies between applications. For mainframe applications, this means needing to think about how the files are arranged. If a program and its subprogram are located in the same application, then it will be an internal dependency. If they are located in different applications, then the dependency will be external.\nIn distributed development, for example with Java, you can reference other external applications by referencing their application programming interfaces (APIs). However, with COBOL (and other mainframe languages), the build process needs to physically pull in the external dependencies as source, via concatenation. Therefore, this is something you will want to consider when designing your SCM layout.\nTo help guide the SCM layout design process, we can approach the SCM design with a two-step process:\n\nDefine which files make up each application: This is like defining the “districts” in your city, if we go back to the city analogy. You can consider looking at boundaries that have already naturally arisen based on ownership and responsibility around different parts of the codebase, in addition to the other guidelines for repository scope.\n\nYou might already have an idea of which files make up each application based on a data dictionary.\nIBM Application Discovery & Delivery Intelligence (ADDI) can assist in visualizing call graphs and clusters of dependencies in your codebase. These visualizations can be useful when you are analyzing the code to figure out how to organize it.\n\nUnderstand the internal versus external elements (such as copybooks) for each application: Here, the objective is to identify interfaces and shared items across application scopes - in other words, we want to determine which application is using which copybooks. In the city analogy, this is like identifying the bus stop names. Based on the repository layout, you can minimize how much changes in one repository will impact other repositories (and in turn, other teams and applications). Ideally, you will end up with files that have a lot of references within the same repository.\n\nThe result of the analysis in the steps above can be reflected within each application’s repository. In the following example, you can see that the file structure is organized to indicate private versus public files and interfaces.\n\n\n\nAn application repository with private and public interfaces organized into different folders\n\n\nMore details about managing dependencies can be found in the documentation for Defining dependency management.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Planning repository layouts"
    ]
  },
  {
    "objectID": "repository-layouts.html#basic-components-in-a-git-scm-layout",
    "href": "repository-layouts.html#basic-components-in-a-git-scm-layout",
    "title": "Planning repository layouts",
    "section": "",
    "text": "At a high level, the Git SCM can be thought of in two components: the central Git provider, which is the source from which developers will be pulling code from and pushing code to when they do their development work, and the clone on the developer workstation.\nWithin the central Git provider, there can be multiple organizations:\n\nEach organization can have multiple repositories inside it.\nIn turn, each repository can have multiple branches (or versions) of the code in the repository.\nEach branch has a full copy of the files and folders in the codebase, allowing each developer to focus on their development task in isolation on a dedicated branch, only needing to worry about upstream and downstream impacts when they are ready to integrate those with their work.\n\nOn the developer’s workstation, the developer will have however many repositories they have decided to clone down onto their local computer to work with. Each local repository clone has one or more branches that they have checked out (or in other words, selected to work with). And once again, each branch will have a full copy of the files and folders in the codebase.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Planning repository layouts"
    ]
  },
  {
    "objectID": "repository-layouts.html#working-with-git",
    "href": "repository-layouts.html#working-with-git",
    "title": "Planning repository layouts",
    "section": "",
    "text": "Git is a distributed version control system, so rather than the files all staying in the mainframe during development, developers will clone the repository they are interested in down to their local workstation. Your organization’s administrator(s) can control who has what permissions, or which repositories each developer has access to.\nWorking with a distributed version control system like Git also means that when planning the SCM layout, one factor to consider is size. Generally, you want to avoid doing something like migrating the entire codebase of a large application into one repository, because that will mean that every time your developers want to clone or update the repository, it will take a long time to transfer all the code, which will make development feel slow and cumbersome rather than light and agile.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Planning repository layouts"
    ]
  },
  {
    "objectID": "repository-layouts.html#visualizing-the-scm-layout",
    "href": "repository-layouts.html#visualizing-the-scm-layout",
    "title": "Planning repository layouts",
    "section": "",
    "text": "Traditionally, mainframe applications are often seen as a monolith, but as mentioned previously, it is generally not preferable to have all of your codebase in one giant repository. To begin thinking about how to separate an application “monolith” into different repositories and designing your SCM layout, you can start to think about boundaries within your application, which are formed by things such as responsibilities and ownership.\nTo use an analogy, you can think of the application like a large capital city. On the surface, it might seem like this city is one big entity - a big monolith. However, upon closer inspection, the city is broken up into districts. And while each district has its own workings (for example, each district has its own school system), all of these districts are connected by a broader infrastructure in a controlled way. There might be public transport, such as a metro or buses, which run on predefined paths and schedules. We can think of this transport infrastructure like the data flowing between application boundaries within a larger system.\nIn a similar sense, the districts within the city could be likened to groupings of related files and programs, which can be formed into their own smaller applications (in a loose sense of the term) with data flowing between them.\nThe following diagram illustrates the point more literally. In this example, there are three applications, which we can again think of like the city districts:\n\n\n\nThree applications with interfaces\n\n\n\nEach application has its own inner workings in the private implementation (represented by the tan parts of the above diagram). In the city analogy, an example of this could be the school system each district has. In the mainframe context, an example of these inner workings could be the COBOL programs for that application.\nAdditionally, each application needs to be able to communicate with other applications, which is accomplished via its public interface (shown in blue on the diagram). Using the city analogy again, the bus stop for each district could be considered as the district’s “public interface”. In the mainframe context, the public interface could be the copybooks defining data structures that are passed between programs.\nBecause each application needs to know how to interact with the other applications, the application also includes usage of interfaces from the other applications (shown in red on the diagram). This can be likened to the application needing to know the bus stops and schedule from the other applications.\n\nTypically, with this layout, each application would have its own Git repository with its own scope. Compared to putting the whole system in a single large Git repository, having these smaller repositories enables more agile development practices. Companies choose a central, web-based repository server where they can manage their repositories, and from which their developers will clone from (for example, Git providers such as GitHub, GitLab, Bitbucket, Azure Repos, and so on). The following section provides some guidance to consider when breaking down a monolithic codebase to achieve this kind of setup.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Planning repository layouts"
    ]
  },
  {
    "objectID": "repository-layouts.html#guidelines-for-git-repository-scope",
    "href": "repository-layouts.html#guidelines-for-git-repository-scope",
    "title": "Planning repository layouts",
    "section": "",
    "text": "The guidelines in this section can be used when considering how to cluster your codebase into Git repositories in a way that makes sense and encourages a streamlined workflow for your organization. There are several factors to balance when scoping repositories, including the following:\n\nAccess control\n\nConsider who (or which teams) need access to which files. Read/write, read-only, and restricted permissions need to be preserved, and this can be achieved by separating the files into different Git repositories and then managing access accordingly through your Git provider.\n\nFunctional areas of the code\n\nIn your current system, if different teams work on different components of your application, it might make sense to make boundaries based on that. An individual can be part of one or several teams.\n\nDecoupling\n\nConsider decoupling applications where it makes sense to do so. This allows different functional areas to have separate release cycles, which can help promote more agility in the overall organization.\n\nInterfaces\n\nWhen scoping for Git repository layouts, consider if or how changes performed by a team in one repository might impact other teams. In other words, within a team, if a developer on that team makes a breaking change to their repository (for example, to the application’s private implementation), they can just work with their team to fix it. However, if a developer makes breaking changes impacting another team (for example, to the application’s public interface), then resolving it can become more complicated.\n\nSize\n\nConsider the scope of changes for a typical change request (or pull request/merge request). For example, it is typically preferable to avoid change requests spanning across multiple repositories. Git providers usually have built-in review and approval mechanisms, so it is recommended to set up the repository layout in a way where these approval and review systems will make sense.\n\nPerformance\n\nGit performance can be a consideration, but it should not be the primary driver to make decisions when organizing repository layouts.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Planning repository layouts"
    ]
  },
  {
    "objectID": "repository-layouts.html#managing-dependencies-between-applications",
    "href": "repository-layouts.html#managing-dependencies-between-applications",
    "title": "Planning repository layouts",
    "section": "",
    "text": "One of the challenges in designing the SCM layout is managing dependencies between applications. For mainframe applications, this means needing to think about how the files are arranged. If a program and its subprogram are located in the same application, then it will be an internal dependency. If they are located in different applications, then the dependency will be external.\nIn distributed development, for example with Java, you can reference other external applications by referencing their application programming interfaces (APIs). However, with COBOL (and other mainframe languages), the build process needs to physically pull in the external dependencies as source, via concatenation. Therefore, this is something you will want to consider when designing your SCM layout.\nTo help guide the SCM layout design process, we can approach the SCM design with a two-step process:\n\nDefine which files make up each application: This is like defining the “districts” in your city, if we go back to the city analogy. You can consider looking at boundaries that have already naturally arisen based on ownership and responsibility around different parts of the codebase, in addition to the other guidelines for repository scope.\n\nYou might already have an idea of which files make up each application based on a data dictionary.\nIBM Application Discovery & Delivery Intelligence (ADDI) can assist in visualizing call graphs and clusters of dependencies in your codebase. These visualizations can be useful when you are analyzing the code to figure out how to organize it.\n\nUnderstand the internal versus external elements (such as copybooks) for each application: Here, the objective is to identify interfaces and shared items across application scopes - in other words, we want to determine which application is using which copybooks. In the city analogy, this is like identifying the bus stop names. Based on the repository layout, you can minimize how much changes in one repository will impact other repositories (and in turn, other teams and applications). Ideally, you will end up with files that have a lot of references within the same repository.\n\nThe result of the analysis in the steps above can be reflected within each application’s repository. In the following example, you can see that the file structure is organized to indicate private versus public files and interfaces.\n\n\n\nAn application repository with private and public interfaces organized into different folders\n\n\nMore details about managing dependencies can be found in the documentation for Defining dependency management.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Planning repository layouts"
    ]
  },
  {
    "objectID": "ide.html",
    "href": "ide.html",
    "title": "Integrated development environment",
    "section": "",
    "text": "An integrated development environment (IDE) provides check-out and check-in capabilities to the source code management (SCM) system. Typically, it supports the language and coding practices of the developer, enables building the modified code within the sandbox, and drives unit tests as part of the coding phase.\nThe IBM Z DevOps solution is geared towards a “Bring Your Own IDE” approach, meaning we want to enable customers to use the IDE of their preference in our solution.\n\n\n\nEclipse-based: IBM Developer for z/OS (IDz)\nVisual Studio Code (VS Code) extension: IBM Z Open Editor (and IBM Z Open Debug)\nBrowser-based: Wazi for Dev Spaces\n\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Integrated development environment"
    ]
  },
  {
    "objectID": "ide.html#common-ide-options-for-zos-application-development",
    "href": "ide.html#common-ide-options-for-zos-application-development",
    "title": "Integrated development environment",
    "section": "",
    "text": "Eclipse-based: IBM Developer for z/OS (IDz)\nVisual Studio Code (VS Code) extension: IBM Z Open Editor (and IBM Z Open Debug)\nBrowser-based: Wazi for Dev Spaces",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Integrated development environment"
    ]
  },
  {
    "objectID": "ide.html#resources",
    "href": "ide.html#resources",
    "title": "Integrated development environment",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Integrated development environment"
    ]
  },
  {
    "objectID": "manual-migration.html",
    "href": "manual-migration.html",
    "title": "Manual migration",
    "section": "",
    "text": "Manual migration\nManual migration of source data from z/OS to Git is generally not recommended, as it tends to be slower, more tedious, and prone to human error. However, it is possible, and can be done several ways, including the following:\n\nCopy the files to z/OS UNIX System Services (z/OS UNIX) via the Interactive System Productivity Facility (ISPF).\nCopy the files to z/OS UNIX via IBM Developer for z/OS (IDz).\n\nDrag and drop members from IDz’s Remote System Explorer (RSE) to a local project.\n\n\nAlthough manual migration is not recommended, if you do proceed with it, then it is important to remember that you must also manually create the .gitattributes file used for code page translation between z/OS and the Git server, and also manually detect and manage code page conversion issues.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "Manual migration"
    ]
  },
  {
    "objectID": "implementing-pipeline-actions.html",
    "href": "implementing-pipeline-actions.html",
    "title": "Implementing pipeline actions",
    "section": "",
    "text": "The following guides for common pipeline orchestrators are available to help you get started with implementing your continuous integration/continuous delivery (CI/CD) pipeline.\nNote: These guides cover commonly-selected combinations of tools for the different CI/CD pipeline components, but they are intended as examples and are therefore not all-inclusive. If you do not see the specific technology or combination of tools your enterprise has selected, that does not necessarily mean it cannot be part of your IBM Z DevOps solution. For most tools being used on the distributed side, you can integrate it into the pipeline as long as it is compatible with z/OS application development and brings value to your enterprise.\n\n\n\nAzure DevOps and IBM Dependency Based Build Integration\n\n\n\n\n\nUsing IBM Dependency Based Build (DBB) with Github Actions\n\n\n\n\n\nBuild a pipeline with GitLab CI, IBM Dependency Based Build, and IBM UrbanCode Deploy\nIntegrating IBM z/OS platform in CI/CD pipelines with GitLab\n\n\n\n\n\nBuild a Pipeline with Jenkins, DBB and UCD\nManaging git credentials in Jenkins to access the central git provider\nPOC Cookbook – Building a modern pipeline in Mainframe\nSetting up the CI Pipeline Agent on IBM Z as a Started Task",
    "crumbs": [
      "Implementing CI/CD",
      "Implementing pipeline actions"
    ]
  },
  {
    "objectID": "implementing-pipeline-actions.html#azure-pipelines",
    "href": "implementing-pipeline-actions.html#azure-pipelines",
    "title": "Implementing pipeline actions",
    "section": "",
    "text": "Azure DevOps and IBM Dependency Based Build Integration",
    "crumbs": [
      "Implementing CI/CD",
      "Implementing pipeline actions"
    ]
  },
  {
    "objectID": "implementing-pipeline-actions.html#github-actions",
    "href": "implementing-pipeline-actions.html#github-actions",
    "title": "Implementing pipeline actions",
    "section": "",
    "text": "Using IBM Dependency Based Build (DBB) with Github Actions",
    "crumbs": [
      "Implementing CI/CD",
      "Implementing pipeline actions"
    ]
  },
  {
    "objectID": "implementing-pipeline-actions.html#gitlab-ci",
    "href": "implementing-pipeline-actions.html#gitlab-ci",
    "title": "Implementing pipeline actions",
    "section": "",
    "text": "Build a pipeline with GitLab CI, IBM Dependency Based Build, and IBM UrbanCode Deploy\nIntegrating IBM z/OS platform in CI/CD pipelines with GitLab",
    "crumbs": [
      "Implementing CI/CD",
      "Implementing pipeline actions"
    ]
  },
  {
    "objectID": "implementing-pipeline-actions.html#jenkins",
    "href": "implementing-pipeline-actions.html#jenkins",
    "title": "Implementing pipeline actions",
    "section": "",
    "text": "Build a Pipeline with Jenkins, DBB and UCD\nManaging git credentials in Jenkins to access the central git provider\nPOC Cookbook – Building a modern pipeline in Mainframe\nSetting up the CI Pipeline Agent on IBM Z as a Started Task",
    "crumbs": [
      "Implementing CI/CD",
      "Implementing pipeline actions"
    ]
  },
  {
    "objectID": "pipeline-strategy.html",
    "href": "pipeline-strategy.html",
    "title": "Architecting the pipeline strategy",
    "section": "",
    "text": "A continuous integration/continuous delivery (CI/CD) pipeline removes the manual tasks of defining a package. This task is now fully automated and occurs once the application is built.\nIt is typical to have multiple packages, with each package being expected to have passed automated quality gate testing. Not all packages will make it to production due to discovered defects.\nBecause the creation of a package most likely occurs at the application component level, the source code management (SCM) layout also has an impact on the packaging.\nTechnically, a package is mainly composed of a list of binaries or files. However, it also includes metadata for the deployment manager. Using the metadata and a defined process, the deployment manager knows how to treat each element of the package during rollout.\nIt is important, especially for build outputs on mainframes, to keep track of each’s element type. For example, during the deployment phase, a CICS program will require different handling than a plain batch program. The same applies for programs using Db2. Therefore, the metadata associated with each artifact should provide a “deploy type”. This type indicates the nature of the artifact and is used to drive the appropriate process when deploying the artifact.\n\n\n\nIt is imperative that you consider build outputs as part of your package. Examples of these items are highlighted in yellow in the following image. However, it is equally important to also consider the application-specific configuration artifacts (items highlighted in blue). Doing so will help teams avoid a flood of change requests, limit constant back and forth communication, and will enable the continued use of deployment automation. Application-specific configurations should be treated as source code, in the same way you manage your application source code (although, not everything needs to end up in a single package; we can also consider configuration and application packages). The following image shows a list of package items that are typical in the mainframe domain.\n\n\n\nExample of typical package items for a mainframe application\n\n\nThe package is represented in an archive format such as .tar (common in the UNIX world). This format is consistent with distributed applications, where teams usually work with full application packages in archives such as a WAR or a JAR file.\nIn all cases, the package consists of two items:\n\nThe actual binaries and files produced by the build manager\nA manifest describing the package’s contents (that is, metadata)\n\nFor mainframe applications, a package will contain executables required to run your application, such as program objects, DBRM, JCL, and control cards – as well as a manifest file. An example of package contents in a typical mainframe application package is shown in the following image.\n\n\n\nExample of package contents in a typical mainframe application package\n\n\n\nThe package can also carry listings or debug files which you use during debugging. By doing so, you ensure that your listing files’ version matches the version of your binary.\n\n\n\n\nPackage creation occurs after a build. Here, binaries are created, but other files are also produced during the build process. The build process takes inputs from source files stored in one or several Git repositories. Usually, when several repositories are involved, one repository will be responsible for providing the parts to build (that is, programs), while the other repositories provide additional files (for example, shared copybooks). The scope of the build, derived from the scope of the main Git repository used during the build, defines the possible content of the package.\nWe need to distinguish between a “full package” – containing all executables and configuration files belonging to an application component – and a “partial package” – containing just the updated executables and configurations. You might be familiar with “incremental packages”; oftentimes, this term may be used interchangeably with the partial package term.\nPartial packages can be divided into two types:\n\n“Delta packages” are produced immediately after each build\n“Cumulative packages” include outputs produced by several builds.\n\nMainframe applications typically work with incremental updates of the execution environment using partial packages.\nAs you might have seen already, there are many factors you will need to consider when selecting your strategy. As a practical exercise, we will walk through the following mainframe scenario using delta packages. In this scenario, summarized in the following figure, there are several execution environments in our system, and they are not updated at the same pace.\n\nThe build is incremental and produces a partial package containing the newly built artifacts.\nThe CI/CD pipeline automatically deploys the package produced by the build in a Test environment. We might see some tests failing, so developers iterate.\nThe build runs three times. It produces three packages. Each of them is deployed in Test. However, QA is not updated yet.\nThe next phase of tests is performed in the QA environment, when a first level of validation has occurred in Test.\n\n\n\n\nA packaging scenario where execution environments are not updated at the same pace\n\n\nCurrently, most mainframe development practices only work within a static and fixed set of known execution environments. Introducing a new execution environment is, today, a significant task in mainframe shops. It prohibits provisioning of test environments for specific projects and/or sprints; a requirement for most agile teams.\nIn contrast, when provisioning capabilities are applied to your mainframe CI/CD pipeline, it becomes possible to make execution environments available more dynamically. A new test environment could be provisioned and used on-demand, and then recycled when the feature’s test has been completed.\nYou may have noticed that the deployment pipeline has several types of execution environments to manage. At times, you will encounter long running, existing environments (that is, those that are updated but not at the same time), and also “fresh” environments - those that are either empty or need to be significantly updated.\nIn the first case (existing execution environments), we see that the environment is updated regularly by the deployment manager. Here it is easy and convenient to work with increments as each increment is deployed.\nIn the second case (“fresh” execution environments), the environment is updated less frequently or sometimes later in the CI/CD process. In this case, working with increments now requires an understanding of the last deployed increment, and the retrieval and the deployment of all increments until the desired state is achieved (in our example, package 1, 2, and 3). In some cases, packages will overlap, although the deployment manager might be smart enough to deploy only the latest version of an artifact.\nIn that latter case (“fresh” execution environments), working with partial packages is even more challenging. We miss out on more than just the latest packages; we miss out on some (if not all) significant history too. Thus, it becomes useful to maintain the complete content of the application, along with complete configuration scripts (for example, CICS declarations, full DDL). If a sound knowledge of the environment’s inventory is maintained, then as we deploy, it will be possible to correctly calculate and apply the delta.\nWe can now examine the two packaging options that we have identified and relate them to the different IBM Dependency Based Build types:\n\nStrategy: Partial packages\n\nContains the outputs of a DBB impact build. The build has identified the impacted elements by querying the collection in the IBM Dependency Based Build server.\n\nStrategy: Full packages\n\nContains all the outputs of an application configuration. The content either needs to be determined through a full build, a simulation build, or an application descriptor. The application descriptor defines the entire scope of an application and its outputs.\n\n\n\n\n\nDue to the nature of mainframe deployments, there is a need to capture additional metadata, such as the type of the object, for each binary in the package. We call this type of metadata the “deploy type”. It gives explicit criteria to follow a series of steps that are appropriate for the artifact to deploy.\nThere is thus a need for a manifest file. This file describes the contents of the application package and adds meta information to each of its artifacts. This information will then also be used by the deployment manager.\nAdditionally, the manifest file captures traceability information about the configuration used to create the package - in particular, a Git hash to trace back to the actual version of the source code. The manifest file will also capture the package type: full or partial.\nThe limits of which environment a package may or may not go is another piece of meta-information that the manifest of a package should contain.\nThe format of the manifest is more of a secondary consideration: it can be .yaml, .json, .xml, and so on. Considering the direction of containers with Kubernetes using Helm charts and OpenShift templates using .yaml, using .yaml for the metadata will make it more consistent with other industry work and make it clearly understandable by z/OS and non z/OS developers. The following image shows a sample schema of an application package manifest.\n\n\n\nA sample schema of an application package manifest\n\n\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Architecting the pipeline strategy"
    ]
  },
  {
    "objectID": "pipeline-strategy.html#building-packaging-and-deploying-in-a-pipeline",
    "href": "pipeline-strategy.html#building-packaging-and-deploying-in-a-pipeline",
    "title": "Architecting the pipeline strategy",
    "section": "",
    "text": "A continuous integration/continuous delivery (CI/CD) pipeline removes the manual tasks of defining a package. This task is now fully automated and occurs once the application is built.\nIt is typical to have multiple packages, with each package being expected to have passed automated quality gate testing. Not all packages will make it to production due to discovered defects.\nBecause the creation of a package most likely occurs at the application component level, the source code management (SCM) layout also has an impact on the packaging.\nTechnically, a package is mainly composed of a list of binaries or files. However, it also includes metadata for the deployment manager. Using the metadata and a defined process, the deployment manager knows how to treat each element of the package during rollout.\nIt is important, especially for build outputs on mainframes, to keep track of each’s element type. For example, during the deployment phase, a CICS program will require different handling than a plain batch program. The same applies for programs using Db2. Therefore, the metadata associated with each artifact should provide a “deploy type”. This type indicates the nature of the artifact and is used to drive the appropriate process when deploying the artifact.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Architecting the pipeline strategy"
    ]
  },
  {
    "objectID": "pipeline-strategy.html#package-content-and-layout",
    "href": "pipeline-strategy.html#package-content-and-layout",
    "title": "Architecting the pipeline strategy",
    "section": "",
    "text": "It is imperative that you consider build outputs as part of your package. Examples of these items are highlighted in yellow in the following image. However, it is equally important to also consider the application-specific configuration artifacts (items highlighted in blue). Doing so will help teams avoid a flood of change requests, limit constant back and forth communication, and will enable the continued use of deployment automation. Application-specific configurations should be treated as source code, in the same way you manage your application source code (although, not everything needs to end up in a single package; we can also consider configuration and application packages). The following image shows a list of package items that are typical in the mainframe domain.\n\n\n\nExample of typical package items for a mainframe application\n\n\nThe package is represented in an archive format such as .tar (common in the UNIX world). This format is consistent with distributed applications, where teams usually work with full application packages in archives such as a WAR or a JAR file.\nIn all cases, the package consists of two items:\n\nThe actual binaries and files produced by the build manager\nA manifest describing the package’s contents (that is, metadata)\n\nFor mainframe applications, a package will contain executables required to run your application, such as program objects, DBRM, JCL, and control cards – as well as a manifest file. An example of package contents in a typical mainframe application package is shown in the following image.\n\n\n\nExample of package contents in a typical mainframe application package\n\n\n\nThe package can also carry listings or debug files which you use during debugging. By doing so, you ensure that your listing files’ version matches the version of your binary.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Architecting the pipeline strategy"
    ]
  },
  {
    "objectID": "pipeline-strategy.html#package-strategy-and-scope",
    "href": "pipeline-strategy.html#package-strategy-and-scope",
    "title": "Architecting the pipeline strategy",
    "section": "",
    "text": "Package creation occurs after a build. Here, binaries are created, but other files are also produced during the build process. The build process takes inputs from source files stored in one or several Git repositories. Usually, when several repositories are involved, one repository will be responsible for providing the parts to build (that is, programs), while the other repositories provide additional files (for example, shared copybooks). The scope of the build, derived from the scope of the main Git repository used during the build, defines the possible content of the package.\nWe need to distinguish between a “full package” – containing all executables and configuration files belonging to an application component – and a “partial package” – containing just the updated executables and configurations. You might be familiar with “incremental packages”; oftentimes, this term may be used interchangeably with the partial package term.\nPartial packages can be divided into two types:\n\n“Delta packages” are produced immediately after each build\n“Cumulative packages” include outputs produced by several builds.\n\nMainframe applications typically work with incremental updates of the execution environment using partial packages.\nAs you might have seen already, there are many factors you will need to consider when selecting your strategy. As a practical exercise, we will walk through the following mainframe scenario using delta packages. In this scenario, summarized in the following figure, there are several execution environments in our system, and they are not updated at the same pace.\n\nThe build is incremental and produces a partial package containing the newly built artifacts.\nThe CI/CD pipeline automatically deploys the package produced by the build in a Test environment. We might see some tests failing, so developers iterate.\nThe build runs three times. It produces three packages. Each of them is deployed in Test. However, QA is not updated yet.\nThe next phase of tests is performed in the QA environment, when a first level of validation has occurred in Test.\n\n\n\n\nA packaging scenario where execution environments are not updated at the same pace\n\n\nCurrently, most mainframe development practices only work within a static and fixed set of known execution environments. Introducing a new execution environment is, today, a significant task in mainframe shops. It prohibits provisioning of test environments for specific projects and/or sprints; a requirement for most agile teams.\nIn contrast, when provisioning capabilities are applied to your mainframe CI/CD pipeline, it becomes possible to make execution environments available more dynamically. A new test environment could be provisioned and used on-demand, and then recycled when the feature’s test has been completed.\nYou may have noticed that the deployment pipeline has several types of execution environments to manage. At times, you will encounter long running, existing environments (that is, those that are updated but not at the same time), and also “fresh” environments - those that are either empty or need to be significantly updated.\nIn the first case (existing execution environments), we see that the environment is updated regularly by the deployment manager. Here it is easy and convenient to work with increments as each increment is deployed.\nIn the second case (“fresh” execution environments), the environment is updated less frequently or sometimes later in the CI/CD process. In this case, working with increments now requires an understanding of the last deployed increment, and the retrieval and the deployment of all increments until the desired state is achieved (in our example, package 1, 2, and 3). In some cases, packages will overlap, although the deployment manager might be smart enough to deploy only the latest version of an artifact.\nIn that latter case (“fresh” execution environments), working with partial packages is even more challenging. We miss out on more than just the latest packages; we miss out on some (if not all) significant history too. Thus, it becomes useful to maintain the complete content of the application, along with complete configuration scripts (for example, CICS declarations, full DDL). If a sound knowledge of the environment’s inventory is maintained, then as we deploy, it will be possible to correctly calculate and apply the delta.\nWe can now examine the two packaging options that we have identified and relate them to the different IBM Dependency Based Build types:\n\nStrategy: Partial packages\n\nContains the outputs of a DBB impact build. The build has identified the impacted elements by querying the collection in the IBM Dependency Based Build server.\n\nStrategy: Full packages\n\nContains all the outputs of an application configuration. The content either needs to be determined through a full build, a simulation build, or an application descriptor. The application descriptor defines the entire scope of an application and its outputs.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Architecting the pipeline strategy"
    ]
  },
  {
    "objectID": "pipeline-strategy.html#specialty-of-mainframe-packages",
    "href": "pipeline-strategy.html#specialty-of-mainframe-packages",
    "title": "Architecting the pipeline strategy",
    "section": "",
    "text": "Due to the nature of mainframe deployments, there is a need to capture additional metadata, such as the type of the object, for each binary in the package. We call this type of metadata the “deploy type”. It gives explicit criteria to follow a series of steps that are appropriate for the artifact to deploy.\nThere is thus a need for a manifest file. This file describes the contents of the application package and adds meta information to each of its artifacts. This information will then also be used by the deployment manager.\nAdditionally, the manifest file captures traceability information about the configuration used to create the package - in particular, a Git hash to trace back to the actual version of the source code. The manifest file will also capture the package type: full or partial.\nThe limits of which environment a package may or may not go is another piece of meta-information that the manifest of a package should contain.\nThe format of the manifest is more of a secondary consideration: it can be .yaml, .json, .xml, and so on. Considering the direction of containers with Kubernetes using Helm charts and OpenShift templates using .yaml, using .yaml for the metadata will make it more consistent with other industry work and make it clearly understandable by z/OS and non z/OS developers. The following image shows a sample schema of an application package manifest.\n\n\n\nA sample schema of an application package manifest",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Architecting the pipeline strategy"
    ]
  },
  {
    "objectID": "pipeline-strategy.html#resources",
    "href": "pipeline-strategy.html#resources",
    "title": "Architecting the pipeline strategy",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Architecting the pipeline strategy"
    ]
  },
  {
    "objectID": "branching-models.html",
    "href": "branching-models.html",
    "title": "Introduction to branching models",
    "section": "",
    "text": "Git branching models are patterns to enable development teams using Git to manage their code in a streamlined manner. Since Git is established as a de facto standard for source code management (SCM) in the developer community, several approaches were designed to fulfill developers’ requirements and manage the application source code lifecycle, with advantages and drawbacks depending on use cases. Based on the experience gained designing branching strategies, the pages in this section describe a blueprint implementation of a mainline-based development approach for mainframe applications using feature branches with an early integration pattern. This setup leverages a standardized development toolset based on an enterprise-wide Git provider and a continuous integration/continuous delivery (CI/CD) toolchain.\nA characteristic of this integration pattern is that developers are implementing changes for a planned release and integrate their changes into a common permanent branch (the shared configuration) that is built, tested, and released together as one consistent entity.\nThe purpose of streamlining both the DevOps solutions and the delivery workflow is to simplify the process for development teams to deliver quality product releases on time. This enables agile development practices that allow the teams to respond more effectively to changes in the market and customer needs. The Git branching model for mainframe development introduces the branching model and outlines the development workflow from the developer’s perspective. The details of the technical implementation with IBM Dependency Based Build (DBB) and zAppBuild, as well as packaging and deployment, are discussed in Pipeline design and implementation supporting the branching model. All branching models are adaptable to the needs of specific teams and their applications. Our branching approach advocates for best practices and indicates where variations can be applied.\nThe target audience of this branching model documentation is mainframe DevOps architects and SCM specialists interested in learning how to design and implement a CI/CD pipeline with a robust and state-of-the-art development workflow.\n\n\nSome aims and assumptions that guide our recommendations include:\n\nThe workflow and branching scheme should both scale up and scale down.\n\nSmall teams with simple and infrequent changes will be able to easily understand, adopt, and have a good experience.\nLarge, busy teams with many concurrent activities will be able to plan, track, and execute with maximum agility using the same fundamental principles.\n\nPlanning and design activities as well as code development aim to align to a regular release cadence.\nThere is no magic answer to managing large numbers of “in-flight” changes, so planning assumptions should aim as much as possible to complete changes quickly, ideally within one release cycle.\n\nDevOps/Agile practices typically encourage that, where possible, development teams should strive to break down larger changes into sets of smaller, incremental deliverables that can each be completed within an iteration. This reduces the number of “in-flight” changes, and allows the team to deliver value (end-to-end functionality) more quickly while still building towards a larger development goal.\n\nWe know it is sometimes unavoidable for work to take longer than one release cycle and we accommodate that as a variant of the base workflow.\n\n\n\n\nYour choice of workflow and the branching model that supports it need to take into account your team’s needs and characteristics.\nAspects to consider include:\n\nSize of the team\nFrequency of change\nGranularity of change\nAmount of parallel development\nFormality of release process\n\nThe workflows of our recommended Git branching model for mainframe development are flexible enough to scale from small teams with an infrequent pattern of small changes, to large and busier teams with many concurrent projects and projects spanning multiple cycles of a formal release cadence. These workflows are supported by the CI/CD pipeline implementation described in Pipeline design and implementation supporting the branching model.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Introduction to branching models"
    ]
  },
  {
    "objectID": "branching-models.html#aims-and-assumptions",
    "href": "branching-models.html#aims-and-assumptions",
    "title": "Introduction to branching models",
    "section": "",
    "text": "Some aims and assumptions that guide our recommendations include:\n\nThe workflow and branching scheme should both scale up and scale down.\n\nSmall teams with simple and infrequent changes will be able to easily understand, adopt, and have a good experience.\nLarge, busy teams with many concurrent activities will be able to plan, track, and execute with maximum agility using the same fundamental principles.\n\nPlanning and design activities as well as code development aim to align to a regular release cadence.\nThere is no magic answer to managing large numbers of “in-flight” changes, so planning assumptions should aim as much as possible to complete changes quickly, ideally within one release cycle.\n\nDevOps/Agile practices typically encourage that, where possible, development teams should strive to break down larger changes into sets of smaller, incremental deliverables that can each be completed within an iteration. This reduces the number of “in-flight” changes, and allows the team to deliver value (end-to-end functionality) more quickly while still building towards a larger development goal.\n\nWe know it is sometimes unavoidable for work to take longer than one release cycle and we accommodate that as a variant of the base workflow.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Introduction to branching models"
    ]
  },
  {
    "objectID": "branching-models.html#choosing-a-workflow-and-branching-model",
    "href": "branching-models.html#choosing-a-workflow-and-branching-model",
    "title": "Introduction to branching models",
    "section": "",
    "text": "Your choice of workflow and the branching model that supports it need to take into account your team’s needs and characteristics.\nAspects to consider include:\n\nSize of the team\nFrequency of change\nGranularity of change\nAmount of parallel development\nFormality of release process\n\nThe workflows of our recommended Git branching model for mainframe development are flexible enough to scale from small teams with an infrequent pattern of small changes, to large and busier teams with many concurrent projects and projects spanning multiple cycles of a formal release cadence. These workflows are supported by the CI/CD pipeline implementation described in Pipeline design and implementation supporting the branching model.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Introduction to branching models"
    ]
  },
  {
    "objectID": "git-branching-model-for-mainframe-dev.html",
    "href": "git-branching-model-for-mainframe-dev.html",
    "title": "Git branching model for mainframe development",
    "section": "",
    "text": "As Git became the de facto version control system in today’s IT world, new terminologies such as “repositories”, “branches”, and “merges” arose. By agreeing upon a central Git server to integrate and consolidate changes, development teams were able to collaborate more efficiently and effectively. Building upon the open-source vanilla Git implementation, popular Git providers including GitHub, GitLab, and Bitbucket have implemented additional workflow features to facilitate a secure and stable development process. These include features such as pull requests (sometimes referred to as “merge requests”) to support coordination with Git in larger teams. The term “pull request” will be used throughout this page to designate the operation of reviewing and merging one branch into another.\nMany mainframe development teams follow a release-based or iteration-based process to deliver incremental updates to a pre-defined production runtime.\n\n\nAs mentioned in the Introduction to branching models, our recommended approach scales very well to support the needs of a range of team sizes, frequency and size of changes, and degrees of concurrent working.\n\n\nThe mainline-based development approach1 with short-lived feature branches is a simple and structured workflow to implement, integrate, and deliver changes with an early integration process flow using a single long-living branch: main. Developers work in isolation in feature branches to implement changes to the source code, and ideally test the changes in a specific environment. Each feature branch (sometimes referred to as a “topic branch”) is dedicated to a specific developer task such as a feature or bug fix.\nThis approach can be compared to a trunk-based branching model that leverages feature branches. A similar workflow is also documented by Microsoft without giving it a name.2\nThe main branch is the point of reference for the entire history of the mainline changes to the code base, and should be a protected branch. All changes should originate on a separate branch created to hold them until they are ready to be merged. Using a branch-and-merge approach is natural in a Git-based development workflow, and is very lightweight both in terms of resource consumption and developer experience.\n\n\n\nStart simple\n\n\n\n\nAlthough all Git-based DevOps services provide direct access to the server copy of the code repository, developers will typically use an integrated development environment (IDE) of their choice to work locally on a clone of the repository. Standard practices ensure developers’ local clones are synchronized with the server copy (typically known as the “remote”) through actions known as “pull” and “push”. (For those unfamiliar with Git terminology, these terms are explained in finer detail in Source code management.)\nIn a small team where there is almost never more than one change in progress at a time, using a branch enables the developer to make the changes in a series of commits rather than as one atomic change as they edit source and save files. Of course, the “one-line” change might mean there genuinely is only one commit that needs merging, but the process is so natural and light-weight that it is not worth making that a special case.\nSuch branches are able to be built prior to merging - this can eliminate the possibility of the merge breaking the build of main, thus reducing the risk in making changes.\n\n\n\nDiagram of a feature branch pipeline\n\n\n\n\n\nA branch holds all the commits for a change - be that a single commit for a one-liner or a sequence of commits as the developer refined the change while making it ready for review and merging into main.\nThe request to merge a branch is made explicitly, but can be as formal or informal as needed by the team. Protection of main can mean that only certain people can perform the merge, or that a review and approval of the change is required before merging it, or both.\nThe action of merging can either simply take all the commits from the branch and add them to main, or that multiple commits in the branch can be “squashed” into one commit - the latter can keep the overall history on main “cleaner” if that’s important to the team.\nmain should always build successfully, enabling the team to choose when to package and deploy.\n\n\n\n\nThe use of branches for concurrently planned activities scales extremely well for busier teams. Additionally, epic and release maintenance branches accommodate specific development workflows and allow the model to scale even further. The latter two branches exist for the duration of the epic or release maintenance and are short-living branches.\nThe implemented changes of the iteration are then delivered collectively as part of the next release. Each development team decides how long an iteration is. We advocate for working towards smaller, quicker release cycles, but this model can also be used with longer iterations. Due to business or technical reasons, the merging of features into the main branch can also be delayed. Although this scenario is a discouraged practice, the recommendation in this case is to group such features into a specific epic branch, as described later.\nThis branching model leverages Git tags to identify the various configurations/versions of the application, such as a release candidate or the version of the application repository that is deployed to production.\nDepending on the type of change, the development workflow can vary. In the standard scenario, developers use the main branch to deliver changes for the next planned release, while the release maintenance branches allow fixing of the current release running in the production runtime environment(s). Using epic branches is optional for development teams, but is a grouping mechanism for multiple features that should be built and tested together, thus allowing teams to increase the concurrency of working on multiple, larger development initiatives of the application. The epic branch also represents a way to manage the lifecycle of features that are not planned for the next planned release. In this way, it is a vehicle to delay merging the set of features into the main branch for a later time.\n\n\n\nsupported development workflows\n\n\nThe main, epic, and release branches are assumed to be protected branches, meaning that no developer can directly push changes to these configurations. It requires developers to make changes on a feature branch and go through the pull request process. Before merging the feature branch into a shared branch (whether it is the main branch or an epic branch), some evidence should be gathered to ensure quality and respect of the coding standards in the enterprise. Peer-reviewed code, a clean pipeline execution, and approvals are examples of such evidence, allowing the development team to confidently merge the feature branch into the target branch. In a continuous integration workflow, integrations are expected to happen early to avoid delaying merge conflicts or merges leading to an unstable build.\n\n\n\nConsistent branch naming conventions help indicate the context for the work that is performed. Throughout this document, the following naming patterns are used:\n\nmain: The only long-living branch which is the only branch from which every release is initially derived\nrelease/rel-2.0.1: The release maintenance branch for an example release named rel-2.0.1\nepic/ai-fraud-detection: An epic branch where “aiFraudDetection” is describing the initiative context (in this example, an initiative to adopt AI technology for fraud detection)\n\nFeature branches also need to relate back to the change request (or issue) from the planning phase and their context. Some examples are shown in the following list:\n\nfeature/new-mortgage-calculation for a planned feature for the next planned release\nhotfix/rel-2.0.1/fix-mortgage-calculation for a fix of the current production version that is running the rel-2.0.1 release\nai-fraud-detection/introduce-ai-model-to-mortgage-calculation for a contribution to the development initiative for adopting AI technology for fraud detection.\n\nA common, recommended practice is to squash the different commits created on the feature branch into a single new commit when merging, which keeps the Git history from becoming cluttered with intermediate work for the feature. This also helps to maintain a tidy history on the main branch with only the important commits.\n\n\n\nSpecific branches, such as main, epic, and release branches can be seen as integration branches, because their purpose is to integrate changes from other branches (typically feature branches). To drive the integration process of changes into a shared branch of code, mechanisms like pull requests are a convenient way as they guide the developers with a streamlined workflow. The number of integration branches required for your development process depends on the needs of the application team. However, while the cost of creating new branches is low, keeping them up-to-date, for instance by integrating release bugfixes from the stabilization phase into concurrent epic branches, can be expensive.\nFor application teams who want to embrace an agile development methodology and who sequentially deliver new releases with limited parallel development initiatives, they can use the main branch and, optionally, the release maintenance branch as integration branches to implement the next planned release and potential bug fixes. The following diagram illustrates a branching model for a Git-based development process with sequential release deliveries.\n\n\n\nBranching diagram showing Git-based development process with sequential release deliveries\n\n\nIf the development teams need to work on a significant development initiative in parallel to the standard scenario of the next planned release, this model allows isolation using the epic branch workflow. The epic branch (epic_1) in the following branching diagram represents an additional integration branch that is used to implement, build, and test multiple features that are planned for the development initiative and can be merged into the main branch at a later time. The team decides which commit/tag of the codebase in the main branch will be used as the base for the epic branch, although it is recommended to start from the last tag for the main branch.\n\n\n\nBranching diagram showing Git-based development process with an epic branch for a larger development solution initiative\n\n\nWhen the work items implemented on the epic branch are planned and ready to be delivered as part of the next planned release, the development team merges the epic branch into the main branch.\nEpic branches can be used to compose various styles of development processes. The documentation for Development process variations provides additional samples.\n\n\n\n\nThis branching model facilitates three different types of development workflows:\n\n(Default development workflow) Deliver changes with the next planned release: With a single long-living branch, the development process allows developers to work and focus on the next planned release. After planning the work items for the next release, the development team is adding changes to the main branch by merging in pull requests for feature branches.\nImplement a fix for the current production state: This workflow enables development teams to resolve a production problem in the currently-released version of the application by leveraging a release branch that is used for maintenance purposes.\nUse an epic branch for a significant development initiative: Concurrent development activities for a significant solution development initiative that includes multiple planned work items for a later delivery (which could even be starting the development of a future release) are supported by creating an epic branch from a commit point in the history of main.\n\nGit tags are used throughout this process to indicate and label important commits, such as the commit of a release that is built from the main branch, or a maintenance release created from a release maintenance branch.\nThe next sections outline the various tasks and activities performed by the development team in the context of the above three scenarios.\n\n\nThe following diagram depicts the typical workflow to deliver changes for the next planned release. In the default workflow, the development team commits changes to the head of the main branch. The changes of the next planned release are built, packaged, and released from the main branch.\n\n\n\nDiagram showing Git branching workflow supporting a release-based delivery approach\n\n\nDevelopers implement their changes by committing to short-living feature branches (visualized in yellow), and integrate those via pull requests into the long-living main branch (visualized in red), which is configured to be a protected branch.\nAt a high level, the development team works through the following tasks:\n\nNew work items are managed in the backlog. The team decides which work items will be implemented in the next iteration. Each application team can decide on the duration of the iteration (which can also be seen as the development cycle). In the above diagram, three work items (Feature 1, Feature 2, and Feature 3) were selected to be implemented for the next iteration. The development team is responsible for coordinating if features are required to be implemented in a specific order.\nFor each work item, a feature branch is created according to pre-defined naming conventions, allowing the assigned developers to have a copy of the codebase on which they can work in isolation from other concurrent development activities.\n\n\n\nDiagram of the feature branch created off the main branch\n\n\nTo start making the necessary modifications for their development task, developers create a copy of the Git repository on their local workstations through Git’s clone operation. If they already have a local clone of the repository, they can simply update their local clone with the latest changes from the central Git repository by fetching or pulling updates into their local clone. This process makes the feature branch available for the developers to work with on their local workstation. They can then open their local clone of the repository in their integrated development environment (IDE), and switch to the feature branch to make their code changes.\nDevelopers use the Dependency Based Build (DBB) User Build facility of their IDE to validate their code changes before committing the changes to their feature branch and pushing the feature branch with their updates to the central Git repository. (Tip: Feature branches created locally can also be pushed to the central Git server).\n\n\n\nDiagram of the IDE DBB User Build capability\n\n\n:::tip\nThis branching model is also known as a continuous integration model to reduce merge conflicts. While developing on the feature branch, a common practice is for developers to regularly sync their feature branch with the main branch by merging the latest changes from the main branch into their feature branch. This ensures that developers are operating based on a recent state of main, and helps to identify any potential merge conflicts so that they can resolve them in their feature branch.\n:::\nDevelopers test their changes before requesting to integrate them into the shared codebase. For example, they can test the build outputs of the User Build step. For a more integrated experience, the CI/CD pipeline orchestrator can be configured to run a pipeline for the feature branch on the central Git server each time the developers push their committed changes to it. This process will start a consolidated build that includes the changed and impacted programs within the application scope. Unit tests can be automated for this pipeline, as well. To continue even further testing the feature branch, the developer might want to validate the build results in a controlled test environment, which is made possible by an optional process to create a preliminary package for the feature branch.\n\n\n\nDiagram of Feature Branch Pipelines\n\n\nWhen developers feel their code changes are ready to be integrated back into the shared main branch, they create a pull request asking to integrate the changes from their feature branch into the main branch. The pull request process provides the capability to add peer review and approval steps before allowing the changes to be merged. As a basic best practice, the changes must be buildable. If the pull request is associated with a feature branch pipeline, this pipeline can also run automated builds of the code in the pull request along with tests and code quality scans.\nOnce the pull request is merged into the main branch, the next execution of the Basic Build Pipeline will build all the changes (and their impacts) of the iteration based on the main branch.\n\n\n\nBranching diagram showing the Basic Build Pipeline execution after the feature branch’s pull request is merged into the main branch\n\n\nThe pipeline can optionally include a stage to deploy the built artifacts (load modules, DBRMs, and so on) into a shared test environment, as highlighted by the blue DEV-TEST icon in the above diagram. In this DEV-TEST environment, the development team can validate their combined changes. This first test environment helps support a shift-left testing strategy by providing a sandbox with the necessary setup and materials for developers to test their changes early. The installation happens through the packaging and deployment process of a preliminary package that cannot be installed to higher environments (because it is compiled with test options), or alternatively through a simplified script solution performing a copy operation. In the latter, no inventory and no deployment history of the DEV-TEST system exist.\nIn the example scenario for this workflow, the development team decides after implementing Feature 1 and Feature 2 to progress further in the delivery process and build a release candidate package based on the current state of the main branch. With this decision, the development team manually runs the Release Pipeline. This pipeline rebuilds the contributed changes for this iteration - with the compiler options to produce executables optimized for performance rather than for debug. The pipeline includes an additional stage to package the build outputs and create a release candidate package (Package RC1 in the following diagram), which is stored in a binary artifact repository.\n\n\n\nDiagram of the Release Pipeline building the changes for the iteration and packaging build outputs into a release candidate package\n\n\nAlthough not depicted in the above diagram, this point in main’s history can be tagged to identify it as a release candidate.\nThe release candidate package is installed in the various test stages and takes a predefined route. The process can be assisted by the pipeline orchestrator itself, or the development team can use the deployment manager. In the event of a defect being found in the new code of the release candidate package, the developer creates a feature branch from the main branch, corrects the issue, and merges it back into the main branch (while still following the normal pull request process). It is expected that the new release candidate package with the fix is required to pass all the quality gates and to be tested again.\nIn this sample walkthrough of an iteration, the development of the third work item (Feature 3) is started later. The same steps as above apply for the developer of this work item. After merging the changes back into the main branch, the team uses the Basic Build Pipeline to validate the changes in the DEV-TEST environment. To create a release candidate package, they make use of the Release Pipeline. This package (Package RC2 in the following diagram) now includes all the changes delivered for this iteration – Feature 1, Feature 2 and Feature 3.\n\n\n\nDiagram of another Release Pipeline building a new release candidate package after Feature 3 is merged in\n\n\nWhen the release is ready to be shipped after all quality gates have passed successfully and the required approvals have been issued by the appropriate reviewers, the deployment of the package from the binary artifact repository to the production runtime environment is performed via the deployment manager or is initiated from the Release Pipeline.\nFinally, during the release process to the production environment, the state of the repository (that is, the commit) from which the release candidate package was produced is tagged following a semantic versioning strategy. This serves to identify what version is currently in production, and also serves as the baseline reference for the calculation of changes for the next release.\n\n\n\nDiagram of another Release Pipeline building a new release candidate package after Feature 3 is merged in\n\n\n\n\n\n\nThe process of urgent fixes for modules in the production environment follows the fix-forward approach, rather than rolling back the affected modules and reverting to the previous deployed state.\nThe following diagram depicts the maintenance process to deliver a fix or maintenance for the active release in production for the application. The process leverages a release maintenance branch to control and manage the fixes. The purpose of the branch is to add maintenance to a release that is already deployed to the production environment. It does not serve the process to add new functionality to a future release, which is covered by the default workflow or the usage of an epic branch.\n\n\n\nGit branching diagram with a release maintenance branch for the production maintenance process\n\n\nWhen implementing a fix for the current production state, the development team works through the following tasks:\n\nIn the event of a required fix or urgent maintenance for the production runtime, which in this example is currently running the 2.1.0 release, the development team creates a release/rel-2.1.0 branch based on the existing Git tag in the central Git server. The release branch is a protected branch and does not allow developers to directly push commits to this branch.\n\nNote: In the diagrams for this workflow, the release/rel-2.1.0 branch is abbreviated to rel-2.1.0 for readability. Other multi-segmented branch names are similarly abbreviated to their last segment name in the diagrams.\n\nFor each necessary fix, a feature branch is created according to pre-defined naming conventions (for example, rel-2.1.0/hotfix_1, based on the release/rel-2.1.0 branch). This allows the assigned developer to have a copy of the codebase on which they can work in isolation from other development activities.\n\n\n\nBranching diagram focused on the hotfix_1 branch created from the release maintenance branch for release 2.1.0\n\n\nThe developers fetch the feature branch from the central Git repository into their local clone of the repository and switch to that branch to start making the necessary modifications. They leverage the user build facility of their IDE to vet out any syntax issues. They can use a feature branch pipeline to build the changed and impacted files. Optionally, the developer can prepare a preliminary package, which can be used for validating the fix in a controlled test environment.\nThe developer initiates the pull request process, which provides the ability to add peer review and approval steps before allowing the changes to be merged into the release/rel-2.1.0 release maintenance branch.\nA Basic Build Pipeline for the release maintenance branch will build all the changes (and their impacts).\nThe developer requests a Release Pipeline for the release/rel-2.1.0 branch that builds the changes (and their impacts), and that includes the packaging process to create the fix package for the production runtime. The developer will test the package in the applicable test environments, as shown in the following diagram.\n\n\n\nBranching diagram showing the Release Pipeline generating a fix package from the release maintenance branch\n\n\nAfter collecting the necessary approvals, the fix package can be deployed to the production environment. To indicate the new state of the production runtime, the developer creates a Git tag (2.1.1 in this example) for the commit that was used to create the fix package. This tag indicates the currently-deployed version of the application.\nFinally, the developer is responsible for starting the pull request process to merge the changes from the release/rel-2.1.0 branch back to the main branch to also include the fix into the next release.\nThe release/rel-2.1.0 branch is retained in case another fix is needed for the active release. The release maintenance branch becomes obsolete when the next planned release (whose starting point is represented by a more recent commit on the main branch) is deployed to production. In this event, the new commit point on the main branch becomes the baseline for a new release maintenance branch.\n\n\n\n…\n\n\n\n\nLet us now focus on change requests that represent significant work effort and require major changes, for instance, due to updates in regulatory frameworks in the banking or insurance industry, or the need to already kick off the development phase of features not planned to be delivered in the very next release.\nIn these situations, the development team cannot follow the business-as-usual workflow to deliver functionality with the next planned release, because the time and work required breaks out of the traditional durations of one release. For each of these scenarios, the development team is using an epic branch to keep the changes in multiple features separated from the other development activities. It is an integration branch to group and integrate multiple features that are planned for this initiative. Ideally, the team has a dedicated test environment assigned (such as EPIC-DEV-TEST and EPIC-INT-TEST in the following diagram), to also plan and implement any infrastructure updates such as Db2 schema changes.\nTrunk-based development suggests using feature flags within the code to implement complex features via the main workflow while allowing the delay of their activation. Feature flags are often employed so that a given business functionality can be activated at a given date, but be implemented and deployed earlier (whether to dev/test or production environments). We do not see this as a common practice for traditional mainframe languages such as COBOL or PL/I, although some development organizations might apply this practice in mainframe development.\nAll these scenarios lead to the requirement on the development process to implement changes independently from the main workflow.\nNote that the epic branch workflow described in this section is not meant to be used for a single, small feature that a developer wants to hold back for an upcoming release. In those smaller cases, the developer retains the feature branch until the change is planned to be released.\n\n\n\nBranching diagram showing the use of an epic branch for a significant development initiative\n\n\nThe development tasks for a development initiative are:\n\nThe team creates an epic branch from the Git tag representing the current production version of the application, which is at this point the most stable configuration. This process provides them isolation of the codebase from any other ongoing changes for the next iteration(s). In this workflow example, the epic branch is named epic/epic_1, and is abbreviated in the diagrams as epic_1.\nBased on how the work items are distributed between the developers, a feature branch is created according to pre-defined naming conventions such as epic_1/feature_4, based on the epic/epic_1 branch.\nThe developers fetch the feature branch from the central Git repository into their local clone of the repository and switch to that branch to start making the necessary modifications. They leverage the user build facility of their IDE for building and testing individual programs. They can also leverage a feature branch pipeline to build the changed and impacted files. Optionally, the developer can prepare a preliminary package, which can be used for validating the fix in a controlled test environment, such as the EPIC-1-FEATURE-TEST environment shown in the following diagram.\n\n\n\nDiagram showing the epic branching workflow with feature branch pipelines\n\n\nThe developer initiates the pull request process, which provides the ability to add peer review and approval steps before allowing the changes to be merged into the epic branch.\nA Basic Build Pipeline for the epic branch will build all the merged features (both the changes and their impacts) from the point where the epic branch was branched off.\nIt is important that the team frequently incorporates updates that have been implemented for the next release and/or released to production via the default development workflow (with the main branch) into the epic branch to prevent the configurations from diverging too much and making the eventual merge of the epic branch into main difficult. A common practice is to integrate changes from main into the epic branch at least after each completion of a release via the default workflow, in order to merge in the latest stable version updates. More frequent integrations may lead to pulling intermediate versions of features that might not be fully implemented from a business perspective; however, this should not deter developers since the main branch should always be in a buildable state.\nWhen the development team feels that they are ready to prototype the changes for the initiative in the initiative’s test environment, they request a Release Pipeline for the epic branch that builds the changes (and their impacts) and includes the packaging process to create a preliminary package. This preliminary package can then be installed into the initiative’s test environment (for example, the EPIC-DEV-TEST environment). The team will test the package in the assigned test environments for this initiative, as shown in the following diagram.\n\n\n\nDiagram showing the Release Pipeline in an epic branching workflow\n\n\nOnce the team is satisfied with their changes for the development initiative, they plan to integrate the changes of the epic branch into the main branch using the pull request process. This happens when the changes should be released towards production with the next planned iteration. The following diagram depicts of the process of integrating the changes implemented for epic/epic_1 in parallel with the default workflow after three releases.\n\n\n\nDiagram showing the integration of changes from an epic branch into main as a planned deliverable of an upcoming release\n\n\n\n\n\n\n\nThis page describes our recommended Git branching model and workflows for mainframe development. This model is intended to be used as a template, and can be adjusted, scaled up, or scaled down according to the needs of the development team. Additional variations for the branching strategies and workflows can be found in the Appendix.\nFor recommendations on designing and implementing the workflows described in this branching model, please refer to Pipeline design and implementation supporting the branching model.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Git branching model for mainframe development"
    ]
  },
  {
    "objectID": "git-branching-model-for-mainframe-dev.html#characteristics-of-mainline-based-development-with-feature-branches",
    "href": "git-branching-model-for-mainframe-dev.html#characteristics-of-mainline-based-development-with-feature-branches",
    "title": "Git branching model for mainframe development",
    "section": "",
    "text": "As mentioned in the Introduction to branching models, our recommended approach scales very well to support the needs of a range of team sizes, frequency and size of changes, and degrees of concurrent working.\n\n\nThe mainline-based development approach1 with short-lived feature branches is a simple and structured workflow to implement, integrate, and deliver changes with an early integration process flow using a single long-living branch: main. Developers work in isolation in feature branches to implement changes to the source code, and ideally test the changes in a specific environment. Each feature branch (sometimes referred to as a “topic branch”) is dedicated to a specific developer task such as a feature or bug fix.\nThis approach can be compared to a trunk-based branching model that leverages feature branches. A similar workflow is also documented by Microsoft without giving it a name.2\nThe main branch is the point of reference for the entire history of the mainline changes to the code base, and should be a protected branch. All changes should originate on a separate branch created to hold them until they are ready to be merged. Using a branch-and-merge approach is natural in a Git-based development workflow, and is very lightweight both in terms of resource consumption and developer experience.\n\n\n\nStart simple\n\n\n\n\nAlthough all Git-based DevOps services provide direct access to the server copy of the code repository, developers will typically use an integrated development environment (IDE) of their choice to work locally on a clone of the repository. Standard practices ensure developers’ local clones are synchronized with the server copy (typically known as the “remote”) through actions known as “pull” and “push”. (For those unfamiliar with Git terminology, these terms are explained in finer detail in Source code management.)\nIn a small team where there is almost never more than one change in progress at a time, using a branch enables the developer to make the changes in a series of commits rather than as one atomic change as they edit source and save files. Of course, the “one-line” change might mean there genuinely is only one commit that needs merging, but the process is so natural and light-weight that it is not worth making that a special case.\nSuch branches are able to be built prior to merging - this can eliminate the possibility of the merge breaking the build of main, thus reducing the risk in making changes.\n\n\n\nDiagram of a feature branch pipeline\n\n\n\n\n\nA branch holds all the commits for a change - be that a single commit for a one-liner or a sequence of commits as the developer refined the change while making it ready for review and merging into main.\nThe request to merge a branch is made explicitly, but can be as formal or informal as needed by the team. Protection of main can mean that only certain people can perform the merge, or that a review and approval of the change is required before merging it, or both.\nThe action of merging can either simply take all the commits from the branch and add them to main, or that multiple commits in the branch can be “squashed” into one commit - the latter can keep the overall history on main “cleaner” if that’s important to the team.\nmain should always build successfully, enabling the team to choose when to package and deploy.\n\n\n\n\nThe use of branches for concurrently planned activities scales extremely well for busier teams. Additionally, epic and release maintenance branches accommodate specific development workflows and allow the model to scale even further. The latter two branches exist for the duration of the epic or release maintenance and are short-living branches.\nThe implemented changes of the iteration are then delivered collectively as part of the next release. Each development team decides how long an iteration is. We advocate for working towards smaller, quicker release cycles, but this model can also be used with longer iterations. Due to business or technical reasons, the merging of features into the main branch can also be delayed. Although this scenario is a discouraged practice, the recommendation in this case is to group such features into a specific epic branch, as described later.\nThis branching model leverages Git tags to identify the various configurations/versions of the application, such as a release candidate or the version of the application repository that is deployed to production.\nDepending on the type of change, the development workflow can vary. In the standard scenario, developers use the main branch to deliver changes for the next planned release, while the release maintenance branches allow fixing of the current release running in the production runtime environment(s). Using epic branches is optional for development teams, but is a grouping mechanism for multiple features that should be built and tested together, thus allowing teams to increase the concurrency of working on multiple, larger development initiatives of the application. The epic branch also represents a way to manage the lifecycle of features that are not planned for the next planned release. In this way, it is a vehicle to delay merging the set of features into the main branch for a later time.\n\n\n\nsupported development workflows\n\n\nThe main, epic, and release branches are assumed to be protected branches, meaning that no developer can directly push changes to these configurations. It requires developers to make changes on a feature branch and go through the pull request process. Before merging the feature branch into a shared branch (whether it is the main branch or an epic branch), some evidence should be gathered to ensure quality and respect of the coding standards in the enterprise. Peer-reviewed code, a clean pipeline execution, and approvals are examples of such evidence, allowing the development team to confidently merge the feature branch into the target branch. In a continuous integration workflow, integrations are expected to happen early to avoid delaying merge conflicts or merges leading to an unstable build.\n\n\n\nConsistent branch naming conventions help indicate the context for the work that is performed. Throughout this document, the following naming patterns are used:\n\nmain: The only long-living branch which is the only branch from which every release is initially derived\nrelease/rel-2.0.1: The release maintenance branch for an example release named rel-2.0.1\nepic/ai-fraud-detection: An epic branch where “aiFraudDetection” is describing the initiative context (in this example, an initiative to adopt AI technology for fraud detection)\n\nFeature branches also need to relate back to the change request (or issue) from the planning phase and their context. Some examples are shown in the following list:\n\nfeature/new-mortgage-calculation for a planned feature for the next planned release\nhotfix/rel-2.0.1/fix-mortgage-calculation for a fix of the current production version that is running the rel-2.0.1 release\nai-fraud-detection/introduce-ai-model-to-mortgage-calculation for a contribution to the development initiative for adopting AI technology for fraud detection.\n\nA common, recommended practice is to squash the different commits created on the feature branch into a single new commit when merging, which keeps the Git history from becoming cluttered with intermediate work for the feature. This also helps to maintain a tidy history on the main branch with only the important commits.\n\n\n\nSpecific branches, such as main, epic, and release branches can be seen as integration branches, because their purpose is to integrate changes from other branches (typically feature branches). To drive the integration process of changes into a shared branch of code, mechanisms like pull requests are a convenient way as they guide the developers with a streamlined workflow. The number of integration branches required for your development process depends on the needs of the application team. However, while the cost of creating new branches is low, keeping them up-to-date, for instance by integrating release bugfixes from the stabilization phase into concurrent epic branches, can be expensive.\nFor application teams who want to embrace an agile development methodology and who sequentially deliver new releases with limited parallel development initiatives, they can use the main branch and, optionally, the release maintenance branch as integration branches to implement the next planned release and potential bug fixes. The following diagram illustrates a branching model for a Git-based development process with sequential release deliveries.\n\n\n\nBranching diagram showing Git-based development process with sequential release deliveries\n\n\nIf the development teams need to work on a significant development initiative in parallel to the standard scenario of the next planned release, this model allows isolation using the epic branch workflow. The epic branch (epic_1) in the following branching diagram represents an additional integration branch that is used to implement, build, and test multiple features that are planned for the development initiative and can be merged into the main branch at a later time. The team decides which commit/tag of the codebase in the main branch will be used as the base for the epic branch, although it is recommended to start from the last tag for the main branch.\n\n\n\nBranching diagram showing Git-based development process with an epic branch for a larger development solution initiative\n\n\nWhen the work items implemented on the epic branch are planned and ready to be delivered as part of the next planned release, the development team merges the epic branch into the main branch.\nEpic branches can be used to compose various styles of development processes. The documentation for Development process variations provides additional samples.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Git branching model for mainframe development"
    ]
  },
  {
    "objectID": "git-branching-model-for-mainframe-dev.html#workflows-in-this-branching-model",
    "href": "git-branching-model-for-mainframe-dev.html#workflows-in-this-branching-model",
    "title": "Git branching model for mainframe development",
    "section": "",
    "text": "This branching model facilitates three different types of development workflows:\n\n(Default development workflow) Deliver changes with the next planned release: With a single long-living branch, the development process allows developers to work and focus on the next planned release. After planning the work items for the next release, the development team is adding changes to the main branch by merging in pull requests for feature branches.\nImplement a fix for the current production state: This workflow enables development teams to resolve a production problem in the currently-released version of the application by leveraging a release branch that is used for maintenance purposes.\nUse an epic branch for a significant development initiative: Concurrent development activities for a significant solution development initiative that includes multiple planned work items for a later delivery (which could even be starting the development of a future release) are supported by creating an epic branch from a commit point in the history of main.\n\nGit tags are used throughout this process to indicate and label important commits, such as the commit of a release that is built from the main branch, or a maintenance release created from a release maintenance branch.\nThe next sections outline the various tasks and activities performed by the development team in the context of the above three scenarios.\n\n\nThe following diagram depicts the typical workflow to deliver changes for the next planned release. In the default workflow, the development team commits changes to the head of the main branch. The changes of the next planned release are built, packaged, and released from the main branch.\n\n\n\nDiagram showing Git branching workflow supporting a release-based delivery approach\n\n\nDevelopers implement their changes by committing to short-living feature branches (visualized in yellow), and integrate those via pull requests into the long-living main branch (visualized in red), which is configured to be a protected branch.\nAt a high level, the development team works through the following tasks:\n\nNew work items are managed in the backlog. The team decides which work items will be implemented in the next iteration. Each application team can decide on the duration of the iteration (which can also be seen as the development cycle). In the above diagram, three work items (Feature 1, Feature 2, and Feature 3) were selected to be implemented for the next iteration. The development team is responsible for coordinating if features are required to be implemented in a specific order.\nFor each work item, a feature branch is created according to pre-defined naming conventions, allowing the assigned developers to have a copy of the codebase on which they can work in isolation from other concurrent development activities.\n\n\n\nDiagram of the feature branch created off the main branch\n\n\nTo start making the necessary modifications for their development task, developers create a copy of the Git repository on their local workstations through Git’s clone operation. If they already have a local clone of the repository, they can simply update their local clone with the latest changes from the central Git repository by fetching or pulling updates into their local clone. This process makes the feature branch available for the developers to work with on their local workstation. They can then open their local clone of the repository in their integrated development environment (IDE), and switch to the feature branch to make their code changes.\nDevelopers use the Dependency Based Build (DBB) User Build facility of their IDE to validate their code changes before committing the changes to their feature branch and pushing the feature branch with their updates to the central Git repository. (Tip: Feature branches created locally can also be pushed to the central Git server).\n\n\n\nDiagram of the IDE DBB User Build capability\n\n\n:::tip\nThis branching model is also known as a continuous integration model to reduce merge conflicts. While developing on the feature branch, a common practice is for developers to regularly sync their feature branch with the main branch by merging the latest changes from the main branch into their feature branch. This ensures that developers are operating based on a recent state of main, and helps to identify any potential merge conflicts so that they can resolve them in their feature branch.\n:::\nDevelopers test their changes before requesting to integrate them into the shared codebase. For example, they can test the build outputs of the User Build step. For a more integrated experience, the CI/CD pipeline orchestrator can be configured to run a pipeline for the feature branch on the central Git server each time the developers push their committed changes to it. This process will start a consolidated build that includes the changed and impacted programs within the application scope. Unit tests can be automated for this pipeline, as well. To continue even further testing the feature branch, the developer might want to validate the build results in a controlled test environment, which is made possible by an optional process to create a preliminary package for the feature branch.\n\n\n\nDiagram of Feature Branch Pipelines\n\n\nWhen developers feel their code changes are ready to be integrated back into the shared main branch, they create a pull request asking to integrate the changes from their feature branch into the main branch. The pull request process provides the capability to add peer review and approval steps before allowing the changes to be merged. As a basic best practice, the changes must be buildable. If the pull request is associated with a feature branch pipeline, this pipeline can also run automated builds of the code in the pull request along with tests and code quality scans.\nOnce the pull request is merged into the main branch, the next execution of the Basic Build Pipeline will build all the changes (and their impacts) of the iteration based on the main branch.\n\n\n\nBranching diagram showing the Basic Build Pipeline execution after the feature branch’s pull request is merged into the main branch\n\n\nThe pipeline can optionally include a stage to deploy the built artifacts (load modules, DBRMs, and so on) into a shared test environment, as highlighted by the blue DEV-TEST icon in the above diagram. In this DEV-TEST environment, the development team can validate their combined changes. This first test environment helps support a shift-left testing strategy by providing a sandbox with the necessary setup and materials for developers to test their changes early. The installation happens through the packaging and deployment process of a preliminary package that cannot be installed to higher environments (because it is compiled with test options), or alternatively through a simplified script solution performing a copy operation. In the latter, no inventory and no deployment history of the DEV-TEST system exist.\nIn the example scenario for this workflow, the development team decides after implementing Feature 1 and Feature 2 to progress further in the delivery process and build a release candidate package based on the current state of the main branch. With this decision, the development team manually runs the Release Pipeline. This pipeline rebuilds the contributed changes for this iteration - with the compiler options to produce executables optimized for performance rather than for debug. The pipeline includes an additional stage to package the build outputs and create a release candidate package (Package RC1 in the following diagram), which is stored in a binary artifact repository.\n\n\n\nDiagram of the Release Pipeline building the changes for the iteration and packaging build outputs into a release candidate package\n\n\nAlthough not depicted in the above diagram, this point in main’s history can be tagged to identify it as a release candidate.\nThe release candidate package is installed in the various test stages and takes a predefined route. The process can be assisted by the pipeline orchestrator itself, or the development team can use the deployment manager. In the event of a defect being found in the new code of the release candidate package, the developer creates a feature branch from the main branch, corrects the issue, and merges it back into the main branch (while still following the normal pull request process). It is expected that the new release candidate package with the fix is required to pass all the quality gates and to be tested again.\nIn this sample walkthrough of an iteration, the development of the third work item (Feature 3) is started later. The same steps as above apply for the developer of this work item. After merging the changes back into the main branch, the team uses the Basic Build Pipeline to validate the changes in the DEV-TEST environment. To create a release candidate package, they make use of the Release Pipeline. This package (Package RC2 in the following diagram) now includes all the changes delivered for this iteration – Feature 1, Feature 2 and Feature 3.\n\n\n\nDiagram of another Release Pipeline building a new release candidate package after Feature 3 is merged in\n\n\nWhen the release is ready to be shipped after all quality gates have passed successfully and the required approvals have been issued by the appropriate reviewers, the deployment of the package from the binary artifact repository to the production runtime environment is performed via the deployment manager or is initiated from the Release Pipeline.\nFinally, during the release process to the production environment, the state of the repository (that is, the commit) from which the release candidate package was produced is tagged following a semantic versioning strategy. This serves to identify what version is currently in production, and also serves as the baseline reference for the calculation of changes for the next release.\n\n\n\nDiagram of another Release Pipeline building a new release candidate package after Feature 3 is merged in\n\n\n\n\n\n\nThe process of urgent fixes for modules in the production environment follows the fix-forward approach, rather than rolling back the affected modules and reverting to the previous deployed state.\nThe following diagram depicts the maintenance process to deliver a fix or maintenance for the active release in production for the application. The process leverages a release maintenance branch to control and manage the fixes. The purpose of the branch is to add maintenance to a release that is already deployed to the production environment. It does not serve the process to add new functionality to a future release, which is covered by the default workflow or the usage of an epic branch.\n\n\n\nGit branching diagram with a release maintenance branch for the production maintenance process\n\n\nWhen implementing a fix for the current production state, the development team works through the following tasks:\n\nIn the event of a required fix or urgent maintenance for the production runtime, which in this example is currently running the 2.1.0 release, the development team creates a release/rel-2.1.0 branch based on the existing Git tag in the central Git server. The release branch is a protected branch and does not allow developers to directly push commits to this branch.\n\nNote: In the diagrams for this workflow, the release/rel-2.1.0 branch is abbreviated to rel-2.1.0 for readability. Other multi-segmented branch names are similarly abbreviated to their last segment name in the diagrams.\n\nFor each necessary fix, a feature branch is created according to pre-defined naming conventions (for example, rel-2.1.0/hotfix_1, based on the release/rel-2.1.0 branch). This allows the assigned developer to have a copy of the codebase on which they can work in isolation from other development activities.\n\n\n\nBranching diagram focused on the hotfix_1 branch created from the release maintenance branch for release 2.1.0\n\n\nThe developers fetch the feature branch from the central Git repository into their local clone of the repository and switch to that branch to start making the necessary modifications. They leverage the user build facility of their IDE to vet out any syntax issues. They can use a feature branch pipeline to build the changed and impacted files. Optionally, the developer can prepare a preliminary package, which can be used for validating the fix in a controlled test environment.\nThe developer initiates the pull request process, which provides the ability to add peer review and approval steps before allowing the changes to be merged into the release/rel-2.1.0 release maintenance branch.\nA Basic Build Pipeline for the release maintenance branch will build all the changes (and their impacts).\nThe developer requests a Release Pipeline for the release/rel-2.1.0 branch that builds the changes (and their impacts), and that includes the packaging process to create the fix package for the production runtime. The developer will test the package in the applicable test environments, as shown in the following diagram.\n\n\n\nBranching diagram showing the Release Pipeline generating a fix package from the release maintenance branch\n\n\nAfter collecting the necessary approvals, the fix package can be deployed to the production environment. To indicate the new state of the production runtime, the developer creates a Git tag (2.1.1 in this example) for the commit that was used to create the fix package. This tag indicates the currently-deployed version of the application.\nFinally, the developer is responsible for starting the pull request process to merge the changes from the release/rel-2.1.0 branch back to the main branch to also include the fix into the next release.\nThe release/rel-2.1.0 branch is retained in case another fix is needed for the active release. The release maintenance branch becomes obsolete when the next planned release (whose starting point is represented by a more recent commit on the main branch) is deployed to production. In this event, the new commit point on the main branch becomes the baseline for a new release maintenance branch.\n\n\n\n…\n\n\n\n\nLet us now focus on change requests that represent significant work effort and require major changes, for instance, due to updates in regulatory frameworks in the banking or insurance industry, or the need to already kick off the development phase of features not planned to be delivered in the very next release.\nIn these situations, the development team cannot follow the business-as-usual workflow to deliver functionality with the next planned release, because the time and work required breaks out of the traditional durations of one release. For each of these scenarios, the development team is using an epic branch to keep the changes in multiple features separated from the other development activities. It is an integration branch to group and integrate multiple features that are planned for this initiative. Ideally, the team has a dedicated test environment assigned (such as EPIC-DEV-TEST and EPIC-INT-TEST in the following diagram), to also plan and implement any infrastructure updates such as Db2 schema changes.\nTrunk-based development suggests using feature flags within the code to implement complex features via the main workflow while allowing the delay of their activation. Feature flags are often employed so that a given business functionality can be activated at a given date, but be implemented and deployed earlier (whether to dev/test or production environments). We do not see this as a common practice for traditional mainframe languages such as COBOL or PL/I, although some development organizations might apply this practice in mainframe development.\nAll these scenarios lead to the requirement on the development process to implement changes independently from the main workflow.\nNote that the epic branch workflow described in this section is not meant to be used for a single, small feature that a developer wants to hold back for an upcoming release. In those smaller cases, the developer retains the feature branch until the change is planned to be released.\n\n\n\nBranching diagram showing the use of an epic branch for a significant development initiative\n\n\nThe development tasks for a development initiative are:\n\nThe team creates an epic branch from the Git tag representing the current production version of the application, which is at this point the most stable configuration. This process provides them isolation of the codebase from any other ongoing changes for the next iteration(s). In this workflow example, the epic branch is named epic/epic_1, and is abbreviated in the diagrams as epic_1.\nBased on how the work items are distributed between the developers, a feature branch is created according to pre-defined naming conventions such as epic_1/feature_4, based on the epic/epic_1 branch.\nThe developers fetch the feature branch from the central Git repository into their local clone of the repository and switch to that branch to start making the necessary modifications. They leverage the user build facility of their IDE for building and testing individual programs. They can also leverage a feature branch pipeline to build the changed and impacted files. Optionally, the developer can prepare a preliminary package, which can be used for validating the fix in a controlled test environment, such as the EPIC-1-FEATURE-TEST environment shown in the following diagram.\n\n\n\nDiagram showing the epic branching workflow with feature branch pipelines\n\n\nThe developer initiates the pull request process, which provides the ability to add peer review and approval steps before allowing the changes to be merged into the epic branch.\nA Basic Build Pipeline for the epic branch will build all the merged features (both the changes and their impacts) from the point where the epic branch was branched off.\nIt is important that the team frequently incorporates updates that have been implemented for the next release and/or released to production via the default development workflow (with the main branch) into the epic branch to prevent the configurations from diverging too much and making the eventual merge of the epic branch into main difficult. A common practice is to integrate changes from main into the epic branch at least after each completion of a release via the default workflow, in order to merge in the latest stable version updates. More frequent integrations may lead to pulling intermediate versions of features that might not be fully implemented from a business perspective; however, this should not deter developers since the main branch should always be in a buildable state.\nWhen the development team feels that they are ready to prototype the changes for the initiative in the initiative’s test environment, they request a Release Pipeline for the epic branch that builds the changes (and their impacts) and includes the packaging process to create a preliminary package. This preliminary package can then be installed into the initiative’s test environment (for example, the EPIC-DEV-TEST environment). The team will test the package in the assigned test environments for this initiative, as shown in the following diagram.\n\n\n\nDiagram showing the Release Pipeline in an epic branching workflow\n\n\nOnce the team is satisfied with their changes for the development initiative, they plan to integrate the changes of the epic branch into the main branch using the pull request process. This happens when the changes should be released towards production with the next planned iteration. The following diagram depicts of the process of integrating the changes implemented for epic/epic_1 in parallel with the default workflow after three releases.\n\n\n\nDiagram showing the integration of changes from an epic branch into main as a planned deliverable of an upcoming release",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Git branching model for mainframe development"
    ]
  },
  {
    "objectID": "git-branching-model-for-mainframe-dev.html#learn-more",
    "href": "git-branching-model-for-mainframe-dev.html#learn-more",
    "title": "Git branching model for mainframe development",
    "section": "",
    "text": "This page describes our recommended Git branching model and workflows for mainframe development. This model is intended to be used as a template, and can be adjusted, scaled up, or scaled down according to the needs of the development team. Additional variations for the branching strategies and workflows can be found in the Appendix.\nFor recommendations on designing and implementing the workflows described in this branching model, please refer to Pipeline design and implementation supporting the branching model.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Git branching model for mainframe development"
    ]
  },
  {
    "objectID": "build-strategy.html",
    "href": "build-strategy.html",
    "title": "Designing the build strategy",
    "section": "",
    "text": "The build step contains all the steps to compile and link the source files into executable code. This page focuses on the different build strategies to support build scenarios at different phases of application development, and points to the relevant sections within the shared sample scripts via IBM’s zAppBuild repository.\nThere are several different types of possible builds. At a high level, there is a user build, which is a build done by a developer on a single program, and a pipeline build, which will build the single or multiple changes pushed to the Git server.\nDue to the nature of mainframe languages, source files need to be associated to a build type. For the build process, it is important to know what type of processing the file needs so that the correct build process can be associated with it. The correlation can be tracked through a fixed mapping list, through a folder organization, or a file extension.\nWithin the zAppBuild samples on GitHub, a mapping in the file file.properties provides the association of the file to a certain build script.\n\n\nIn an early development phase, developers need the ability to build the single program they are working on, as well as the unit test programs being created.\nThe following integrated development environment (IDE) tools provide an option to enable the developer to compile a selected program in an easy and fast way, without the need to commit or push the change to the repository, while still using the same build options defined for the pipeline build. The purpose is to build fast for the personal testing scenario:\n\nIBM Developer for z/OS (IDz)\nWazi for Visual Studio Code\nWazi for Dev Spaces\n\nAdditional information about performing a user build can be found in the documentation for IBM Dependency Based Build and for the IDEs listed above.\n\n\n\nA pipeline build is generally a build of a set of changes (but could also be a build of a single change), that have been pushed to the Git server. It produces the official binaries, outputs that can be packaged and deployed to different environments, including production. By having the clear definition of what went into each build and the official build outputs, this ensures there are audit records.\n\n\nThe continuous integration/continuous delivery (CI/CD) pipeline can kick off builds for many different scenarios, providing different overrides for each. The build scripts need to handle all these scenarios. Examples of the multiple possible build scenarios include a full build, a dependency build, and a scoped build.\nA pipeline build is also possible for the short-lived topic (or feature) branches. However, the binaries resulting from this particular scenario cannot go to production because they typically lack a review and approval process.\n\n\nA full build compiles the full defined configuration. The full list of source files is provided as input from the pipeline to the build scripts. One strategy is to allow the specification of the build type in the pipeline orchestrator, as demonstrated in the following screenshot of a Jenkins sample pipeline. The build script would then need to handle this input as the build type.\n\n\n\nJenkins build parameterization\n\n\n\n\n\nInstead of a full build, most builds are dependency-based impact builds, only building the changed files and the files that depend on (or are impacted by) those changed files. However, there are different qualities of defining the scope of an impact build, which is reflected in the impact calculation phase of a build.\n\n\nThe easiest strategy is to only build the modified files. How to calculate this depends on the scenario. If it is a topic branch build, this can simply be the modified files identified in the Git commit. If this is a release build, you can use a diff of the current baseline to the previous successful build baseline to calculate the modifications.\nThe next consideration is if you want to include the directly impacted files in the list of buildable files. This calculation for direct dependencies occurs on the source file level. If a copybook is changed, all files using this copybook are directly impacted. This level of impact calculation is managed by the IBM Dependency Based Build server. The server contains a collection representing all direct dependencies in a configuration. Your build script needs to query the dependency-based build server to retrieve the list of impacted files and pass this along to the build script. The use cases can vary. If you want to automatically rebuild programs including a modified copybook, you should pursue the strategy above. Instead of rebuilding all dependent programs, you might want to provide a function for the user to select which, if any, of these impacted files should be rebuilt.\nThe ImpactUtilities.groovy sample script for IBM Dependency Based Build in the public zAppBuild GitHub repository provides an example of this impact calculation.\n\n\n\n\nWhile dependency-based impact builds cover strategies with a good level of automation for the build process, the user-defined build scope provides full flexibility to the user. It is up to the user to provide the list of files being considered in the build. The list could also reference a work item and its related changes. As a variation, the list can include the files from where the impact calculation should be performed.\nA file containing the build list is stored as part of the repository.\n\n\n\nUser-defined build list\n\n\nSimilar to the build scripts, we recommend storing the user-defined build list as part of the Git repository. Additionally, you need to consider a parameterized Jenkins job including the relevant logic, which can serve all three build types: full build, dependency-based impact build, and user-defined build.\n\n\n\n\nAs we now have shown, there are different possible build scenarios. It is important to select the right build types for your setup. Using a mixture, depending on the level, should serve your needs. At each given level the choices will be limited, with the most flexibility at the development level. If you intend to maintain the traditional library population strategy, you can implement user builds to build development activities within the development stage and set up a dependency-based impact build in the integration stage to ensure consistency across the application.\nIt is important to recognize that by moving to Git and using an artifact repository for outputs, you are no longer tied to a library structure. You can now use the same build configuration strategy while pulling any already-built parts out of the artifact repository.\n\n\n\n\nThis page contains reformatted excerpts from Develop Mainframe Software with OpenSource Source code managers and IBM Dependency Based Build.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Designing the build strategy"
    ]
  },
  {
    "objectID": "build-strategy.html#user-build",
    "href": "build-strategy.html#user-build",
    "title": "Designing the build strategy",
    "section": "",
    "text": "In an early development phase, developers need the ability to build the single program they are working on, as well as the unit test programs being created.\nThe following integrated development environment (IDE) tools provide an option to enable the developer to compile a selected program in an easy and fast way, without the need to commit or push the change to the repository, while still using the same build options defined for the pipeline build. The purpose is to build fast for the personal testing scenario:\n\nIBM Developer for z/OS (IDz)\nWazi for Visual Studio Code\nWazi for Dev Spaces\n\nAdditional information about performing a user build can be found in the documentation for IBM Dependency Based Build and for the IDEs listed above.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Designing the build strategy"
    ]
  },
  {
    "objectID": "build-strategy.html#pipeline-build",
    "href": "build-strategy.html#pipeline-build",
    "title": "Designing the build strategy",
    "section": "",
    "text": "A pipeline build is generally a build of a set of changes (but could also be a build of a single change), that have been pushed to the Git server. It produces the official binaries, outputs that can be packaged and deployed to different environments, including production. By having the clear definition of what went into each build and the official build outputs, this ensures there are audit records.\n\n\nThe continuous integration/continuous delivery (CI/CD) pipeline can kick off builds for many different scenarios, providing different overrides for each. The build scripts need to handle all these scenarios. Examples of the multiple possible build scenarios include a full build, a dependency build, and a scoped build.\nA pipeline build is also possible for the short-lived topic (or feature) branches. However, the binaries resulting from this particular scenario cannot go to production because they typically lack a review and approval process.\n\n\nA full build compiles the full defined configuration. The full list of source files is provided as input from the pipeline to the build scripts. One strategy is to allow the specification of the build type in the pipeline orchestrator, as demonstrated in the following screenshot of a Jenkins sample pipeline. The build script would then need to handle this input as the build type.\n\n\n\nJenkins build parameterization\n\n\n\n\n\nInstead of a full build, most builds are dependency-based impact builds, only building the changed files and the files that depend on (or are impacted by) those changed files. However, there are different qualities of defining the scope of an impact build, which is reflected in the impact calculation phase of a build.\n\n\nThe easiest strategy is to only build the modified files. How to calculate this depends on the scenario. If it is a topic branch build, this can simply be the modified files identified in the Git commit. If this is a release build, you can use a diff of the current baseline to the previous successful build baseline to calculate the modifications.\nThe next consideration is if you want to include the directly impacted files in the list of buildable files. This calculation for direct dependencies occurs on the source file level. If a copybook is changed, all files using this copybook are directly impacted. This level of impact calculation is managed by the IBM Dependency Based Build server. The server contains a collection representing all direct dependencies in a configuration. Your build script needs to query the dependency-based build server to retrieve the list of impacted files and pass this along to the build script. The use cases can vary. If you want to automatically rebuild programs including a modified copybook, you should pursue the strategy above. Instead of rebuilding all dependent programs, you might want to provide a function for the user to select which, if any, of these impacted files should be rebuilt.\nThe ImpactUtilities.groovy sample script for IBM Dependency Based Build in the public zAppBuild GitHub repository provides an example of this impact calculation.\n\n\n\n\nWhile dependency-based impact builds cover strategies with a good level of automation for the build process, the user-defined build scope provides full flexibility to the user. It is up to the user to provide the list of files being considered in the build. The list could also reference a work item and its related changes. As a variation, the list can include the files from where the impact calculation should be performed.\nA file containing the build list is stored as part of the repository.\n\n\n\nUser-defined build list\n\n\nSimilar to the build scripts, we recommend storing the user-defined build list as part of the Git repository. Additionally, you need to consider a parameterized Jenkins job including the relevant logic, which can serve all three build types: full build, dependency-based impact build, and user-defined build.\n\n\n\n\nAs we now have shown, there are different possible build scenarios. It is important to select the right build types for your setup. Using a mixture, depending on the level, should serve your needs. At each given level the choices will be limited, with the most flexibility at the development level. If you intend to maintain the traditional library population strategy, you can implement user builds to build development activities within the development stage and set up a dependency-based impact build in the integration stage to ensure consistency across the application.\nIt is important to recognize that by moving to Git and using an artifact repository for outputs, you are no longer tied to a library structure. You can now use the same build configuration strategy while pulling any already-built parts out of the artifact repository.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Designing the build strategy"
    ]
  },
  {
    "objectID": "build-strategy.html#resources",
    "href": "build-strategy.html#resources",
    "title": "Designing the build strategy",
    "section": "",
    "text": "This page contains reformatted excerpts from Develop Mainframe Software with OpenSource Source code managers and IBM Dependency Based Build.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Designing the build strategy"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html",
    "href": "dbb-migration-tool.html",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "IBM® Dependency Based Build (DBB) provides a migration tool that facilitates the copying of source code PDS members into a pre-existing local Git repository in z/OS UNIX System Services (z/OS UNIX), stored on the z/OS File System (zFS). The populated local Git repository can then be committed and pushed to a distributed Git server. During the copy process, the DBB Migration Tool will perform the necessary code page conversion from the z/OS code page to the traditional code page used in Git when applicable, create (or update) the .gitattributes file with the correct encoding mappings, tag the z/OS UNIX files with the appropriate code page values, and (potentially) report on any issues encountered during the migration.\nThe migration tool is bundled as part of the SMP/e installation of the Dependency Based Build toolkit (FMID HBGZ110) and is typically found under the migration/bin sub-directory of the DBB installation folder (which by default is /usr/lpp/IBM/dbb, unless customized during installation). The setup, general usage, and various options provided by this tool to assist in the migration can be found on the IBM Documentation website.\nThe following sections of this page will showcase various migration scenarios using the context that has been described in Managing code page conversion. The intent is not to provide an exhaustive set of scenarios supported by the migration tool, but rather to focus on common use cases. The scenarios are:\n\nMigration using the default settings\nMigration using the pdsEncoding Mapping Rule\nDetection of non-roundtripable characters\nDetection of non-printable characters\n\n\n\n\nTo illustrate the scenarios, the following sample PDSs were constructed to highlight some specific migration situations that may be encountered and how to mitigate potential issues:\n\nMIGRATE.TECHDOC.SOURCE\nMIGRATE.TECHDOC.COPYBOOK\n\n\n\nContent of the MIGRATE.TECHDOC.SOURCE dataset:\n\n\n\n\nMember\n\n\nDescription\n\n\n\n\n\n\nIBM037\n\n\nMember that has been created using the code page of IBM-037.Example: IBM-037 Code Page\n          void main(int argc, char *argv[])\n        \nNote: Under the IBM-037 code page, the hexadecimal codes for the [ and the ] characters are x’BA’ and x’BB’, respectively.\n\n\n\n\nIBM1047\n\n\nMember that has been created using the code page of IBM-1047.Example: IBM-1047 Code Page\n            void main(int argc, char *argv[])\n          \nNote: Under the IBM-1047 code page, the hexadecimal codes for the [ and the ] characters are x’AD’ and x’BD’, respectively.\n\n\n\n\n\n\n\nContent of the MIGRATE.TECHDOC.COPYBOOK dataset:\n\n\n\n\nMember\n\n\nDescription\n\n\n\n\n\n\nNROUND\n\n\nMember that contains non-roundtripable characters.Example: Non-roundtripable characters\n          Line with CHAR_NL (0x15)  ␤\n          Line with CHAR_CR (0x0D)  ␍\n          Line with CHAR_LF (0x25)  ␊\n          Line with CHAR_SHIFT_IN (0x0F)  _\n          Line with CHAR_SHIFT_OUT(0x0E)  _\n          Line with empty CHAR_SHIFT_OUT(0x0E) and CHAR_SHIFT_IN (0x0F)  __\n        \n\n\n\n\nNPRINT\n\n\nMember that contains non-printable characters.Example: Non-Printable Characters\n          Line with (0x06)  ?\n          Line with (0x07)  \n          Line with (0x1B)  \n        \n\n\n\n\nHEXCODED\n\n\nMember that contains non-printable and non-roundtripable characters.Example: Hexadecimal Coded Characters\n          01   DFHBMSCA.\n            02 DFHBMPEM  PICTURE X VALUE IS '_'.\n            02 DFHBMPNL  PICTURE X VALUE IS '␤'.\n            02 DFHBMPFF  PICTURE X VALUE IS '_'.\n            02 DFHBMPCR  PICTURE X VALUE IS '␍'.\n            02 DFHBMASK  PICTURE X VALUE IS '0'.\n            02 DFHBMUNP  PICTURE X VALUE IS ' '.\n        \n\n\n\n\nHEXVALUE\n\n\nMember that contains the suggested transformation of the non-printable or non-roundtripable characters contained in the “HEXCODED” member in a more suitable format.Example: Hexadecimal Values\n          01   DFHBMSCA.\n            02 DFHBMPEM  PICTURE X VALUE IS X'19'.\n            02 DFHBMPNL  PICTURE X VALUE IS X'15'.\n            02 DFHBMPFF  PICTURE X VALUE IS X'0C'.\n            02 DFHBMPCR  PICTURE X VALUE IS X'0D'.\n            02 DFHBMASK  PICTURE X VALUE IS '0'.\n            02 DFHBMUNP  PICTURE X VALUE IS ' '.\n       \n\n\n\n\n\n\n\n\n\n\nIn this scenario, we will be migrating all the source members in the MIGRATE.TECHDOC.SOURCE PDS using the default settings into a local z/OS UNIX Git Repository under /u/user1/Migration. This is the most simplistic form of invoking the migration tool and, in most cases, satisfies most needs.\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -m Mapping Rule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true] SOURCE\n\nSetting dbb.file.tagging = true\nLocal GIT repository: /u/user1/Migration\nMapping: MappingRule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true]\nMappingRuleId: com.ibm.dbb.migration.MappingRule\nMappingRuleAttrs: [hlq:MIGRATE.TECHDOC, extension:SRC, toLower:true]\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set SOURCE\nCopying MIGRATE.TECHDOC.SOURCE(IBM037) to /u/user1/Migration/source/ibm037.src using default encoding\nCopying MIGRATE.TECHDOC.SOURCE(IBM1047) to /u/user1/Migration/source/ibm1047.src using default encoding\n** Build finished\nNote that the migration tool is using a default encoding, which is IBM-1047.\nAn examination of the files on the local z/OS UNIX Git repository will reveal that the files were copied and were tagged with the default code page of IBM-1047.\nls -alT /u/user1/Migration/source\ntotal 64\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  4 12:33 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  4 12:33 ..\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 12:33 ibm037.src\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 12:33 ibm1047.src\nAdditionally, the .gitattributes file was created (or updated) with the correct encoding mappings. All source artifacts, except those tagged as binary (to be discussed later), will be stowed in the distributed Git server using the UTF-8 code page (as defined by the git-encoding=utf-8 parameter), whereas any artifacts that are copied from the distributed Git server to z/OS will be translated to the IBM-1047 code page (as defined by the zos-working-tree-encoding=ibm-1047 parameter). The documentation on Defining the code page of files in Git provides more information.\ncat /u/user1/Migration/.gitattributes\nsource/*.src zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\nAt this point, Git actions such as add/commit/push can be performed on the migrated source artifacts to the distributed Git server.\ngit add .\ngit commit -m \"Simple Migration Example\"\n[main 6436b92] Simple Migration Example\n 3 files changed, 5 insertions(+)\n create mode 100644 .gitattributes\n create mode 100644 source/ibm037.src\n create mode 100644 source/ibm1047.src\ngit push\nCounting objects: 6, done.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 634 bytes | 211.00 KiB/s, done.\nTotal 6 (delta 0), reused 0 (delta 0)\nTo github.ibm.com:user1/Migration.git\n   3d2962a..6436b92  main -&gt; main\nOnce the Git push command has completed to the distributed Git server, the resulting files should be translated into the correct UTF-8 code page.\n\n\n\nDBB Migration Tool scenario 1, .gitattributes file\n\n\n\n\n\nDBB Migration Tool scenario 1, ibm1047.src file\n\n\n\n\n\nDBB Migration Tool scenario 1, ibm037.src file\n\n\nHowever, as indicated in the last picture above, the ibm037.src file reveals an encoding issue. This will be discussed in the next scenario.\n\n\n\nIn this scenario, we will be migrating a single source member from the MIGRATE.TECHDOC.SOURCE PDS using the pdsEncoding keyword in the mapping rule to override the default encoding. Recall that from the previous migration scenario, there was an encoding issue with the final copy of the ibm037.src file in the distributed Git server. This occurred because the z/OS file was written using the IBM-037 code page instead of the default IBM-1047 code page. The problem is not in how the file was encoded, but rather how the Rocket Git client converted the file when sending it to Git.\nThis can be a common occurrence for source files that pre-date the introduction of z/OS Unix System Services (z/OS UNIX), and where high-level languages, such as C/370, were utilized. As stated previously, determining the original encoding can be a challenge since the code page used to create the file is generally specified in the 3270 Emulator (TN3270) client session set-up. Therefore, an analysis of the z/OS source should be performed to determine the original code page used to create the source. To determine the code page used to create files through ISPF, an alternate option is to ask the developers which code page they are using to edit the files through their 3270 connections.\nTo correct the encoding issue identified in the previous scenario, we will use the pdsEncoding keyword of the mapping rule to override the default IBM-1047 code page with IBM-037 for the offending member.\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -m MappingRule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true,pdsEncoding:IBM-037] \"SOURCE(IBM037)\"\n\nSetting dbb.file.tagging = true\nLocal GIT repository: /u/user1/Migration\nMapping: MappingRule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true,pdsEncoding:IBM-037]\nMappingRuleId: com.ibm.dbb.migration.MappingRule\nMappingRuleAttrs: [hlq:MIGRATE.TECHDOC, extension:SRC, toLower:true, pdsEncoding:IBM-037]\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set SOURCE(IBM037)\nCopying MIGRATE.TECHDOC.SOURCE(IBM037) to /u/user1/Migration/source/ibm037.src using IBM-037\n** Build finished\nNote that the migration tool is using the override encoding of IBM-037 for a named member. This override does not necessarily have to be performed on a member-by-member basis, as the migration tool supports the ability to override the encoding for an entire PDS being migrated.\nAn examination of the files on the local z/OS UNIX Git repository will reveal that the file was copied and tagged with the override code page of IBM-037.\nls -alT /u/user1/Migration/source\ntotal 64\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  4 13:10 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  4 13:10 ..\nt IBM-037     T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 14:54 ibm037.src\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 13:10 ibm1047.src\nAdditionally, the .gitattributes file was updated with the correct encoding mappings:\ncat /u/user1/Migration/.gitattributes\nsource/*.src zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\nsource/*.src zos-working-tree-encoding=IBM-037 git-encoding=utf-8\nHowever, in this example you will notice a slight anomaly in that there are two (2) entries for the same sub-folder source/*.src. This will cause an encoding conflict during the Git add action. To correct this situation, the .gitattributes file must be manually updated to add the file name. Wild cards can be used in the file name should there be more than one member that matches this situation. The order of these entries is important, with the last entry taking precedence. In some cases, additional wild carding may be required to prevent further conflicts.\ncat /u/user1/Migration/.gitattributes\nsource/*.src zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\nsource/ibm037.src zos-working-tree-encoding=IBM-037 git-encoding=utf-8\nOnce the correction has been made to the .gitattributes file, the Git commit and push actions can be performed on the updated files to the distributed Git server:\ngit add .\ngit commit -m \"IBM037 Code Page Fix\"\n[main 107c86c] IBM037 Code Page Fix\n 2 files changed, 2 insertions(+), 1 deletion(-)\ngit push\nCounting objects: 5, done.\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 485 bytes | 485.00 KiB/s, done.\nTotal 5 (delta 2), reused 0 (delta 0)\nTo github.ibm.com:user1/Migration.git\n   6436b92..107c86c  main -&gt; main   3d2962a..6436b92  main -&gt; main\nNow when examining the offending file on the distributed Git server, the contents of the file should be translated correctly:\n\n\n\nDBB Migration Tool scenario 2, ibm037.src file\n\n\nThe probability that members of a single PDS were written using a different code page, though possible, is extremely low. However, it is worth pointing out that it could expose an issue in how the migration tool generates the .gitattributes file.\n\n\n\nIn this scenario, we will examine how the migration tool can assist in the detection of what is known as non-roundtripable characters. The documentation on Managing non-printable and non-roundtripable characters provides more conceptual background information. To illustrate this, we will be migrating the single source member MIGRATE.TECHDOC.COPYBOOK(NROUND), which contains both types of characters.\nDuring the migration of a PDS member to a z/OS UNIX file, the migration tool will scan the content of the file to see if it detects any non-roundtripable characters. These characters are defined in the migrate.groovy script and are:\n@Field def CHAR_NL = 0x15\n@Field def CHAR_CR = 0x0D\n@Field def CHAR_LF = 0x25\n@Field def CHAR_SHIFT_IN = 0x0F\n@Field def CHAR_SHIFT_OUT = 0x0E\nIf detected, the migration tool will emit a diagnostic message in the console log and will copy the member to z/OS UNIX as binary and therefore no code page conversion will be performed:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] \"COPYBOOK(NROUND)\"\n\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK(NROUND)\n[WARNING] Copying MIGRATE.TECHDOC.COPYBOOK(NROUND) to /u/user1/Migration/copybook/nround.cpy\n ! Possible migration issue:\n      Line 2 contains non-roundtripable characters:\n        Char 0x15 at column 27\n      Line 3 contains non-roundtripable characters:\n        Char 0x0D at column 27\n      Line 4 contains non-roundtripable characters:\n        Char 0x25 at column 27\n      Line 7 contains non-roundtripable characters:\n        Empty Shift Out and Shift In at column 74\n\n ! Copying using BINARY mode\n** Build finished\nNote that the migration tool has detected numerous non-roundtripable characters on various lines and has performed the copy as binary.\nAn examination of the files on the local z/OS UNIX Git Repository will reveal that the file was copied but left untagged (this is a current known limitation for the DBB Toolkit in its 1.1.3 version, and a workaround is available upon request).\nls -alT /u/user1/Migration/copybook\ntotal 48\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  6 13:42 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  6 13:42 ..\n- untagged    T=off -rw-r--r--   1 USER1    OMVS         560 May  6 13:42 nround.cpy\nTo be processed by the Rocket Git client when performing the Git add command, the file should be manually tagged as binary first. To correctly tag the file as binary, use the chtag -b command prior to performing the Git add command:\nls -alT /u/user1/Migration/copybook\ntotal 48\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  8 13:10 .\n                    drwxr-xr-x   5 USER1    OMVS        8192 May  8 13:12 ..\nb binary      T=off -rw-r--r--   1 USER1    OMVS         560 May  8 13:10 nround.cpy\nAdditionally, the .gitattributes file was automatically updated by the migration script to indicate that the file is mapped as binary:\ncat /u/user1/Migration/.gitattributes\ncopybook/nround.cpy binary\nDuring the Git push to the distributed Git server, Git will treat this as a binary file and no conversion to UTF-8 will take place. In essence, the resulting file on the distributed Git server will be the original contents of the PDS member, in EBCDIC.\ngit add .\nwarning: copybook/nround.cpy added file have been automatically tagged BINARY because they were untagged yet the .gitattributes file specifies they should be tagged\ngit commit -m \"Binary File\"\n[main 0213795] Binary File\n 2 files changed, 1 insertion(+)\n create mode 100644 .gitattributes\n create mode 100644 copybook/nround.cpy\ngit push\nCounting objects: 5, done.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (5/5), 598 bytes | 598.00 KiB/s, done.\nTotal 5 (delta 0), reused 0 (delta 0)\nTo github.ibm.com:user1/Migration.git\n   e901992..0213795  main -&gt; main\nOnce the Git push action has completed to the distributed Git server, the resulting file will be treated as binary:\n\n\n\nDBB Migration Tool scenario 3, binary nround.cpy file in Git\n\n\nThis may not be an ideal situation as described in the documentation on Managing non-printable and non-roundtripable characters, and should be corrected/reconciled before continuing with the migration.\n\n\n\nIn this final scenario, we will examine how the migration tool can assist in the detection of what is known as non-printable characters. The documentation on Managing non-printable and non-roundtripable characters provides more conceptual background information. To illustrate this, we will again migrate the single source member MIGRATE.TECHDOC.COPYBOOK(NPRINT), which only contains non-printable characters.\nDuring the migration of a PDS member to a z/OS UNIX file, the migration tool will scan the content of the file to see if it detects any non-printable characters. These characters are defined as any hexadecimal values that are an EBCDIC x’40’ or less, and not one of the five (5) non-roundtripable characters.\nThe migration tool provides three (3) options on how to handle and report on these characters:\n\nDo Not Check: The non-printable characters are not researched; the member is simply copied as text. Code page conversion will occur.\nInfo: The script will emit an Informational diagnostic message in the console log if a non-printable character is detected and the file is copied as text. Code page conversion will occur.\nWarning: The script will emit a Warning diagnostic message in the console log if a non-printable character is detected and the file is copied as binary. No code page conversion will occur, and the offending characters will be treated like non-roundtripable characters.\n\nIt should be noted that for the “Do Not Check” and “Info” options, although code page conversion is taking place, this could cause an issue later. The file may not be easily maintained with a distributed editor, and you run the risk of having corrupted files.\nControlling what level of checking should be performed during the migration is done via the optional scan level -np, --non-printable &lt;level&gt; parameter switch passed to the migration script. If the scan level parameter is not specified, the non-printable characters are not checked (equates to the “Do Not Check” option).\n\n\nExample output for migration performed with scan level set to info:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -np info -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] \"COPYBOOK(NPRINT)\"\n\nNon-printable scan level is info\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK(NPRINT)\n[INFO] Copying MIGRATE.TECHDOC.COPYBOOK(NPRINT) to /u/user1/Migration/copybook/nprint.cpy using IBM-037\n ! Possible migration issue:\n      Line 2 contains non-printable characters:\n        Char 0x00 at column 19\n      Line 3 contains non-printable characters:\n        Char 0x06 at column 19\n      Line 4 contains non-printable characters:\n        Char 0x07 at column 19\n      Line 5 contains non-printable characters:\n        Char 0x1B at column 19\n\n** Build finished\nNote that the migration tool has detected numerous non-printable characters on various lines and has performed the copy as text and will be tagged on z/OS UNIX using the supplied encoding of IBM-037:\nls -alT /u/user1/Migration/copybook\ntotal 64\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  6 15:39 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  6 14:55 ..\nt IBM-037     T=on  -rw-r--r--   1 USER1    OMVS         114 May  6 15:39 nprint.cpy\n\n\n\nExample output for migration performed with scan level set to warning:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -np warning -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] \"COPYBOOK(NPRINT)\"\n\nNon-printable scan level is warning\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK(NPRINT)\n[WARNING] Copying MIGRATE.TECHDOC.COPYBOOK(NPRINT) to /u/user1/Migration/copybook/nprint.cpy\n ! Possible migration issue:\n      Line 2 contains non-printable characters:\n        Char 0x06 at column 19\n      Line 3 contains non-printable characters:\n        Char 0x07 at column 19\n      Line 4 contains non-printable characters:\n        Char 0x1B at column 19\n\n ! Copying using BINARY mode\n** Build finished\nNote that the migration tool has detected numerous non-printable characters on various lines and has performed the copy as binary. The file remains untagged on z/OS UNIX (the manual tagging of the file is still required):\nls -alT /u/user1/Migration/copybook\ntotal 48\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  6 17:17 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  6 17:17 ..\n- untagged    T=off -rw-r--r--   1 USER1    OMVS         320 May  6 17:17 nprint.cpy\n\n\n\n\n\nA strategy must be decided on how to handle both non-printable and non-roundtripable characters found in source members that are to be migrated from z/OS PDSs to Git. The HEXCODED member, though not demonstrated in the above scenarios, is a common occurrence in many older legacy applications. Source code members such as this need to be identified and transformed to that shown in the HEXVALUE member if the goal is to manage the source seamlessly using the modernized tooling provided through Git. For more conceptual background information, please refer to Managing non-printable and non-roundtripable characters.\nThe DBB Migration Tool provides an option to perform a scan of the z/OS PDSs to assist in the analysis and correction/reconciliation of these situations prior to performing the copy to the local Git repository. Invoking this scan is done via the optional preview -p, --preview parameter switch and will bypass the copy. The default is “No Preview”. An example of the preview follows:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -np warning -p -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] COPYBOOK\n\nNon-printable scan level is warning\nPreview flag is specified, no members will be copied to HFS\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK\n[WARNING] Previewing MIGRATE.TECHDOC.COPYBOOK(HEXCODED)\n ! Possible migration issue:\n      Line 3 contains non-printable characters:\n        Char 0x19 at column 37\n      Line 4 contains non-roundtripable characters:\n        Char 0x15 at column 37\n      Line 5 contains non-printable characters:\n        Char 0x0C at column 37\n      Line 6 contains non-roundtripable characters:\n        Char 0x0D at column 37\n\n ! Will copy using BINARY mode\nPreviewing MIGRATE.TECHDOC.COPYBOOK(HEXVALUE). Using IBM-037.\n[WARNING] Previewing MIGRATE.TECHDOC.COPYBOOK(NPRINT)\n ! Possible migration issue:\n      Line 2 contains non-printable characters:\n        Char 0x06 at column 19\n      Line 3 contains non-printable characters:\n        Char 0x07 at column 19\n      Line 4 contains non-printable characters:\n        Char 0x1B at column 19\n\n ! Will copy using BINARY mode\n[WARNING] Previewing MIGRATE.TECHDOC.COPYBOOK(NROUND)\n ! Possible migration issue:\n      Line 2 contains non-roundtripable characters:\n        Char 0x15 at column 27\n      Line 3 contains non-roundtripable characters:\n        Char 0x0D at column 27\n      Line 4 contains non-roundtripable characters:\n        Char 0x25 at column 27\n      Line 5 contains non-printable characters:\n        Char 0x0F at column 33\n      Line 7 contains non-roundtripable characters:\n        Empty Shift Out and Shift In at column 74\n\n ! Will copy using BINARY mode\nWith the -l, --log option, a log file can be created to contain all the messages about the migration process, including the non-printable and non-roundtripable characters encountered during the scan. This log file can be used by the developers to perform the necessary changes in their original source code members prior to the real migration process.\nMany other options of the Mapping Rule parameter can be leveraged to control the behavior of the DBB Migration Tool. These options are described on the IBM Documentation website.\n\n\n\nFor specific information on using the DBB Migration Tool to migrate from IBM Engineering Workflow Management (EWM) to Git, please reference Migrating from IBM Engineering Workflow Management (EWM) to GIT.\n\n\n\n\nThis page contains reformatted excerpts from Managing the code page conversion when migrating z/OS source files to Git.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html#dbb-migration-tool-overview",
    "href": "dbb-migration-tool.html#dbb-migration-tool-overview",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "IBM® Dependency Based Build (DBB) provides a migration tool that facilitates the copying of source code PDS members into a pre-existing local Git repository in z/OS UNIX System Services (z/OS UNIX), stored on the z/OS File System (zFS). The populated local Git repository can then be committed and pushed to a distributed Git server. During the copy process, the DBB Migration Tool will perform the necessary code page conversion from the z/OS code page to the traditional code page used in Git when applicable, create (or update) the .gitattributes file with the correct encoding mappings, tag the z/OS UNIX files with the appropriate code page values, and (potentially) report on any issues encountered during the migration.\nThe migration tool is bundled as part of the SMP/e installation of the Dependency Based Build toolkit (FMID HBGZ110) and is typically found under the migration/bin sub-directory of the DBB installation folder (which by default is /usr/lpp/IBM/dbb, unless customized during installation). The setup, general usage, and various options provided by this tool to assist in the migration can be found on the IBM Documentation website.\nThe following sections of this page will showcase various migration scenarios using the context that has been described in Managing code page conversion. The intent is not to provide an exhaustive set of scenarios supported by the migration tool, but rather to focus on common use cases. The scenarios are:\n\nMigration using the default settings\nMigration using the pdsEncoding Mapping Rule\nDetection of non-roundtripable characters\nDetection of non-printable characters",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html#example-setup",
    "href": "dbb-migration-tool.html#example-setup",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "To illustrate the scenarios, the following sample PDSs were constructed to highlight some specific migration situations that may be encountered and how to mitigate potential issues:\n\nMIGRATE.TECHDOC.SOURCE\nMIGRATE.TECHDOC.COPYBOOK\n\n\n\nContent of the MIGRATE.TECHDOC.SOURCE dataset:\n\n\n\n\nMember\n\n\nDescription\n\n\n\n\n\n\nIBM037\n\n\nMember that has been created using the code page of IBM-037.Example: IBM-037 Code Page\n          void main(int argc, char *argv[])\n        \nNote: Under the IBM-037 code page, the hexadecimal codes for the [ and the ] characters are x’BA’ and x’BB’, respectively.\n\n\n\n\nIBM1047\n\n\nMember that has been created using the code page of IBM-1047.Example: IBM-1047 Code Page\n            void main(int argc, char *argv[])\n          \nNote: Under the IBM-1047 code page, the hexadecimal codes for the [ and the ] characters are x’AD’ and x’BD’, respectively.\n\n\n\n\n\n\n\nContent of the MIGRATE.TECHDOC.COPYBOOK dataset:\n\n\n\n\nMember\n\n\nDescription\n\n\n\n\n\n\nNROUND\n\n\nMember that contains non-roundtripable characters.Example: Non-roundtripable characters\n          Line with CHAR_NL (0x15)  ␤\n          Line with CHAR_CR (0x0D)  ␍\n          Line with CHAR_LF (0x25)  ␊\n          Line with CHAR_SHIFT_IN (0x0F)  _\n          Line with CHAR_SHIFT_OUT(0x0E)  _\n          Line with empty CHAR_SHIFT_OUT(0x0E) and CHAR_SHIFT_IN (0x0F)  __\n        \n\n\n\n\nNPRINT\n\n\nMember that contains non-printable characters.Example: Non-Printable Characters\n          Line with (0x06)  ?\n          Line with (0x07)  \n          Line with (0x1B)  \n        \n\n\n\n\nHEXCODED\n\n\nMember that contains non-printable and non-roundtripable characters.Example: Hexadecimal Coded Characters\n          01   DFHBMSCA.\n            02 DFHBMPEM  PICTURE X VALUE IS '_'.\n            02 DFHBMPNL  PICTURE X VALUE IS '␤'.\n            02 DFHBMPFF  PICTURE X VALUE IS '_'.\n            02 DFHBMPCR  PICTURE X VALUE IS '␍'.\n            02 DFHBMASK  PICTURE X VALUE IS '0'.\n            02 DFHBMUNP  PICTURE X VALUE IS ' '.\n        \n\n\n\n\nHEXVALUE\n\n\nMember that contains the suggested transformation of the non-printable or non-roundtripable characters contained in the “HEXCODED” member in a more suitable format.Example: Hexadecimal Values\n          01   DFHBMSCA.\n            02 DFHBMPEM  PICTURE X VALUE IS X'19'.\n            02 DFHBMPNL  PICTURE X VALUE IS X'15'.\n            02 DFHBMPFF  PICTURE X VALUE IS X'0C'.\n            02 DFHBMPCR  PICTURE X VALUE IS X'0D'.\n            02 DFHBMASK  PICTURE X VALUE IS '0'.\n            02 DFHBMUNP  PICTURE X VALUE IS ' '.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html#migration-scenarios",
    "href": "dbb-migration-tool.html#migration-scenarios",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "In this scenario, we will be migrating all the source members in the MIGRATE.TECHDOC.SOURCE PDS using the default settings into a local z/OS UNIX Git Repository under /u/user1/Migration. This is the most simplistic form of invoking the migration tool and, in most cases, satisfies most needs.\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -m Mapping Rule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true] SOURCE\n\nSetting dbb.file.tagging = true\nLocal GIT repository: /u/user1/Migration\nMapping: MappingRule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true]\nMappingRuleId: com.ibm.dbb.migration.MappingRule\nMappingRuleAttrs: [hlq:MIGRATE.TECHDOC, extension:SRC, toLower:true]\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set SOURCE\nCopying MIGRATE.TECHDOC.SOURCE(IBM037) to /u/user1/Migration/source/ibm037.src using default encoding\nCopying MIGRATE.TECHDOC.SOURCE(IBM1047) to /u/user1/Migration/source/ibm1047.src using default encoding\n** Build finished\nNote that the migration tool is using a default encoding, which is IBM-1047.\nAn examination of the files on the local z/OS UNIX Git repository will reveal that the files were copied and were tagged with the default code page of IBM-1047.\nls -alT /u/user1/Migration/source\ntotal 64\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  4 12:33 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  4 12:33 ..\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 12:33 ibm037.src\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 12:33 ibm1047.src\nAdditionally, the .gitattributes file was created (or updated) with the correct encoding mappings. All source artifacts, except those tagged as binary (to be discussed later), will be stowed in the distributed Git server using the UTF-8 code page (as defined by the git-encoding=utf-8 parameter), whereas any artifacts that are copied from the distributed Git server to z/OS will be translated to the IBM-1047 code page (as defined by the zos-working-tree-encoding=ibm-1047 parameter). The documentation on Defining the code page of files in Git provides more information.\ncat /u/user1/Migration/.gitattributes\nsource/*.src zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\nAt this point, Git actions such as add/commit/push can be performed on the migrated source artifacts to the distributed Git server.\ngit add .\ngit commit -m \"Simple Migration Example\"\n[main 6436b92] Simple Migration Example\n 3 files changed, 5 insertions(+)\n create mode 100644 .gitattributes\n create mode 100644 source/ibm037.src\n create mode 100644 source/ibm1047.src\ngit push\nCounting objects: 6, done.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 634 bytes | 211.00 KiB/s, done.\nTotal 6 (delta 0), reused 0 (delta 0)\nTo github.ibm.com:user1/Migration.git\n   3d2962a..6436b92  main -&gt; main\nOnce the Git push command has completed to the distributed Git server, the resulting files should be translated into the correct UTF-8 code page.\n\n\n\nDBB Migration Tool scenario 1, .gitattributes file\n\n\n\n\n\nDBB Migration Tool scenario 1, ibm1047.src file\n\n\n\n\n\nDBB Migration Tool scenario 1, ibm037.src file\n\n\nHowever, as indicated in the last picture above, the ibm037.src file reveals an encoding issue. This will be discussed in the next scenario.\n\n\n\nIn this scenario, we will be migrating a single source member from the MIGRATE.TECHDOC.SOURCE PDS using the pdsEncoding keyword in the mapping rule to override the default encoding. Recall that from the previous migration scenario, there was an encoding issue with the final copy of the ibm037.src file in the distributed Git server. This occurred because the z/OS file was written using the IBM-037 code page instead of the default IBM-1047 code page. The problem is not in how the file was encoded, but rather how the Rocket Git client converted the file when sending it to Git.\nThis can be a common occurrence for source files that pre-date the introduction of z/OS Unix System Services (z/OS UNIX), and where high-level languages, such as C/370, were utilized. As stated previously, determining the original encoding can be a challenge since the code page used to create the file is generally specified in the 3270 Emulator (TN3270) client session set-up. Therefore, an analysis of the z/OS source should be performed to determine the original code page used to create the source. To determine the code page used to create files through ISPF, an alternate option is to ask the developers which code page they are using to edit the files through their 3270 connections.\nTo correct the encoding issue identified in the previous scenario, we will use the pdsEncoding keyword of the mapping rule to override the default IBM-1047 code page with IBM-037 for the offending member.\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -m MappingRule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true,pdsEncoding:IBM-037] \"SOURCE(IBM037)\"\n\nSetting dbb.file.tagging = true\nLocal GIT repository: /u/user1/Migration\nMapping: MappingRule[hlq:MIGRATE.TECHDOC,extension:SRC,toLower:true,pdsEncoding:IBM-037]\nMappingRuleId: com.ibm.dbb.migration.MappingRule\nMappingRuleAttrs: [hlq:MIGRATE.TECHDOC, extension:SRC, toLower:true, pdsEncoding:IBM-037]\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set SOURCE(IBM037)\nCopying MIGRATE.TECHDOC.SOURCE(IBM037) to /u/user1/Migration/source/ibm037.src using IBM-037\n** Build finished\nNote that the migration tool is using the override encoding of IBM-037 for a named member. This override does not necessarily have to be performed on a member-by-member basis, as the migration tool supports the ability to override the encoding for an entire PDS being migrated.\nAn examination of the files on the local z/OS UNIX Git repository will reveal that the file was copied and tagged with the override code page of IBM-037.\nls -alT /u/user1/Migration/source\ntotal 64\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  4 13:10 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  4 13:10 ..\nt IBM-037     T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 14:54 ibm037.src\nt IBM-1047    T=on  -rw-r--r--   1 USER1    OMVS          61 May  4 13:10 ibm1047.src\nAdditionally, the .gitattributes file was updated with the correct encoding mappings:\ncat /u/user1/Migration/.gitattributes\nsource/*.src zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\nsource/*.src zos-working-tree-encoding=IBM-037 git-encoding=utf-8\nHowever, in this example you will notice a slight anomaly in that there are two (2) entries for the same sub-folder source/*.src. This will cause an encoding conflict during the Git add action. To correct this situation, the .gitattributes file must be manually updated to add the file name. Wild cards can be used in the file name should there be more than one member that matches this situation. The order of these entries is important, with the last entry taking precedence. In some cases, additional wild carding may be required to prevent further conflicts.\ncat /u/user1/Migration/.gitattributes\nsource/*.src zos-working-tree-encoding=ibm-1047 git-encoding=utf-8\nsource/ibm037.src zos-working-tree-encoding=IBM-037 git-encoding=utf-8\nOnce the correction has been made to the .gitattributes file, the Git commit and push actions can be performed on the updated files to the distributed Git server:\ngit add .\ngit commit -m \"IBM037 Code Page Fix\"\n[main 107c86c] IBM037 Code Page Fix\n 2 files changed, 2 insertions(+), 1 deletion(-)\ngit push\nCounting objects: 5, done.\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 485 bytes | 485.00 KiB/s, done.\nTotal 5 (delta 2), reused 0 (delta 0)\nTo github.ibm.com:user1/Migration.git\n   6436b92..107c86c  main -&gt; main   3d2962a..6436b92  main -&gt; main\nNow when examining the offending file on the distributed Git server, the contents of the file should be translated correctly:\n\n\n\nDBB Migration Tool scenario 2, ibm037.src file\n\n\nThe probability that members of a single PDS were written using a different code page, though possible, is extremely low. However, it is worth pointing out that it could expose an issue in how the migration tool generates the .gitattributes file.\n\n\n\nIn this scenario, we will examine how the migration tool can assist in the detection of what is known as non-roundtripable characters. The documentation on Managing non-printable and non-roundtripable characters provides more conceptual background information. To illustrate this, we will be migrating the single source member MIGRATE.TECHDOC.COPYBOOK(NROUND), which contains both types of characters.\nDuring the migration of a PDS member to a z/OS UNIX file, the migration tool will scan the content of the file to see if it detects any non-roundtripable characters. These characters are defined in the migrate.groovy script and are:\n@Field def CHAR_NL = 0x15\n@Field def CHAR_CR = 0x0D\n@Field def CHAR_LF = 0x25\n@Field def CHAR_SHIFT_IN = 0x0F\n@Field def CHAR_SHIFT_OUT = 0x0E\nIf detected, the migration tool will emit a diagnostic message in the console log and will copy the member to z/OS UNIX as binary and therefore no code page conversion will be performed:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] \"COPYBOOK(NROUND)\"\n\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK(NROUND)\n[WARNING] Copying MIGRATE.TECHDOC.COPYBOOK(NROUND) to /u/user1/Migration/copybook/nround.cpy\n ! Possible migration issue:\n      Line 2 contains non-roundtripable characters:\n        Char 0x15 at column 27\n      Line 3 contains non-roundtripable characters:\n        Char 0x0D at column 27\n      Line 4 contains non-roundtripable characters:\n        Char 0x25 at column 27\n      Line 7 contains non-roundtripable characters:\n        Empty Shift Out and Shift In at column 74\n\n ! Copying using BINARY mode\n** Build finished\nNote that the migration tool has detected numerous non-roundtripable characters on various lines and has performed the copy as binary.\nAn examination of the files on the local z/OS UNIX Git Repository will reveal that the file was copied but left untagged (this is a current known limitation for the DBB Toolkit in its 1.1.3 version, and a workaround is available upon request).\nls -alT /u/user1/Migration/copybook\ntotal 48\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  6 13:42 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  6 13:42 ..\n- untagged    T=off -rw-r--r--   1 USER1    OMVS         560 May  6 13:42 nround.cpy\nTo be processed by the Rocket Git client when performing the Git add command, the file should be manually tagged as binary first. To correctly tag the file as binary, use the chtag -b command prior to performing the Git add command:\nls -alT /u/user1/Migration/copybook\ntotal 48\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  8 13:10 .\n                    drwxr-xr-x   5 USER1    OMVS        8192 May  8 13:12 ..\nb binary      T=off -rw-r--r--   1 USER1    OMVS         560 May  8 13:10 nround.cpy\nAdditionally, the .gitattributes file was automatically updated by the migration script to indicate that the file is mapped as binary:\ncat /u/user1/Migration/.gitattributes\ncopybook/nround.cpy binary\nDuring the Git push to the distributed Git server, Git will treat this as a binary file and no conversion to UTF-8 will take place. In essence, the resulting file on the distributed Git server will be the original contents of the PDS member, in EBCDIC.\ngit add .\nwarning: copybook/nround.cpy added file have been automatically tagged BINARY because they were untagged yet the .gitattributes file specifies they should be tagged\ngit commit -m \"Binary File\"\n[main 0213795] Binary File\n 2 files changed, 1 insertion(+)\n create mode 100644 .gitattributes\n create mode 100644 copybook/nround.cpy\ngit push\nCounting objects: 5, done.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (5/5), 598 bytes | 598.00 KiB/s, done.\nTotal 5 (delta 0), reused 0 (delta 0)\nTo github.ibm.com:user1/Migration.git\n   e901992..0213795  main -&gt; main\nOnce the Git push action has completed to the distributed Git server, the resulting file will be treated as binary:\n\n\n\nDBB Migration Tool scenario 3, binary nround.cpy file in Git\n\n\nThis may not be an ideal situation as described in the documentation on Managing non-printable and non-roundtripable characters, and should be corrected/reconciled before continuing with the migration.\n\n\n\nIn this final scenario, we will examine how the migration tool can assist in the detection of what is known as non-printable characters. The documentation on Managing non-printable and non-roundtripable characters provides more conceptual background information. To illustrate this, we will again migrate the single source member MIGRATE.TECHDOC.COPYBOOK(NPRINT), which only contains non-printable characters.\nDuring the migration of a PDS member to a z/OS UNIX file, the migration tool will scan the content of the file to see if it detects any non-printable characters. These characters are defined as any hexadecimal values that are an EBCDIC x’40’ or less, and not one of the five (5) non-roundtripable characters.\nThe migration tool provides three (3) options on how to handle and report on these characters:\n\nDo Not Check: The non-printable characters are not researched; the member is simply copied as text. Code page conversion will occur.\nInfo: The script will emit an Informational diagnostic message in the console log if a non-printable character is detected and the file is copied as text. Code page conversion will occur.\nWarning: The script will emit a Warning diagnostic message in the console log if a non-printable character is detected and the file is copied as binary. No code page conversion will occur, and the offending characters will be treated like non-roundtripable characters.\n\nIt should be noted that for the “Do Not Check” and “Info” options, although code page conversion is taking place, this could cause an issue later. The file may not be easily maintained with a distributed editor, and you run the risk of having corrupted files.\nControlling what level of checking should be performed during the migration is done via the optional scan level -np, --non-printable &lt;level&gt; parameter switch passed to the migration script. If the scan level parameter is not specified, the non-printable characters are not checked (equates to the “Do Not Check” option).\n\n\nExample output for migration performed with scan level set to info:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -np info -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] \"COPYBOOK(NPRINT)\"\n\nNon-printable scan level is info\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK(NPRINT)\n[INFO] Copying MIGRATE.TECHDOC.COPYBOOK(NPRINT) to /u/user1/Migration/copybook/nprint.cpy using IBM-037\n ! Possible migration issue:\n      Line 2 contains non-printable characters:\n        Char 0x00 at column 19\n      Line 3 contains non-printable characters:\n        Char 0x06 at column 19\n      Line 4 contains non-printable characters:\n        Char 0x07 at column 19\n      Line 5 contains non-printable characters:\n        Char 0x1B at column 19\n\n** Build finished\nNote that the migration tool has detected numerous non-printable characters on various lines and has performed the copy as text and will be tagged on z/OS UNIX using the supplied encoding of IBM-037:\nls -alT /u/user1/Migration/copybook\ntotal 64\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  6 15:39 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  6 14:55 ..\nt IBM-037     T=on  -rw-r--r--   1 USER1    OMVS         114 May  6 15:39 nprint.cpy\n\n\n\nExample output for migration performed with scan level set to warning:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -np warning -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] \"COPYBOOK(NPRINT)\"\n\nNon-printable scan level is warning\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK(NPRINT)\n[WARNING] Copying MIGRATE.TECHDOC.COPYBOOK(NPRINT) to /u/user1/Migration/copybook/nprint.cpy\n ! Possible migration issue:\n      Line 2 contains non-printable characters:\n        Char 0x06 at column 19\n      Line 3 contains non-printable characters:\n        Char 0x07 at column 19\n      Line 4 contains non-printable characters:\n        Char 0x1B at column 19\n\n ! Copying using BINARY mode\n** Build finished\nNote that the migration tool has detected numerous non-printable characters on various lines and has performed the copy as binary. The file remains untagged on z/OS UNIX (the manual tagging of the file is still required):\nls -alT /u/user1/Migration/copybook\ntotal 48\n                    drwxr-xr-x   2 USER1    OMVS        8192 May  6 17:17 .\n                    drwxr-xr-x   4 USER1    OMVS        8192 May  6 17:17 ..\n- untagged    T=off -rw-r--r--   1 USER1    OMVS         320 May  6 17:17 nprint.cpy",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html#recommendations-for-using-the-migration-utility",
    "href": "dbb-migration-tool.html#recommendations-for-using-the-migration-utility",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "A strategy must be decided on how to handle both non-printable and non-roundtripable characters found in source members that are to be migrated from z/OS PDSs to Git. The HEXCODED member, though not demonstrated in the above scenarios, is a common occurrence in many older legacy applications. Source code members such as this need to be identified and transformed to that shown in the HEXVALUE member if the goal is to manage the source seamlessly using the modernized tooling provided through Git. For more conceptual background information, please refer to Managing non-printable and non-roundtripable characters.\nThe DBB Migration Tool provides an option to perform a scan of the z/OS PDSs to assist in the analysis and correction/reconciliation of these situations prior to performing the copy to the local Git repository. Invoking this scan is done via the optional preview -p, --preview parameter switch and will bypass the copy. The default is “No Preview”. An example of the preview follows:\n$DBB_HOME/migration/bin/migrate.sh -r /u/user1/Migration -np warning -p -m MappingRule[hlq:MIGRATE.TECHDOC,extension:CPY,toLower:true,pdsEncoding:IBM-037] COPYBOOK\n\nNon-printable scan level is warning\nPreview flag is specified, no members will be copied to HFS\nLocal GIT repository: /u/user1/Migration\nUsing mapping rule com.ibm.dbb.migration.MappingRule to migrate the data sets\nMigrating data set COPYBOOK\n[WARNING] Previewing MIGRATE.TECHDOC.COPYBOOK(HEXCODED)\n ! Possible migration issue:\n      Line 3 contains non-printable characters:\n        Char 0x19 at column 37\n      Line 4 contains non-roundtripable characters:\n        Char 0x15 at column 37\n      Line 5 contains non-printable characters:\n        Char 0x0C at column 37\n      Line 6 contains non-roundtripable characters:\n        Char 0x0D at column 37\n\n ! Will copy using BINARY mode\nPreviewing MIGRATE.TECHDOC.COPYBOOK(HEXVALUE). Using IBM-037.\n[WARNING] Previewing MIGRATE.TECHDOC.COPYBOOK(NPRINT)\n ! Possible migration issue:\n      Line 2 contains non-printable characters:\n        Char 0x06 at column 19\n      Line 3 contains non-printable characters:\n        Char 0x07 at column 19\n      Line 4 contains non-printable characters:\n        Char 0x1B at column 19\n\n ! Will copy using BINARY mode\n[WARNING] Previewing MIGRATE.TECHDOC.COPYBOOK(NROUND)\n ! Possible migration issue:\n      Line 2 contains non-roundtripable characters:\n        Char 0x15 at column 27\n      Line 3 contains non-roundtripable characters:\n        Char 0x0D at column 27\n      Line 4 contains non-roundtripable characters:\n        Char 0x25 at column 27\n      Line 5 contains non-printable characters:\n        Char 0x0F at column 33\n      Line 7 contains non-roundtripable characters:\n        Empty Shift Out and Shift In at column 74\n\n ! Will copy using BINARY mode\nWith the -l, --log option, a log file can be created to contain all the messages about the migration process, including the non-printable and non-roundtripable characters encountered during the scan. This log file can be used by the developers to perform the necessary changes in their original source code members prior to the real migration process.\nMany other options of the Mapping Rule parameter can be leveraged to control the behavior of the DBB Migration Tool. These options are described on the IBM Documentation website.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html#migrating-from-ibm-engineering-workflow-management-to-git",
    "href": "dbb-migration-tool.html#migrating-from-ibm-engineering-workflow-management-to-git",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "For specific information on using the DBB Migration Tool to migrate from IBM Engineering Workflow Management (EWM) to Git, please reference Migrating from IBM Engineering Workflow Management (EWM) to GIT.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "dbb-migration-tool.html#resources",
    "href": "dbb-migration-tool.html#resources",
    "title": "DBB Migration Tool",
    "section": "",
    "text": "This page contains reformatted excerpts from Managing the code page conversion when migrating z/OS source files to Git.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Methods for migrating data",
      "DBB Migration Tool"
    ]
  },
  {
    "objectID": "journey-to-cicd.html",
    "href": "journey-to-cicd.html",
    "title": "Understanding the journey to a CI/CD pipeline",
    "section": "",
    "text": "What is involved in the migration effort from green screen to CI/CD?\n\nHave the right tools (CI/CD pipeline tools): The major components are summarized in the CI/CD for z/OS applications page.\nHave the right people (Roles): The DevOps transformation journey includes a range of roles to help accomplish the various milestones along the way. For example, the enterprise will need roles to architect the CI/CD pipeline and workflows, implement the infrastructure, understand how to use the CI/CD tools, and so on. You can read more about each role in the Roles section.\n\nIBM and business partners can also help support you at different milestones of your journey to the CI/CD pipeline. Available resources are described for each milestone in the Migration journey milestones section.\n\nHave the right mindset: Migrating from legacy tooling and development processes to DevOps and CI/CD often requires work to bring about cultural change in the organization. In fact, many successful DevOps migration efforts involve a change transformation specialist to help the teams impacted by this migration understand the concepts and workflows behind the DevOps change and the benefits they bring.\n\n\n\nThe migration to a Git-based CI/CD pipeline can be approached in a couple different ways. A “big-bang” migration approach involves designing and implementing the future state, migrating code from z/OS to the Git provider, and training all the developers on the new way of working while the legacy system is still in place and active. Then, at a given conversion date, all developers will switch to using the new CI/CD pipeline, with the caveats that they will have received training in a short timeframe and that the pipeline will support all workflows from day one.\nHowever, many enterprises prefer a “phased” migration approach, with the iterative migration of applications from the legacy system to the new CI/CD pipeline. The phased migration approach begins by designing the future state, and then training and migrating just one (or a couple) complete application team(s) at a time to the new CI/CD pipeline. Additional applications are subsequently onboarded to the new CI/CD pipeline over several iterations. This iterative process allows the organization to find and iron out wrinkles in the migration process and/or pipeline on a smaller scale, and then apply the experience and improve for later iterations. By the time the enterprise is mostly using the CI/CD pipeline, the migration process will be more familiar, and the switch from the legacy system to the modern CI/CD pipeline becomes much less disruptive. Application dependencies across the new CI/CD pipeline and the legacy system are expected during the iterative migration process, but the IBM Z DevOps Acceleration Team (DAT) can provide resources that help you establish processes to address this period of co-existence.\n\n\n\nHaving a team with the right skills and mindset is critical to a successful DevOps transformation effort. While the following roles each have their own specific skillsets and tasks, an individual can perform more than one role if it makes sense for their team and organization. You can click on each role to learn about it.\n\n\n\n\n\n\nArchitect\n\n\n\n\n\nThe architect helps define the new software delivery process.\nGenerally, the architect will be someone with strong z/OS skills who understands the infrastructure and current build processes. This deep background knowledge about the current z/OS infrastructure state and mainframe application build processes is important for understanding how to translate those processes into the more modern DevOps pipeline.\nA key task is to condense existing mainframe workflows and design the to-be state in the CI/CD pipeline. For this, the architect collaborates with the distributed teams to create a common enterprise software delivery processes. Additionally, the architect is involved in defining necessary integration points of the pipeline, as well as designing the migration process.\n\nBackground skills and knowledge:\n\nStrong z/OS skills (average of about 10 years of experience)\nKnowledge about mainframe development processes and workflows\n\nSkills and concepts to learn:\n\nCI/CD and DevOps principles\nGit concepts and architecture\n\nTasks:\n\nCollaborate between the mainframe development team(s) and distributed teams to transform the existing mainframe workflows into the to-be CI/CD pipeline\n\nJob positions that you might find filling this role:\n\nEnterprise architect\nEnterprise application architect\nIT architect\n\n\n\n\n\n\n\n\n\n\n\nBuild specialist\n\n\n\n\n\nThe build specialist develops and maintains the build scripts for the new pipeline.\nThis is a developer type of role that focuses on turning the source code into a deployable artifact, so familiarity with z/OS build processes is required. The build specialist might adapt a distributed example of build scripting to z/OS.\n\nBackground skills and knowledge:\n\nMainframe build fundamentals (for example, JCL/REXX, understanding of compile/link/bind options, and so on)\n\nSkills and concepts to learn:\n\nGit concepts\nIBM Dependency Based Build architecture (for example, dependency management and build results)\nGroovy scripting\n\nTasks:\n\nPlan and perform migrations\nDevelop and maintain the customized build framework\n\nJob positions that might fill this role:\n\nBuild engineer\nz/OS build administrator\n\n\n\n\n\n\n\n\n\n\n\nPipeline specialist\n\n\n\n\n\nThe pipeline specialist assembles the pipeline in the CI/CD orchestrator.\nThis is a developer type of role that focuses on building, scaling, and maintaining the CI/CD pipeline structure. The pipeline specialist does not need to be as z/OS-aligned as the build specialist. Rather than being concerned with building COBOL programs (or other z/OS languages), the pipeline specialist is more concerned about integrating tools together. This role often already exists in the distributed side of the enterprise.\nTypically, the pipeline specialist is the first role to adopt the DevOps tooling, and then teaches the tools and workflows to other teams in the organization (for example, z/OS application development teams).\n\nBackground skills and knowledge:\n\nCI/CD specialist\nGit concepts\nChange management integrations\n\nSkills and concepts to learn:\n\nFoundational IBM DBB concepts\nGroovy, Shell scripting\n\nTasks:\n\nDevelop and maintain customized integration scripts between the different pipeline building blocks (for example, using Groovy and Shell scripting)\n\nJob positions that might fill this role:\n\nDevOps engineer\nDevOps pipeline administrator\nDevOps team\n\n\n\n\n\n\n\n\n\n\n\nChange transformation specialist\n\n\n\n\n\nThe change transformation specialist drives the cultural and organizational change required for a successful modernization journey.\nThis role is more of a consulting and people-focused role rather than a technical one. Enterprises sometimes hire an individual specifically for this role when embarking on the DevOps transformation journey - for example, someone with specialized training in coaching DevOps/Agile methodologies, who has experience in helping teams make the transformation succeed from a cultural and cross-team point of view.\n\nBackground skills and knowledge:\n\nStrong communication\nPlanning and organizing\nChange transformation\nUnderstanding of DevOps and Agile concepts\n\nSkills and concepts to learn:\n\nUnderstand the needs and concerns of all groups, in order to be the “voice of the transformation”\n\nTasks:\n\nEffectively communicate to teams the motivation and purpose behind the transformation journey\nCollaborate with teams to coordinate training for the cultural and organizational change\n\nJob positions that might fill this role:\n\nChange transformation specialist\nDevOps/Agile coach\nTransformation enablement team\n\n\n\n\n\n\n\n\n\n\n\nDeployment specialist\n\n\n\n\n\nThe application deployment specialist implements the deployment solution.\nThis developer type of role may be part of the DevOps team (with the pipeline specialist), and might already be using a deployment manager with distributed teams. It is helpful for them to have some understanding of the mainframe subsystem and infrastructure interfaces, as those will also be involved in the z/OS application deployment processes.\n\nBackground skills and knowledge:\n\nDeployment management\nGit concepts\n\nSkills and concepts to learn:\n\nMainframe subsystem and infrastructure interfaces\n\nTasks:\n\nDevelop and maintain central deployment processes\nCollaborate with the build specialist and pipeline specialist to design the to-be solution\n\nJob positions that might fill this role:\n\nDevOps engineer\nDevOps team\n\n\n\n\n\n\n\n\n\n\n\nIntegrated development environment specialist\n\n\n\n\n\nThe integrated development environment (IDE) specialist is a developer type of role that helps implement the workflows within the IDE.\nSince the IDE is a central tool used by application developers, it is important that someone in the organization is trained on how to use the IDE effectively, and can also guide others on using it. The IDE specialist understands (or learns) how to use the IDE, and shares this knowledge with others in their organization.\n\nBackground skills and knowledge:\n\nSoftware development tasks and use cases\n\nSkills and concepts to learn (if not already acquired):\n\nIDE customization\nGit concepts\n\nTasks:\n\nCustomization and documentation of the IDE\nIDE installation and deployment to developer workstations (including upgrades)\nTraining and coaching others on using the IDE\n\nJob positions that might fill this role:\n\nSoftware developer (or application developer)\nSoftware engineer\n\n\n\n\n\n\n\n\n\n\n\nMiddleware specialist\n\n\n\n\n\nThe middleware specialist role is an umbrella term that covers different technical roles that help install and configure the tools for the CI/CD pipeline.\nThis role might be handled by more than one individual, as it can cover setup tasks on both Linux and mainframe environments, depending on the enterprise’s needs.\n\nBackground skills and knowledge:\n\nBackground in managing or administering the requisite middleware system\n\nSkills and concepts to learn (if not already acquired):\n\nInitial install and configure steps for DevOps tooling\n\nTasks:\n\nAssist with installation and configuration of Linux-based components and/or z/OS host components (depending on selected DevOps technology stack)\n\nJob positions that might fill this role:\n\nMiddleware system programmer or system administrator (for example, CICS administrator and/or Db2 administrator)\nInfrastructure team\n\n\n\n\n\n\n\n\n\n\n\nMigration specialist\n\n\n\n\n\nThe migration specialist is typically a transitional role that focuses on facilitating the migration from the legacy development tools and processes to the modern CI/CD pipeline.\nThis role can either be handled by a selected team in the enterprise, or by a business partner.\n\nBackground skills and knowledge:\n\nMainframe data fundamentals\nUnderstanding of the legacy development system\n\nSkills and concepts to learn (if not already acquired):\n\nGit concepts\nIBM Dependency Based Build fundamentals (for example, DBB Migration Tool)\n\nTasks:\n\nHelp move data from legacy z/OS application development systems to Git\n\nJob positions that might fill this role:\n\nDevOps implementation architect\nBuild engineer and DevOps team\n\n\n\n\n\n\n\n\n\n\n\nTesting specialist\n\n\n\n\n\nThe testing specialist is technical role that focuses on quality assurance in the software.\nWhile testing in legacy development workflows is often manual and time consuming, the move to a modernized DevOps toolchain allows the testing specialist to create tests that can be automatically run by the developer, and/or as part of a CI/CD pipeline. The scope of these tests can range from individual unit tests to larger-scale integration tests on dedicated testing platforms.\n\nBackground skills and knowledge:\n\nUnderstanding of the z/OS application functionality and use cases\nExperience testing z/OS applications\n\nSkills and concepts to learn (if not already acquired):\n\nGit concepts\nIBM Dependency Based Build fundamentals (for example, running a DBB User Build)\nModern z/OS testing tools such as zUnit, IBM Virtual Dev and Test for z/OS (ZVDT), and/or IBM Z Virtual Test Platform (VTP)\n\nTasks:\n\nCreate and automate testing processes for the CI/CD pipeline (for example, unit and/or integration testing)\n\nJob positions that might fill this role:\n\nQuality engineer\nQuality assurance team\nTesting team\n\n\n\n\n\n\n\n\nAs you migrate from the green screen to CI/CD, there are several key milestones along the way. IBM teams and partners are available to help you at each step of the way:\n\n\n\nMilestones in the journey to a CI/CD pipeline\n\n\n\nMilestone 1: Design and validate the high level future design:\n\nThis is where you learn about and validate the pipeline approach. It involves sketching out a high-level design of the to-be future state, and understanding the technical composition of the pipeline. In this milestone, building a proof-of-concept (POC) and/or a pilot will help further validate the approach and build up skills, either to implement the pipeline with in-house resources or to be able to make informed decisions together with a delivery organization. The POC/pilot will also help establish support from technical champions and management/leadership, which is key for a successful DevOps transformation.\nMilestone 1 resources:\n\nAt the start of Milestone 1, free online resources and training provided by IBM can help you explore the IBM Z DevOps solution.\nEngage with the IBM Sales and Tech Sales team to learn more about how the IBM Z DevOps solution can benefit your enterprise.\nWhen your team is ready to build a POC and/or a pilot, the DAT can help with additional training and design sessions for the POC/pilot, including a primer on how to analyze your current application to design its to-be state in Git repository layouts.\n\n\nMilestone 2: Deep-dive analysis to build business case and migration strategy\n\nThis milestone is about understanding your system’s current state. Your system has evolved over time, perhaps over decades. Therefore, it requires a detailed assessment of the current build setup, the current repository layouts, and how ownership of code is defined. By assessing and understanding your system’s current state, you can clarify the designs for your future state, as well as the steps for how to get there. This is also when you will want to consider the best way for your organization to migrate to the new CI/CD pipeline, whether that would be to take a phased migration approach (which most customers do) or a big-bang approach.\nMilestone 2 resources: The following IBM teams and business partners can help with analysis and refinement, leading to a proposal to move off of the legacy source control manager (SCM):\n\nIBM Services such as the Software Migration Project Office (SMPO) and Global Business Services (GBS)\nIBM business partners\nSystem integrators\nIBM DevOps Acceleration Program\n\n\nMilestone 3: Architect and implement the future state and new way of working\n\nMilestone 3 gets into the details of the solution by refining designs and ideas from the previous milestones to create a low-level design of the future workflow. This includes planning the timing of different parts of the DevOps transformation journey to prioritize core pieces. If the design for the pipeline is in a stable state, then the necessary pipeline pieces can go forward with implementation. This is when the “plumbing” work for the pipeline begins, integrating the different CI/CD components together in the context of your selected technologies (for example: pipeline orchestrator, Git provider, deployment manager, and so on). In this milestone, it is also important to communicate the DevOps transformation journey to the intended users of the new CI/CD pipeline. This can include demos, training, and provisioning of sandbox systems where the users can explore hands-on the cuture state.\nMilestone 3 resources:\n\nBuild framework samples for zAppBuild can be found in zAppBuild’s GitHub documentation.\nFirst-class implementation support and guidance are available from IBM Services (SMPO, GBS), IBM business partners, and system integrators.\n\n\nMilestone 4: Roll out the new way of working\n\nIn Milestone 4, the plan is put into action. This means the application development and operation teams are trained and onboarded with the new way of working, and the migration plan begins to move applications from the legacy system to the new CI/CD pipeline. This can be done in an iterative way (that is, a phased migration approach), allowing the organization to continuously incorporate feedback for improving the solution and migration process. By the end of this milestone, a CI/CD pipeline is in place and active.\nMilestone 4 resources:\n\nLaunch the new pipeline and make the change stick using IBM Services (SMPO, GBS), IBM business partners, and system integrators.\n\n\nMilestone 5: Realize the benefits\n\nMilestone 5 wraps up the migration part of the DevOps transformation journey, with teams now working in a standardized CI/CD pipeline. At this point, any independent software vendor (ISV) licenses can be returned, and the organization can refine the CI/CD pipeline to enable new capabilities and optimize the DevOps workflow.",
    "crumbs": [
      "Getting Started",
      "Understanding the journey to a CI/CD pipeline"
    ]
  },
  {
    "objectID": "journey-to-cicd.html#migration-approaches",
    "href": "journey-to-cicd.html#migration-approaches",
    "title": "Understanding the journey to a CI/CD pipeline",
    "section": "",
    "text": "The migration to a Git-based CI/CD pipeline can be approached in a couple different ways. A “big-bang” migration approach involves designing and implementing the future state, migrating code from z/OS to the Git provider, and training all the developers on the new way of working while the legacy system is still in place and active. Then, at a given conversion date, all developers will switch to using the new CI/CD pipeline, with the caveats that they will have received training in a short timeframe and that the pipeline will support all workflows from day one.\nHowever, many enterprises prefer a “phased” migration approach, with the iterative migration of applications from the legacy system to the new CI/CD pipeline. The phased migration approach begins by designing the future state, and then training and migrating just one (or a couple) complete application team(s) at a time to the new CI/CD pipeline. Additional applications are subsequently onboarded to the new CI/CD pipeline over several iterations. This iterative process allows the organization to find and iron out wrinkles in the migration process and/or pipeline on a smaller scale, and then apply the experience and improve for later iterations. By the time the enterprise is mostly using the CI/CD pipeline, the migration process will be more familiar, and the switch from the legacy system to the modern CI/CD pipeline becomes much less disruptive. Application dependencies across the new CI/CD pipeline and the legacy system are expected during the iterative migration process, but the IBM Z DevOps Acceleration Team (DAT) can provide resources that help you establish processes to address this period of co-existence.",
    "crumbs": [
      "Getting Started",
      "Understanding the journey to a CI/CD pipeline"
    ]
  },
  {
    "objectID": "journey-to-cicd.html#roles",
    "href": "journey-to-cicd.html#roles",
    "title": "Understanding the journey to a CI/CD pipeline",
    "section": "",
    "text": "Having a team with the right skills and mindset is critical to a successful DevOps transformation effort. While the following roles each have their own specific skillsets and tasks, an individual can perform more than one role if it makes sense for their team and organization. You can click on each role to learn about it.\n\n\n\n\n\n\nArchitect\n\n\n\n\n\nThe architect helps define the new software delivery process.\nGenerally, the architect will be someone with strong z/OS skills who understands the infrastructure and current build processes. This deep background knowledge about the current z/OS infrastructure state and mainframe application build processes is important for understanding how to translate those processes into the more modern DevOps pipeline.\nA key task is to condense existing mainframe workflows and design the to-be state in the CI/CD pipeline. For this, the architect collaborates with the distributed teams to create a common enterprise software delivery processes. Additionally, the architect is involved in defining necessary integration points of the pipeline, as well as designing the migration process.\n\nBackground skills and knowledge:\n\nStrong z/OS skills (average of about 10 years of experience)\nKnowledge about mainframe development processes and workflows\n\nSkills and concepts to learn:\n\nCI/CD and DevOps principles\nGit concepts and architecture\n\nTasks:\n\nCollaborate between the mainframe development team(s) and distributed teams to transform the existing mainframe workflows into the to-be CI/CD pipeline\n\nJob positions that you might find filling this role:\n\nEnterprise architect\nEnterprise application architect\nIT architect\n\n\n\n\n\n\n\n\n\n\n\nBuild specialist\n\n\n\n\n\nThe build specialist develops and maintains the build scripts for the new pipeline.\nThis is a developer type of role that focuses on turning the source code into a deployable artifact, so familiarity with z/OS build processes is required. The build specialist might adapt a distributed example of build scripting to z/OS.\n\nBackground skills and knowledge:\n\nMainframe build fundamentals (for example, JCL/REXX, understanding of compile/link/bind options, and so on)\n\nSkills and concepts to learn:\n\nGit concepts\nIBM Dependency Based Build architecture (for example, dependency management and build results)\nGroovy scripting\n\nTasks:\n\nPlan and perform migrations\nDevelop and maintain the customized build framework\n\nJob positions that might fill this role:\n\nBuild engineer\nz/OS build administrator\n\n\n\n\n\n\n\n\n\n\n\nPipeline specialist\n\n\n\n\n\nThe pipeline specialist assembles the pipeline in the CI/CD orchestrator.\nThis is a developer type of role that focuses on building, scaling, and maintaining the CI/CD pipeline structure. The pipeline specialist does not need to be as z/OS-aligned as the build specialist. Rather than being concerned with building COBOL programs (or other z/OS languages), the pipeline specialist is more concerned about integrating tools together. This role often already exists in the distributed side of the enterprise.\nTypically, the pipeline specialist is the first role to adopt the DevOps tooling, and then teaches the tools and workflows to other teams in the organization (for example, z/OS application development teams).\n\nBackground skills and knowledge:\n\nCI/CD specialist\nGit concepts\nChange management integrations\n\nSkills and concepts to learn:\n\nFoundational IBM DBB concepts\nGroovy, Shell scripting\n\nTasks:\n\nDevelop and maintain customized integration scripts between the different pipeline building blocks (for example, using Groovy and Shell scripting)\n\nJob positions that might fill this role:\n\nDevOps engineer\nDevOps pipeline administrator\nDevOps team\n\n\n\n\n\n\n\n\n\n\n\nChange transformation specialist\n\n\n\n\n\nThe change transformation specialist drives the cultural and organizational change required for a successful modernization journey.\nThis role is more of a consulting and people-focused role rather than a technical one. Enterprises sometimes hire an individual specifically for this role when embarking on the DevOps transformation journey - for example, someone with specialized training in coaching DevOps/Agile methodologies, who has experience in helping teams make the transformation succeed from a cultural and cross-team point of view.\n\nBackground skills and knowledge:\n\nStrong communication\nPlanning and organizing\nChange transformation\nUnderstanding of DevOps and Agile concepts\n\nSkills and concepts to learn:\n\nUnderstand the needs and concerns of all groups, in order to be the “voice of the transformation”\n\nTasks:\n\nEffectively communicate to teams the motivation and purpose behind the transformation journey\nCollaborate with teams to coordinate training for the cultural and organizational change\n\nJob positions that might fill this role:\n\nChange transformation specialist\nDevOps/Agile coach\nTransformation enablement team\n\n\n\n\n\n\n\n\n\n\n\nDeployment specialist\n\n\n\n\n\nThe application deployment specialist implements the deployment solution.\nThis developer type of role may be part of the DevOps team (with the pipeline specialist), and might already be using a deployment manager with distributed teams. It is helpful for them to have some understanding of the mainframe subsystem and infrastructure interfaces, as those will also be involved in the z/OS application deployment processes.\n\nBackground skills and knowledge:\n\nDeployment management\nGit concepts\n\nSkills and concepts to learn:\n\nMainframe subsystem and infrastructure interfaces\n\nTasks:\n\nDevelop and maintain central deployment processes\nCollaborate with the build specialist and pipeline specialist to design the to-be solution\n\nJob positions that might fill this role:\n\nDevOps engineer\nDevOps team\n\n\n\n\n\n\n\n\n\n\n\nIntegrated development environment specialist\n\n\n\n\n\nThe integrated development environment (IDE) specialist is a developer type of role that helps implement the workflows within the IDE.\nSince the IDE is a central tool used by application developers, it is important that someone in the organization is trained on how to use the IDE effectively, and can also guide others on using it. The IDE specialist understands (or learns) how to use the IDE, and shares this knowledge with others in their organization.\n\nBackground skills and knowledge:\n\nSoftware development tasks and use cases\n\nSkills and concepts to learn (if not already acquired):\n\nIDE customization\nGit concepts\n\nTasks:\n\nCustomization and documentation of the IDE\nIDE installation and deployment to developer workstations (including upgrades)\nTraining and coaching others on using the IDE\n\nJob positions that might fill this role:\n\nSoftware developer (or application developer)\nSoftware engineer\n\n\n\n\n\n\n\n\n\n\n\nMiddleware specialist\n\n\n\n\n\nThe middleware specialist role is an umbrella term that covers different technical roles that help install and configure the tools for the CI/CD pipeline.\nThis role might be handled by more than one individual, as it can cover setup tasks on both Linux and mainframe environments, depending on the enterprise’s needs.\n\nBackground skills and knowledge:\n\nBackground in managing or administering the requisite middleware system\n\nSkills and concepts to learn (if not already acquired):\n\nInitial install and configure steps for DevOps tooling\n\nTasks:\n\nAssist with installation and configuration of Linux-based components and/or z/OS host components (depending on selected DevOps technology stack)\n\nJob positions that might fill this role:\n\nMiddleware system programmer or system administrator (for example, CICS administrator and/or Db2 administrator)\nInfrastructure team\n\n\n\n\n\n\n\n\n\n\n\nMigration specialist\n\n\n\n\n\nThe migration specialist is typically a transitional role that focuses on facilitating the migration from the legacy development tools and processes to the modern CI/CD pipeline.\nThis role can either be handled by a selected team in the enterprise, or by a business partner.\n\nBackground skills and knowledge:\n\nMainframe data fundamentals\nUnderstanding of the legacy development system\n\nSkills and concepts to learn (if not already acquired):\n\nGit concepts\nIBM Dependency Based Build fundamentals (for example, DBB Migration Tool)\n\nTasks:\n\nHelp move data from legacy z/OS application development systems to Git\n\nJob positions that might fill this role:\n\nDevOps implementation architect\nBuild engineer and DevOps team\n\n\n\n\n\n\n\n\n\n\n\nTesting specialist\n\n\n\n\n\nThe testing specialist is technical role that focuses on quality assurance in the software.\nWhile testing in legacy development workflows is often manual and time consuming, the move to a modernized DevOps toolchain allows the testing specialist to create tests that can be automatically run by the developer, and/or as part of a CI/CD pipeline. The scope of these tests can range from individual unit tests to larger-scale integration tests on dedicated testing platforms.\n\nBackground skills and knowledge:\n\nUnderstanding of the z/OS application functionality and use cases\nExperience testing z/OS applications\n\nSkills and concepts to learn (if not already acquired):\n\nGit concepts\nIBM Dependency Based Build fundamentals (for example, running a DBB User Build)\nModern z/OS testing tools such as zUnit, IBM Virtual Dev and Test for z/OS (ZVDT), and/or IBM Z Virtual Test Platform (VTP)\n\nTasks:\n\nCreate and automate testing processes for the CI/CD pipeline (for example, unit and/or integration testing)\n\nJob positions that might fill this role:\n\nQuality engineer\nQuality assurance team\nTesting team",
    "crumbs": [
      "Getting Started",
      "Understanding the journey to a CI/CD pipeline"
    ]
  },
  {
    "objectID": "journey-to-cicd.html#migration-journey-milestones",
    "href": "journey-to-cicd.html#migration-journey-milestones",
    "title": "Understanding the journey to a CI/CD pipeline",
    "section": "",
    "text": "As you migrate from the green screen to CI/CD, there are several key milestones along the way. IBM teams and partners are available to help you at each step of the way:\n\n\n\nMilestones in the journey to a CI/CD pipeline\n\n\n\nMilestone 1: Design and validate the high level future design:\n\nThis is where you learn about and validate the pipeline approach. It involves sketching out a high-level design of the to-be future state, and understanding the technical composition of the pipeline. In this milestone, building a proof-of-concept (POC) and/or a pilot will help further validate the approach and build up skills, either to implement the pipeline with in-house resources or to be able to make informed decisions together with a delivery organization. The POC/pilot will also help establish support from technical champions and management/leadership, which is key for a successful DevOps transformation.\nMilestone 1 resources:\n\nAt the start of Milestone 1, free online resources and training provided by IBM can help you explore the IBM Z DevOps solution.\nEngage with the IBM Sales and Tech Sales team to learn more about how the IBM Z DevOps solution can benefit your enterprise.\nWhen your team is ready to build a POC and/or a pilot, the DAT can help with additional training and design sessions for the POC/pilot, including a primer on how to analyze your current application to design its to-be state in Git repository layouts.\n\n\nMilestone 2: Deep-dive analysis to build business case and migration strategy\n\nThis milestone is about understanding your system’s current state. Your system has evolved over time, perhaps over decades. Therefore, it requires a detailed assessment of the current build setup, the current repository layouts, and how ownership of code is defined. By assessing and understanding your system’s current state, you can clarify the designs for your future state, as well as the steps for how to get there. This is also when you will want to consider the best way for your organization to migrate to the new CI/CD pipeline, whether that would be to take a phased migration approach (which most customers do) or a big-bang approach.\nMilestone 2 resources: The following IBM teams and business partners can help with analysis and refinement, leading to a proposal to move off of the legacy source control manager (SCM):\n\nIBM Services such as the Software Migration Project Office (SMPO) and Global Business Services (GBS)\nIBM business partners\nSystem integrators\nIBM DevOps Acceleration Program\n\n\nMilestone 3: Architect and implement the future state and new way of working\n\nMilestone 3 gets into the details of the solution by refining designs and ideas from the previous milestones to create a low-level design of the future workflow. This includes planning the timing of different parts of the DevOps transformation journey to prioritize core pieces. If the design for the pipeline is in a stable state, then the necessary pipeline pieces can go forward with implementation. This is when the “plumbing” work for the pipeline begins, integrating the different CI/CD components together in the context of your selected technologies (for example: pipeline orchestrator, Git provider, deployment manager, and so on). In this milestone, it is also important to communicate the DevOps transformation journey to the intended users of the new CI/CD pipeline. This can include demos, training, and provisioning of sandbox systems where the users can explore hands-on the cuture state.\nMilestone 3 resources:\n\nBuild framework samples for zAppBuild can be found in zAppBuild’s GitHub documentation.\nFirst-class implementation support and guidance are available from IBM Services (SMPO, GBS), IBM business partners, and system integrators.\n\n\nMilestone 4: Roll out the new way of working\n\nIn Milestone 4, the plan is put into action. This means the application development and operation teams are trained and onboarded with the new way of working, and the migration plan begins to move applications from the legacy system to the new CI/CD pipeline. This can be done in an iterative way (that is, a phased migration approach), allowing the organization to continuously incorporate feedback for improving the solution and migration process. By the end of this milestone, a CI/CD pipeline is in place and active.\nMilestone 4 resources:\n\nLaunch the new pipeline and make the change stick using IBM Services (SMPO, GBS), IBM business partners, and system integrators.\n\n\nMilestone 5: Realize the benefits\n\nMilestone 5 wraps up the migration part of the DevOps transformation journey, with teams now working in a standardized CI/CD pipeline. At this point, any independent software vendor (ISV) licenses can be returned, and the organization can refine the CI/CD pipeline to enable new capabilities and optimize the DevOps workflow.",
    "crumbs": [
      "Getting Started",
      "Understanding the journey to a CI/CD pipeline"
    ]
  },
  {
    "objectID": "dependency-management.html",
    "href": "dependency-management.html",
    "title": "Defining dependency management",
    "section": "",
    "text": "An IT system is developed by many teams and composed of different applications driven by the line of businesses and consumers. Applications need to interact to provide the overall system and interact through defined interfaces. Using well-defined interfaces allows the parts of the application to be worked on independently without necessarily requiring a change in other parts of the system. This application separation is visible and clear in a modern source code management (SCM) system, allowing clear identification of each of the distributed applications. However, in most traditional library managers, the applications all share a set of common libraries, so it is much more difficult to create the isolation.\nThis page discusses ways to componentize mainframe applications so they can be separated and the boundaries made more easily visible.\n\n\nFrom a runtime perspective in z/OS, programs run either independently (batch programs) or online in a middleware (CICS, IMS) runtime environment. Programs can use messaging resources like MQ queues or data persistence in the form of database tables, or files. Programs can also call other programs. In z/OS, called programs can either be statically bound or use dynamic linking. If a COBOL program is the first program in a run unit, that COBOL program is the main program. Otherwise, the COBOL program and all other COBOL programs in the run unit are subprograms1. The runtime environment involves various layers, including dependencies expressed between programs and resources or programs and subprograms.\nThere are multiple types of relationships to consider. The source files in the SCM produce the binaries that run on z/OS. To create the binaries, a set of source level dependencies must be understood. There is also a set of dependencies used during run time. These multiple levels of dependencies are defined in different ways, and in some cases not clearly defined at all. Understanding and finding the dependencies in source files is the first challenge.\nBuilding a program involves different steps:\n\nCompilation including any pre-compilation steps, defined as explicit steps or as option of the compiler, creates a non-executable binary (object deck) file.\nLink-edit, which assembles the object deck of the program with other objects and runtime libraries as appropriate. Link-edit can be driven by instructions (a link card) from the SCM or as dynamically defined in the build process.\n\nThese steps are summarized in the following diagram.\n\n\n\nProducing object code and executables\n\n\nAs part of the traditional build process, some additional steps, such as binds to databases, are sometimes included. The function of these steps is to prepare the runtime for a given execution environment. These should not be included in the build process itself, but should instead be included in the deployment process.\n\n\nMost of the time when people think about an application, it is from a runtime point of view. Several components are required for an application to run. Some of these are required as dependencies, such as the database or middleware and its configuration, while others are required as related, such as other applications that might be called.\nEverything running in a runtime environment starts as source from an SCM - or at least, this should be the goal when you consider infrastructure as code. Some source files represent definitions or are scripts that are not required to be built. Those that do require being built generally require other source files such as copybooks, but might not require the CICS definition, for example. Some of the source files are also included in many different programs - for example, a copybook can be used by many programs to represent the shared data structure. It is important to understand the relationships and dependencies between the source files, and when those relationships or dependencies have importance. The copybook is required to build the program, so it is required at compile time, but it is not used during run time. The configuration for a program such as the CICS transaction definition or the database schema is related to the application, but is required only for the runtime environment.\nA concrete dependency is the interface description when calling a program. A copybook defines the data structure to pass parameters to a program. So, the copybook is important to be shared while the program is part of the implementation.\n\n\n\nOn z/OS, there are two ways programs are generally called: dynamically and statically. Statically called programs are linked together at build time. These dependencies must be tracked as part of the build process to ensure they are correctly assembled. For dynamic calls, the two programs are totally separate. The programs are built and link-edited separately. At runtime, the subprogram is called based on the library concatenation.\nMany organizations have been moving to increased usage of dynamic calls as that approach reduces the complexity at build time. However, this approach means that the runtime dependencies need to be tracked and understood if any changes are made that require updates in both program and subprogram.\nThese programs and subprograms are interdependent even when using dynamic calls. When a program calls another program, generally they share data. A transfer of control occurs between the program and the subprogram with the main program passing a set of data to the subprogram and generally expecting some data in response.\nDifferent mechanisms exist to share pieces of data based on the language or the runtime. However, there is a need for the caller and the called program to define the data structure to be shared. The call of a subprogram is based on a list of transfer parameters, represented in the interface description like an application programming interface (API), but it is more tightly coupled than today’s APIs that are based on representational state transfer (REST).\nCommonly, shared data structure is defined in an included source file - for example, COBOL uses copybooks.\n\n\n\nTransfer of control sharing data between a program and subprogram\n\n\nIt is very common to define multiple copybooks for programs in order to isolate data structures and reuse them in other areas of an application component. Using copybooks allows more modularity at source level and facilitates dealing with private and shared data structures, or even private or shared functions.\n\n\n\n\nIn a web application, it is relatively easy to define an application because the physical artifact that is deployed is the complete representation of such an application: the EAR or WAR file. In the Windows world, it is more complicated since an application can be made of several executables and DLLs, but these are generally packaged together in an installable application or defined by a package manager.\nAn application is generally defined by the function or functions it provides. Sometimes there is a strong mapping between the physical parts that are shipped, and sometimes it is a set of parts that run the application.\nIn the mainframe, we fall closer to the second case where applications are defined by functions. However, based on the way the applications have grown over the years, there may be no clear boundary as to where one application ends and another one begins. An application can be defined physically by a set of resources (load modules, DBRMs, definitions) that belong together as they contribute to the same purpose: the calculation of health insurance policies, customer account management, and so on.\nAt the source file level, the relevant files contributing to an application are derived from the runtime of an application. These files can usually be identified by different means: a set of naming conventions, the ownership, information stored in the SCM, etc. It may not seem obvious at first glance, but most of the time it is possible to define which source files contribute to a given application.\nScoping your source files to an application has many benefits. It formalizes the boundaries of the application, and therefore its interfaces; it allows you to define clear ownership; and it helps with the inventory of the portfolio of an organization. Planning of future features to implement should be more accurate based on this scoping.\n\n\n\nWithin an organization, multiple applications generally make up the business function. An insurance company may have applications dedicated to health insurance, car insurance, personal health, or group health policies. These applications may be managed by different teams, but they must interact. Teams must define the interfaces or contracts between the applications. Today, many of these interactions are tightly coupled with only a shared interface defining the relationship.\nAs we have seen so far, for traditional z/OS applications, the interface is not separate but defined in source via a shared interface definition, generally a copybook or include. This source must be included in each program build for them to be able to interact. With this information, an application can be defined by two main components: shared interfaces that are used to communicate with other programs and the actual implementation of the programs.\nIt is important to note that shared copybooks could be shared not only within an application, but also across programs or across applications. The only way other programs or applications can interact with the program is by including the shared interface definition. A z/OS load module does not work like a Java Archive (JAR) file, because it is does not expose interface definitions.\nThe following diagram illustrates the concept of an application having a private implementation (its inner workings, represented in tan), and a public interface to interact with other programs and applications (represented in blue).\n\n\n\nAn application exposes a public interface\n\n\nAs applications communicate, their implementation consumes the public interface of the applications with which they interact. This concept of a public interface is common in Java programs and the way the communication between applications is defined. This principle can also be applied to existing COBOL and PL/I programs to help explain the structure required for a modern SCM, and is illustrated in the following diagram, with the applications’ usage of other applications’ interfaces indicated in red.\n\n\n\nApplications consume public interfaces of other applications\n\n\n\n\n\nThere are additional capabilities that might need to be shared in addition to sets of data structures for application communication. These capabilities might include standard security or logging functions and can be considered cross-cutting, infrastructure-level interfaces (represented in brown in the following diagram). These capabilities may be developed once and then included in many different programs. It would be very helpful if these additional included capabilities could also be handled as shared components with their own application lifecycle. The challenge comes when these components change in a non-compatible way. These types of changes are generally infrequent but might be needed at times.\n\n\n\nInterfaces used by applications, including cross-cutting interfaces\n\n\nIn the preceding sections, we have laid out some of the key factors when considering the source code of traditional mainframe applications. The environment generally consists of many different applications that can provide shared interfaces and could consume shared components, or cross-cutting interfaces.\nThe knowledge of these factors and their respective lifecycles can guide the desired structure of source files in the SCM. Several patterns are possible to provide appropriate isolation, but to also provide appropriate sharing based on different requirements.\n\n\n\nAs discussed in previous sections, applications often provide interfaces and rely on other applications’ interfaces to communicate. Each application has its own life cycle of changes. In this process, changes are made. The changes are stabilized, and then go to production. In the end, there is a single production execution environment where all the applications will run. The applications must integrate, but the most common breaking point is their interfaces. If an application’s interface changes in a non-backward compatible manner and the other applications do not react to this change (to at least recompile the modules affected by the interface change), then the production application will break. Therefore, most changes are implemented in such a way so that the structure of the data that is shared is not changed, but instead filler fields are used, or data is added to the end where only applications who need to react to the change are required to respond. In discussing the management of shared interfaces, we will refer to the process of reacting to changes as “adoption”, and the applications consuming the interface as “consumers”.\nThe main types of changes that can be made to an interface are as follows:\n\nBackward compatible change: The interface changes, but the change is done such that all applications do not need to react. A typical example is when a field is added to a data structure, but this field does not have to be handled by most applications except a selected few, which requested this new field. Usually, fillers are used in a data structure and when people declare a new field, the overall structure of the data structure stays the same. Only the consumers interested in the new field need to adopt by using the new element in the interface. The others should use the new interface to stay current, but do not need to adopt it at this point. A standard practice is to adopt the change at least for those programs which are changing and recompiling for other reasons. Many organizations do not do this type of adoption until the change has made it to production, to be sure they will not be dependent on a change that gets pulled from the release late in the cycle.\nBreaking change: A data structure has changed in such a way that the overall memory layout of the data is impacted. It could be because of an array in a data structure. There are other cases, as well (such as changes in arrays or condition name changes also known as level 88). In these cases, all of the consumers need to rebuild, whether they have functional changes to their code or not.\n\nEnterprises typically already have their own rules in place for how to manage cross-application dependencies and impacts. It is most common for shared interface modifications to be done in a backward-compatible manner, meaning a change of the interface does not break the communication. Or, if there would be a breaking change, the developers create a new copy rather than extending and breaking the existing interface. Thus, applications consuming that interface can continue to work even without adopting the new change. The only consumers that would need to rebuild would be those that will leverage the new feature (or other change that was made to the interface). This way of making backward-compatible changes depends on the developers’ expertise in implementing them.\nEspecially with breaking changes, a process needs to be defined for planning and delivering application interfaces’ changes. Traditionally, the adoption process is often handled by an audit report generally run late in the cycle. However, having early notification and understanding of these changes helps improve and speed delivery of business function. Understanding your enterprise’s current rules for managing shared interfaces is important for being able to accommodate them when implementing build rules in your new CI/CD pipeline system.\n\n\nChanges to public interfaces should be coordinated between the different applications. Various levels of coordination for the adoption process can be defined:\n\nNo process (in other words, immediate adoption):\n\nIn this case, the application owning the modified interface shares it by publishing it to a shared file system (for example, the COPYLIB). The next time applications consuming that interface compile, they will get impacted by the changes immediately. This is the unfortunate reality of many organizations today. Developers can be hit by unexpected changes that break their code. To minimize the risk, developers start to develop and test in isolation before sharing the updated interfaces. With Git and DBB, they can develop in isolation, build within their isolation, and fully test before they integrate with the rest of the team.\nUsually this scheme does not scale (that is, it works for small teams, or small sets of applications only) and slows down development teams as they grow. It can be applied for application internal interfaces without impacts to other applications.\n\nAdoption at a planned date:\n\nThe application providing the shared interface announces (for example, via a meeting, email, or planning tool) that they will introduce a change at a given date, when the consuming applications can pick it up. This allows the consuming applications to prepare for the change, and is part of the resource planning. An outline of the future version might also be made available with the announcement.\n\nAdoption at own pace:\n\nThe providing application publishes the new version of the interface to a shared location, which could be a file system, a separated Git repository, or an artifact repository. The teams using the interface can then determine themselves when to accept the change into their consuming application(s), or if they want to work with an older version of the interface.\n\n\nWhichever adoption method is selected, we need to ensure there is only one production runtime environment. On the source code level, applications need to consolidate and reconcile development activities to ensure changes are not accidentally picked up before being ready to move forward. The most common method for achieving this is to use an agreed-upon Git branching strategy to organize and manage development work feeding into the CI/CD pipeline.\n\n\n\nWhen implementing rules for the build process in the CI/CD pipeline, different build strategies exist with different implications. There are two typical approaches you can consider:\n\nAutomated build of all consumers: After changing and making a shared interface available to others, the build pipelines will automatically build the programs which include the copybook.\n\nExample: When a shared copybook gets modified and made available to others, the build pipeline will automatically build programs that include that copybook.\nQuestions to consider with this approach:\n\nWho owns the testing efforts to validate that the program logic did not break?\nWho is responsible of packaging, staging and deploying the new executable? (This can only be the application owner.)\n\n\nPhased adoption of changed shared interfaces: If the modifications to the copybooks are backward-compatible, then consumers will incorporate the changes the next time they modify the programs. Thus, adoption of the external copybook change happens at different times.\n\nSince consumers of the shared interface are not forced to rebuild with this approach, this means that while some production load modules might run with the latest copybook, others might run with older versions of the copybook. In other words, there will be multiple versions of this shared interface included in the load modules in production. As long as the copybooks are modified in a backward compatible way, this should not be a problem.\n\n\nThe build strategy you pick for shared interfaces should depend on what your current workflow is, and if you want to continue with a similar workflow, or change that flow. General strategies for designing different builds with DBB and zAppBuild are described in Designing the build strategy.\nFor pipeline builds using DBB, zAppBuild’s impactBuild capability can detect all changed files, such as modified copybooks or subroutines, and then based on that, it can find all the programs (in Git repositories) that are impacted by that change and build them. The most common use case for pipeline build automation with DBB is an automated impact build within an application (rather than between repositories). However, if you would like to force a rebuild of all consumers of a shared interface, including those in other repositories, then some considerations should be made to understand how the consumers include the external module, and subsequently, how to configure the build scope for external dependencies. Several scenarios for implementing specific build scopes depending on development practices and applications’ requirements are described in the document Managing the build scope in IBM DBB builds with IBM zAppBuild.\n\n\n\n\nThis page contains reformatted excerpts from Develop Mainframe Software with OpenSource Source code managers and IBM Dependency Based Build.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "dependency-management.html#layout-of-dependencies-of-a-mainframe-application",
    "href": "dependency-management.html#layout-of-dependencies-of-a-mainframe-application",
    "title": "Defining dependency management",
    "section": "",
    "text": "From a runtime perspective in z/OS, programs run either independently (batch programs) or online in a middleware (CICS, IMS) runtime environment. Programs can use messaging resources like MQ queues or data persistence in the form of database tables, or files. Programs can also call other programs. In z/OS, called programs can either be statically bound or use dynamic linking. If a COBOL program is the first program in a run unit, that COBOL program is the main program. Otherwise, the COBOL program and all other COBOL programs in the run unit are subprograms1. The runtime environment involves various layers, including dependencies expressed between programs and resources or programs and subprograms.\nThere are multiple types of relationships to consider. The source files in the SCM produce the binaries that run on z/OS. To create the binaries, a set of source level dependencies must be understood. There is also a set of dependencies used during run time. These multiple levels of dependencies are defined in different ways, and in some cases not clearly defined at all. Understanding and finding the dependencies in source files is the first challenge.\nBuilding a program involves different steps:\n\nCompilation including any pre-compilation steps, defined as explicit steps or as option of the compiler, creates a non-executable binary (object deck) file.\nLink-edit, which assembles the object deck of the program with other objects and runtime libraries as appropriate. Link-edit can be driven by instructions (a link card) from the SCM or as dynamically defined in the build process.\n\nThese steps are summarized in the following diagram.\n\n\n\nProducing object code and executables\n\n\nAs part of the traditional build process, some additional steps, such as binds to databases, are sometimes included. The function of these steps is to prepare the runtime for a given execution environment. These should not be included in the build process itself, but should instead be included in the deployment process.\n\n\nMost of the time when people think about an application, it is from a runtime point of view. Several components are required for an application to run. Some of these are required as dependencies, such as the database or middleware and its configuration, while others are required as related, such as other applications that might be called.\nEverything running in a runtime environment starts as source from an SCM - or at least, this should be the goal when you consider infrastructure as code. Some source files represent definitions or are scripts that are not required to be built. Those that do require being built generally require other source files such as copybooks, but might not require the CICS definition, for example. Some of the source files are also included in many different programs - for example, a copybook can be used by many programs to represent the shared data structure. It is important to understand the relationships and dependencies between the source files, and when those relationships or dependencies have importance. The copybook is required to build the program, so it is required at compile time, but it is not used during run time. The configuration for a program such as the CICS transaction definition or the database schema is related to the application, but is required only for the runtime environment.\nA concrete dependency is the interface description when calling a program. A copybook defines the data structure to pass parameters to a program. So, the copybook is important to be shared while the program is part of the implementation.\n\n\n\nOn z/OS, there are two ways programs are generally called: dynamically and statically. Statically called programs are linked together at build time. These dependencies must be tracked as part of the build process to ensure they are correctly assembled. For dynamic calls, the two programs are totally separate. The programs are built and link-edited separately. At runtime, the subprogram is called based on the library concatenation.\nMany organizations have been moving to increased usage of dynamic calls as that approach reduces the complexity at build time. However, this approach means that the runtime dependencies need to be tracked and understood if any changes are made that require updates in both program and subprogram.\nThese programs and subprograms are interdependent even when using dynamic calls. When a program calls another program, generally they share data. A transfer of control occurs between the program and the subprogram with the main program passing a set of data to the subprogram and generally expecting some data in response.\nDifferent mechanisms exist to share pieces of data based on the language or the runtime. However, there is a need for the caller and the called program to define the data structure to be shared. The call of a subprogram is based on a list of transfer parameters, represented in the interface description like an application programming interface (API), but it is more tightly coupled than today’s APIs that are based on representational state transfer (REST).\nCommonly, shared data structure is defined in an included source file - for example, COBOL uses copybooks.\n\n\n\nTransfer of control sharing data between a program and subprogram\n\n\nIt is very common to define multiple copybooks for programs in order to isolate data structures and reuse them in other areas of an application component. Using copybooks allows more modularity at source level and facilitates dealing with private and shared data structures, or even private or shared functions.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "dependency-management.html#applications-and-programs",
    "href": "dependency-management.html#applications-and-programs",
    "title": "Defining dependency management",
    "section": "",
    "text": "In a web application, it is relatively easy to define an application because the physical artifact that is deployed is the complete representation of such an application: the EAR or WAR file. In the Windows world, it is more complicated since an application can be made of several executables and DLLs, but these are generally packaged together in an installable application or defined by a package manager.\nAn application is generally defined by the function or functions it provides. Sometimes there is a strong mapping between the physical parts that are shipped, and sometimes it is a set of parts that run the application.\nIn the mainframe, we fall closer to the second case where applications are defined by functions. However, based on the way the applications have grown over the years, there may be no clear boundary as to where one application ends and another one begins. An application can be defined physically by a set of resources (load modules, DBRMs, definitions) that belong together as they contribute to the same purpose: the calculation of health insurance policies, customer account management, and so on.\nAt the source file level, the relevant files contributing to an application are derived from the runtime of an application. These files can usually be identified by different means: a set of naming conventions, the ownership, information stored in the SCM, etc. It may not seem obvious at first glance, but most of the time it is possible to define which source files contribute to a given application.\nScoping your source files to an application has many benefits. It formalizes the boundaries of the application, and therefore its interfaces; it allows you to define clear ownership; and it helps with the inventory of the portfolio of an organization. Planning of future features to implement should be more accurate based on this scoping.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "dependency-management.html#applications-and-application-groups",
    "href": "dependency-management.html#applications-and-application-groups",
    "title": "Defining dependency management",
    "section": "",
    "text": "Within an organization, multiple applications generally make up the business function. An insurance company may have applications dedicated to health insurance, car insurance, personal health, or group health policies. These applications may be managed by different teams, but they must interact. Teams must define the interfaces or contracts between the applications. Today, many of these interactions are tightly coupled with only a shared interface defining the relationship.\nAs we have seen so far, for traditional z/OS applications, the interface is not separate but defined in source via a shared interface definition, generally a copybook or include. This source must be included in each program build for them to be able to interact. With this information, an application can be defined by two main components: shared interfaces that are used to communicate with other programs and the actual implementation of the programs.\nIt is important to note that shared copybooks could be shared not only within an application, but also across programs or across applications. The only way other programs or applications can interact with the program is by including the shared interface definition. A z/OS load module does not work like a Java Archive (JAR) file, because it is does not expose interface definitions.\nThe following diagram illustrates the concept of an application having a private implementation (its inner workings, represented in tan), and a public interface to interact with other programs and applications (represented in blue).\n\n\n\nAn application exposes a public interface\n\n\nAs applications communicate, their implementation consumes the public interface of the applications with which they interact. This concept of a public interface is common in Java programs and the way the communication between applications is defined. This principle can also be applied to existing COBOL and PL/I programs to help explain the structure required for a modern SCM, and is illustrated in the following diagram, with the applications’ usage of other applications’ interfaces indicated in red.\n\n\n\nApplications consume public interfaces of other applications",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "dependency-management.html#cross-cutting-interfaces",
    "href": "dependency-management.html#cross-cutting-interfaces",
    "title": "Defining dependency management",
    "section": "",
    "text": "There are additional capabilities that might need to be shared in addition to sets of data structures for application communication. These capabilities might include standard security or logging functions and can be considered cross-cutting, infrastructure-level interfaces (represented in brown in the following diagram). These capabilities may be developed once and then included in many different programs. It would be very helpful if these additional included capabilities could also be handled as shared components with their own application lifecycle. The challenge comes when these components change in a non-compatible way. These types of changes are generally infrequent but might be needed at times.\n\n\n\nInterfaces used by applications, including cross-cutting interfaces\n\n\nIn the preceding sections, we have laid out some of the key factors when considering the source code of traditional mainframe applications. The environment generally consists of many different applications that can provide shared interfaces and could consume shared components, or cross-cutting interfaces.\nThe knowledge of these factors and their respective lifecycles can guide the desired structure of source files in the SCM. Several patterns are possible to provide appropriate isolation, but to also provide appropriate sharing based on different requirements.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "dependency-management.html#managing-and-life-cycle-of-shared-interfaces",
    "href": "dependency-management.html#managing-and-life-cycle-of-shared-interfaces",
    "title": "Defining dependency management",
    "section": "",
    "text": "As discussed in previous sections, applications often provide interfaces and rely on other applications’ interfaces to communicate. Each application has its own life cycle of changes. In this process, changes are made. The changes are stabilized, and then go to production. In the end, there is a single production execution environment where all the applications will run. The applications must integrate, but the most common breaking point is their interfaces. If an application’s interface changes in a non-backward compatible manner and the other applications do not react to this change (to at least recompile the modules affected by the interface change), then the production application will break. Therefore, most changes are implemented in such a way so that the structure of the data that is shared is not changed, but instead filler fields are used, or data is added to the end where only applications who need to react to the change are required to respond. In discussing the management of shared interfaces, we will refer to the process of reacting to changes as “adoption”, and the applications consuming the interface as “consumers”.\nThe main types of changes that can be made to an interface are as follows:\n\nBackward compatible change: The interface changes, but the change is done such that all applications do not need to react. A typical example is when a field is added to a data structure, but this field does not have to be handled by most applications except a selected few, which requested this new field. Usually, fillers are used in a data structure and when people declare a new field, the overall structure of the data structure stays the same. Only the consumers interested in the new field need to adopt by using the new element in the interface. The others should use the new interface to stay current, but do not need to adopt it at this point. A standard practice is to adopt the change at least for those programs which are changing and recompiling for other reasons. Many organizations do not do this type of adoption until the change has made it to production, to be sure they will not be dependent on a change that gets pulled from the release late in the cycle.\nBreaking change: A data structure has changed in such a way that the overall memory layout of the data is impacted. It could be because of an array in a data structure. There are other cases, as well (such as changes in arrays or condition name changes also known as level 88). In these cases, all of the consumers need to rebuild, whether they have functional changes to their code or not.\n\nEnterprises typically already have their own rules in place for how to manage cross-application dependencies and impacts. It is most common for shared interface modifications to be done in a backward-compatible manner, meaning a change of the interface does not break the communication. Or, if there would be a breaking change, the developers create a new copy rather than extending and breaking the existing interface. Thus, applications consuming that interface can continue to work even without adopting the new change. The only consumers that would need to rebuild would be those that will leverage the new feature (or other change that was made to the interface). This way of making backward-compatible changes depends on the developers’ expertise in implementing them.\nEspecially with breaking changes, a process needs to be defined for planning and delivering application interfaces’ changes. Traditionally, the adoption process is often handled by an audit report generally run late in the cycle. However, having early notification and understanding of these changes helps improve and speed delivery of business function. Understanding your enterprise’s current rules for managing shared interfaces is important for being able to accommodate them when implementing build rules in your new CI/CD pipeline system.\n\n\nChanges to public interfaces should be coordinated between the different applications. Various levels of coordination for the adoption process can be defined:\n\nNo process (in other words, immediate adoption):\n\nIn this case, the application owning the modified interface shares it by publishing it to a shared file system (for example, the COPYLIB). The next time applications consuming that interface compile, they will get impacted by the changes immediately. This is the unfortunate reality of many organizations today. Developers can be hit by unexpected changes that break their code. To minimize the risk, developers start to develop and test in isolation before sharing the updated interfaces. With Git and DBB, they can develop in isolation, build within their isolation, and fully test before they integrate with the rest of the team.\nUsually this scheme does not scale (that is, it works for small teams, or small sets of applications only) and slows down development teams as they grow. It can be applied for application internal interfaces without impacts to other applications.\n\nAdoption at a planned date:\n\nThe application providing the shared interface announces (for example, via a meeting, email, or planning tool) that they will introduce a change at a given date, when the consuming applications can pick it up. This allows the consuming applications to prepare for the change, and is part of the resource planning. An outline of the future version might also be made available with the announcement.\n\nAdoption at own pace:\n\nThe providing application publishes the new version of the interface to a shared location, which could be a file system, a separated Git repository, or an artifact repository. The teams using the interface can then determine themselves when to accept the change into their consuming application(s), or if they want to work with an older version of the interface.\n\n\nWhichever adoption method is selected, we need to ensure there is only one production runtime environment. On the source code level, applications need to consolidate and reconcile development activities to ensure changes are not accidentally picked up before being ready to move forward. The most common method for achieving this is to use an agreed-upon Git branching strategy to organize and manage development work feeding into the CI/CD pipeline.\n\n\n\nWhen implementing rules for the build process in the CI/CD pipeline, different build strategies exist with different implications. There are two typical approaches you can consider:\n\nAutomated build of all consumers: After changing and making a shared interface available to others, the build pipelines will automatically build the programs which include the copybook.\n\nExample: When a shared copybook gets modified and made available to others, the build pipeline will automatically build programs that include that copybook.\nQuestions to consider with this approach:\n\nWho owns the testing efforts to validate that the program logic did not break?\nWho is responsible of packaging, staging and deploying the new executable? (This can only be the application owner.)\n\n\nPhased adoption of changed shared interfaces: If the modifications to the copybooks are backward-compatible, then consumers will incorporate the changes the next time they modify the programs. Thus, adoption of the external copybook change happens at different times.\n\nSince consumers of the shared interface are not forced to rebuild with this approach, this means that while some production load modules might run with the latest copybook, others might run with older versions of the copybook. In other words, there will be multiple versions of this shared interface included in the load modules in production. As long as the copybooks are modified in a backward compatible way, this should not be a problem.\n\n\nThe build strategy you pick for shared interfaces should depend on what your current workflow is, and if you want to continue with a similar workflow, or change that flow. General strategies for designing different builds with DBB and zAppBuild are described in Designing the build strategy.\nFor pipeline builds using DBB, zAppBuild’s impactBuild capability can detect all changed files, such as modified copybooks or subroutines, and then based on that, it can find all the programs (in Git repositories) that are impacted by that change and build them. The most common use case for pipeline build automation with DBB is an automated impact build within an application (rather than between repositories). However, if you would like to force a rebuild of all consumers of a shared interface, including those in other repositories, then some considerations should be made to understand how the consumers include the external module, and subsequently, how to configure the build scope for external dependencies. Several scenarios for implementing specific build scopes depending on development practices and applications’ requirements are described in the document Managing the build scope in IBM DBB builds with IBM zAppBuild.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "dependency-management.html#resources",
    "href": "dependency-management.html#resources",
    "title": "Defining dependency management",
    "section": "",
    "text": "This page contains reformatted excerpts from Develop Mainframe Software with OpenSource Source code managers and IBM Dependency Based Build.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Defining dependency management"
    ]
  },
  {
    "objectID": "artifact-repository.html",
    "href": "artifact-repository.html",
    "title": "Artifact repository",
    "section": "",
    "text": "Once building occurs, the pipeline then publishes and stores the build outputs as a package in the artifact repository. This package contains any artifact that will need to be deployed, such as load modules, DBRMs, DDL, and the configuration files for the subsystems. Importantly, the package also contains the build artifacts’ metadata and other necessary pieces of information that enable any changes to be traced back to the version control system. Depending on the system, the package can be a WAR, EAR files, a Windows installer package, among others. The artifact repository can also be used as the publishing platform to store intermediate files needed in the build phase.\nThe artifact repository contains a complete history of packages, and therefore also provides access to older versions. This feature is especially important in cases where a rollback or audit is required. The artifact repository is meant to be the single point of truth for binaries, much in the same way that a SCM is the single point of truth for source files.\nIt is expected that a package will be deployed to several execution environments, each of them being used for different testing phases. Ultimately, some packages will be deployed to production. In this arrangement, the artifact repository acts like a proxy for the deployment manager, which is responsible for deploying the artifacts produced by the build system to one or more runtime environments.\nThe key mission and benefit of an artifact repository is to decouple source code management (SCM) configurations from runtime environments. This supports the fundamental DevOps principle of “build once, deploy many”. Once you build and test a set of binaries to verify it, then that is the same set of binaries that you will want to deploy to the production environment. By ensuring you can use the same set of executables between your deployment environments, from testing to production, you not only reduce the risk of build time issues going undetected into your production environments, but it also becomes much easier to determine if a deployment problem is the result of a build time issue or a runtime environment issue.\n\n\n\nJFrog Artifactory\nSonatype Nexus\nUrbanCode Deploy (UCD) Codestation\nAzure Artifacts\n\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Artifact repository"
    ]
  },
  {
    "objectID": "artifact-repository.html#common-artifact-repository-options",
    "href": "artifact-repository.html#common-artifact-repository-options",
    "title": "Artifact repository",
    "section": "",
    "text": "JFrog Artifactory\nSonatype Nexus\nUrbanCode Deploy (UCD) Codestation\nAzure Artifacts",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Artifact repository"
    ]
  },
  {
    "objectID": "artifact-repository.html#resources",
    "href": "artifact-repository.html#resources",
    "title": "Artifact repository",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Artifact repository"
    ]
  },
  {
    "objectID": "pipeline-orchestrator.html",
    "href": "pipeline-orchestrator.html",
    "title": "Pipeline orchestrator",
    "section": "",
    "text": "Also known as the CI (Continuous Integration) Orchestrator - This is where automation happens. The CI Orchestrator provides connectors to version control, build systems, and packaging and deployment. Its goal is to remove manual and repetitive tasks as much as possible. It also drives the building of the application package, includes automated unit tests, and publishes the results in an artifact repository to make them available to the provisioning and deployment practices.\n\n\n\nGitLab CI\nJenkins\nGitHub Actions\nAzure Pipelines\n\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Pipeline orchestrator"
    ]
  },
  {
    "objectID": "pipeline-orchestrator.html#common-pipeline-orchestrator-options",
    "href": "pipeline-orchestrator.html#common-pipeline-orchestrator-options",
    "title": "Pipeline orchestrator",
    "section": "",
    "text": "GitLab CI\nJenkins\nGitHub Actions\nAzure Pipelines",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Pipeline orchestrator"
    ]
  },
  {
    "objectID": "pipeline-orchestrator.html#resources",
    "href": "pipeline-orchestrator.html#resources",
    "title": "Pipeline orchestrator",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Pipeline orchestrator"
    ]
  },
  {
    "objectID": "ibm-z-devops-solution.html",
    "href": "ibm-z-devops-solution.html",
    "title": "IBM Z DevOps solution",
    "section": "",
    "text": "IBM Z DevOps is a solution for z/OS application modernization that uses a Git-based continuous integration/continuous delivery (CI/CD) pipeline. It consists of several key components, including Git for source code management and IBM Dependency Based Build (DBB), that empower mainframe application teams to participate in modern DevOps processes using industry-standard and open source tools.\nThe components that make up the IBM Z DevOps solution are loosely coupled and highly customizable, allowing you to implement a CI/CD pipeline that maximizes the tools and processes shared by your distributed and mainframe teams.\nThere are multiple advantages to adopting a standardized CI/CD pipeline across your enterprise:\n\nEquip developers for productivity\n\nOpen source tools such as Git, Jenkins, and Artifactory have become the de facto industry standard for powering continuous integration and continuous delivery in the enterprise. These tools support today’s best practices and provide advanced functionality in version control, automation, and more.\n\nCollaborate and communicate more effectively across the enterprise\n\nStandardizing the CI/CD pipeline and development workflows where possible between distributed and mainframe teams makes it easier to communicate and collaborate across the enterprise. This is especially beneficial for hybrid applications, which consist of both mainframe and distributed components.\n\nEnable modern approaches such as DevOps and Agile for mainframe applications\n\nIndustry-standard tools such as version control with Git and automation with CI/CD pipelines enable smaller, high-quality development increments and deliverables. This lets your z/OS application teams respond faster and more effectively to changing market pressures or customer needs, while also building towards a larger development goal.\n\nReduce the overhead of onboarding the next generation of z/OS application developers.\n\nNewer developers are likely to be more familiar with industry-standard tools, and being able to use these technologies in the z/OS workplace lowers the entry barrier for them to get started with contributions to the team.\n\n\nThis documentation site aims to help your enterprise succeed in the journey from legacy development processes and library managers to a modern CI/CD pipeline.\n\n\n\nThe IBM Z DevOps Acceleration Program (DAP) can help your enterprise get started on the journey to adopt this modern and agile approach to z/OS applications. DAP is a no charge value-add early adoption program designed to partner with clients during four distinct stages that are necessary for any DevOps transformation:\n\nValue Stream Assessment\nTraining\nDeployment\nAdoption\n\nTo learn what’s involved in migrating your z/OS applications to the IBM Z DevOps solution, see Understanding the migration effort.\nTo learn more about the CI/CD pipeline itself for z/OS applications, see the CI/CD pipeline introduction.",
    "crumbs": [
      "Overview",
      "IBM Z DevOps solution"
    ]
  },
  {
    "objectID": "ibm-z-devops-solution.html#what-is-ibm-z-devops",
    "href": "ibm-z-devops-solution.html#what-is-ibm-z-devops",
    "title": "IBM Z DevOps solution",
    "section": "",
    "text": "IBM Z DevOps is a solution for z/OS application modernization that uses a Git-based continuous integration/continuous delivery (CI/CD) pipeline. It consists of several key components, including Git for source code management and IBM Dependency Based Build (DBB), that empower mainframe application teams to participate in modern DevOps processes using industry-standard and open source tools.\nThe components that make up the IBM Z DevOps solution are loosely coupled and highly customizable, allowing you to implement a CI/CD pipeline that maximizes the tools and processes shared by your distributed and mainframe teams.\nThere are multiple advantages to adopting a standardized CI/CD pipeline across your enterprise:\n\nEquip developers for productivity\n\nOpen source tools such as Git, Jenkins, and Artifactory have become the de facto industry standard for powering continuous integration and continuous delivery in the enterprise. These tools support today’s best practices and provide advanced functionality in version control, automation, and more.\n\nCollaborate and communicate more effectively across the enterprise\n\nStandardizing the CI/CD pipeline and development workflows where possible between distributed and mainframe teams makes it easier to communicate and collaborate across the enterprise. This is especially beneficial for hybrid applications, which consist of both mainframe and distributed components.\n\nEnable modern approaches such as DevOps and Agile for mainframe applications\n\nIndustry-standard tools such as version control with Git and automation with CI/CD pipelines enable smaller, high-quality development increments and deliverables. This lets your z/OS application teams respond faster and more effectively to changing market pressures or customer needs, while also building towards a larger development goal.\n\nReduce the overhead of onboarding the next generation of z/OS application developers.\n\nNewer developers are likely to be more familiar with industry-standard tools, and being able to use these technologies in the z/OS workplace lowers the entry barrier for them to get started with contributions to the team.\n\n\nThis documentation site aims to help your enterprise succeed in the journey from legacy development processes and library managers to a modern CI/CD pipeline.",
    "crumbs": [
      "Overview",
      "IBM Z DevOps solution"
    ]
  },
  {
    "objectID": "ibm-z-devops-solution.html#ibm-z-devops-acceleration-program",
    "href": "ibm-z-devops-solution.html#ibm-z-devops-acceleration-program",
    "title": "IBM Z DevOps solution",
    "section": "",
    "text": "The IBM Z DevOps Acceleration Program (DAP) can help your enterprise get started on the journey to adopt this modern and agile approach to z/OS applications. DAP is a no charge value-add early adoption program designed to partner with clients during four distinct stages that are necessary for any DevOps transformation:\n\nValue Stream Assessment\nTraining\nDeployment\nAdoption\n\nTo learn what’s involved in migrating your z/OS applications to the IBM Z DevOps solution, see Understanding the migration effort.\nTo learn more about the CI/CD pipeline itself for z/OS applications, see the CI/CD pipeline introduction.",
    "crumbs": [
      "Overview",
      "IBM Z DevOps solution"
    ]
  },
  {
    "objectID": "intro-migrating-data.html",
    "href": "intro-migrating-data.html",
    "title": "Introduction to migrating data",
    "section": "",
    "text": "Migrating your source code data from z/OS to Git is one of the first steps in implementing a modern CI/CD pipeline. Once you have planned your to-be Git repository layouts, you can use the steps in the following section to migrate your data from z/OS to Git. During the migration process, it is important to be aware of potential issues with migrating non-printable or non-roundtrippable characters, and to have a plan for how to handle them if you encounter them. You can read more about how to handle these characters in Managing code page conversion.\n\n\n\nAt a high level, the steps for migrating data from z/OS to Git are as follows:\n\nUnload the source code from the legacy version control system to PDS libraries: Legacy version control systems often store data in a proprietary format, which IBM’s Dependency Based Build (DBB) cannot directly access. Therefore, the source code should first be unloaded to PDS libraries.\nLoad files from PDS libraries to a Git repository on z/OS UNIX System Services (z/OS UNIX) using one of the following methods:\n\nDBB migration tool: The DBB migration tool is provided with DBB, and is the most commonly-used method for migrating data from z/OS to Git.\nSCLM-to-Git migration tool: You can use the SCLM-to-Git migration tool if you are moving away from SCLM as your current library manager.\nManual migration: Manual migration of source code from z/OS to Git is possible, but is generally not recommended since it tends to be slower, more tedious, and prone to human error.\n\nOnce your code is in z/OS UNIX, you can use Git to push it up to your central Git provider.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Introduction to migrating data"
    ]
  },
  {
    "objectID": "intro-migrating-data.html#introduction-to-migrating-data",
    "href": "intro-migrating-data.html#introduction-to-migrating-data",
    "title": "Introduction to migrating data",
    "section": "",
    "text": "Migrating your source code data from z/OS to Git is one of the first steps in implementing a modern CI/CD pipeline. Once you have planned your to-be Git repository layouts, you can use the steps in the following section to migrate your data from z/OS to Git. During the migration process, it is important to be aware of potential issues with migrating non-printable or non-roundtrippable characters, and to have a plan for how to handle them if you encounter them. You can read more about how to handle these characters in Managing code page conversion.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Introduction to migrating data"
    ]
  },
  {
    "objectID": "intro-migrating-data.html#steps-for-migrating-data-from-zos-to-git",
    "href": "intro-migrating-data.html#steps-for-migrating-data-from-zos-to-git",
    "title": "Introduction to migrating data",
    "section": "",
    "text": "At a high level, the steps for migrating data from z/OS to Git are as follows:\n\nUnload the source code from the legacy version control system to PDS libraries: Legacy version control systems often store data in a proprietary format, which IBM’s Dependency Based Build (DBB) cannot directly access. Therefore, the source code should first be unloaded to PDS libraries.\nLoad files from PDS libraries to a Git repository on z/OS UNIX System Services (z/OS UNIX) using one of the following methods:\n\nDBB migration tool: The DBB migration tool is provided with DBB, and is the most commonly-used method for migrating data from z/OS to Git.\nSCLM-to-Git migration tool: You can use the SCLM-to-Git migration tool if you are moving away from SCLM as your current library manager.\nManual migration: Manual migration of source code from z/OS to Git is possible, but is generally not recommended since it tends to be slower, more tedious, and prone to human error.\n\nOnce your code is in z/OS UNIX, you can use Git to push it up to your central Git provider.",
    "crumbs": [
      "Migrating data from z/OS to Git",
      "Introduction to migrating data"
    ]
  },
  {
    "objectID": "intro-cicd-zos.html",
    "href": "intro-cicd-zos.html",
    "title": "Introduction to CI/CD for z/OS",
    "section": "",
    "text": "CI/CD a development process that empowers enterprises to develop and deliver higher quality software more frequently. The “CI” in “CI/CD” stands for “continuous integration”, while the “CD” can stand for either “continuous delivery” and/or “continuous deployment”.\n\n“Continuous integration” means that new code is merged into the shared code base at more frequent intervals (for example, daily or weekly). This allows for more frequent builds of the code base, enabling more frequent automated testing against the application builds, which can help identify integration issues earlier in the development cycle, when they are easier to fix, and subsequently reduce issues closer to release.\n“Continous delivery” is when the changes to an application are automatically tested and uploaded (delivered) to a repository, where it can then be deployed to different environments. This practice can reduce time to deployment due to the integration of continuous testing and development, and can thus reduce the cost of development without compromising quality. Although the deployment itself is automated, manual approval is still required to authorize the deployment.\n“Continuous deployment” is when the changes to an application are automatically deployed to a production environment for use by your customers. It is considered to be one step further than continuous delivery, and might or might not be implemented depending on business needs. With continuous deployment, the developers and automated tests in production-like test environments are trusted enough that the new code is considered approved when it passes the tests. Deployment no longer requires manual approval and is also automated to speed the time to customer value.\n\nCI/CD is powered by a set of tools that automates processes in the software development lifecycle. As a whole, the CI/CD pipeline can enable teams to adopt Agile and DevOps workflows so that application teams can deliver changes more frequently and reliably, and respond more efficiently to feedback from operations. For this reason, CI/CD is considered a best practice in today’s industry standards. IBM’s general DevOps page provides additional information on CI/CD in the context of DevOps.\n\n\n\nLegacy z/OS development tools and processes have traditionally made it difficult for z/OS applications to participate in modern CI/CD pipelines. However, the IBM Z DevOps solution allows you to integrate z/OS applications into the same CI/CD pipelines used by today’s distributed applications. That means z/OS teams can now leverage the same modern, industry standard, open source technologies for integrated development environments (IDEs), version control, automation, and more. You can learn more about how IBM supports DevOps for the mainframe at the IBM Z DevOps and hybrid cloud continuous integration pages.\nThe defining components of the IBM Z DevOps solution are Git-based source code management and the IBM Dependency Based Build (DBB) tool. When these components are integrated together with other industry-standard CI/CD pipeline tools, the developer workflow looks something like the Day in the life of a developer described in the following section.\n\n\nWith the IBM Z DevOps CI/CD pipeline in place, an application developer would typically use the following (or similar) workflow when completing a development task (for example, a bugfix or feature enhancement):\nTip: The IBM Z DevOps CI/CD pipeline is based on Git as the source code management tool. If you are new to Git and its concepts (such as branches and pull/merge requests), you can check out the Source code management page to get familiar with the terminology so you can better understand the workflow below.\n\nThe developer will start by cloning or pulling a copy of the application code from a central Git repository down to her local workstation.\nShe can then create a new personal branch of that code for her specific task. This will allow her to work on the task in isolation, in parallel with her team, without having to worry about other development activities disturbing her work.\nOnce she has made her code changes that she is ready to test, she can use IBM’s Dependency Based Build (DBB) tool to build the program so that she can test it individually and verify that her fix works and does not cause regressions.\nOnce she is happy with her changes, she can commit them to her personal branch of code, and then push her personal branch with her code changes to the central Git repository.\nNow, she can open a pull request to have the changes in her personal branch of code be merged into the shared, common branch of code for her team. Her team has set up an automated pipeline to run builds of the code in the pull request, which also include tests and code scans.\n\nThis is also the point at which her team has an approvals process where she can add teammates to review her changes and approve them before merging them into their common branch of code.\n\nOnce her pull request is approved and her changes are merged into the common branch of code, the personal branch of code where she originally did her development work can be deleted, and a full or impact pipeline build can run on the common branch of code to move the changes forward or associate them with a release.\n\n\n\n\nThe developer’s workflow in the previous section is enabled by CI/CD pipeline tooling, the major components of which are summarized in this section. The following image depicts steps in the CI/CD pipeline with two different colors, yellow and green. Yellow steps highlight the steps performed by the developer, while green steps are performed by the pipeline. For the developer, their steps include check-out and check-in of code, but also include triggering of a pipeline job. Developer-level operations end at this point.\nAll subsequent steps, in green, are performed by the pipeline. These steps include building, publishing to an artifact repository, and deploying to an execution environment.\n\n\n\nMain components of a CI/CD pipeline for z/OS applications\n\n\nYou can click on each component in the following list to learn more about it and see common technology options:\n\nIntegrated development environment (IDE): The IDE is what the developer uses to check out and edit her code, as well it check it back into the version control system. Many modern editors have features that enhance development capabilities, such as syntax highlighting, code completion, outline view navigation, and variable lookups, as well as integrations such as debugging and unit testing.\nSource code management (SCM, Version control): The SCM is used to store and manage different versions of source code files, as well as application configuration files, test cases, and more. This is what enables the application development team to do parallel development. The IBM Z DevOps solution is based on Git as the SCM. For more information about Git and why it is paramount to the IBM Z DevOps solution, as well as an explanation of the Git concepts, see the SCM documentation.\nBuild: The build component takes care of understanding dependencies, and then compiling and linking programs to produce the executable binaries such as load modules and DBRMs. When running this component, you can also integrate automated steps for unit testing and code quality inspection (although these are sometimes considered as separate components in the CI/CD pipeline). In the IBM Z DevOps solution, the build is handled by IBM Dependency Based Build (DBB), which has intelligent build capabilities that enable you to perform different types of build to support various steps in your workflow. Some examples of these build types include single-program user builds, full application builds, and impact builds.\nArtifact repository: Once the build component has created the executable binaries, they are packaged together and uploaded into the artifact repository, along with metadata to help trace those binaries back to the source. This component is crucial for decoupling the source code management from the runtime environments, enabling the key DevOps practice of “Build once, deploy many”.\nDeployment manager: The deployment manager is the tool that rolls out the application packages. When it is time to deploy the application, the deployment manager downloads the package from the artifact repository and uploads the contents to the target libraries. If there are other steps to perform, such as installation steps like CICS NEWCOPY or PHASE-IN, or a bind step when DB2 is involved, the deployment manager also handles those. Importantly, it also keeps track of the inventory of execution environments so that you can know what each environment is running.\nPipeline orchestrator: The pipeline orchestrator oversees all the automated processes in the pipeline. This component integrates the steps from the different tools together and ensures they all run in the correct order.\n\nAlthough it might seem CI/CD requires developers to learn and work with a lot of different tools, they are primarily just working with the IDE for code editing, the SCM for version control, and performing some individual user builds. Once development gets to point where they want to integrate their code changes into their team’s shared codebase, the pipeline is largely automated via the pipeline orchestrator. This means that once the CI/CD pipeline is in place, if the developer has to interact with any of the automated components at all, they would mostly just be checking a dashboard or status, performing any intentionally manual approval steps, and/or verifying the results of the pipeline job.\n\n\nYou will find for many of the CI/CD pipeline components, multiple tools are available to perform the functionality. However, to reap the benefits of standardization across the enterprise, we generally recommend that clients pick the option that other parts of their organization (such as distributed development teams) are already using. Popular tooling options for each component in this section are listed on the respective component’s dedicated page, although it should be noted that these lists are not necessarily all-inclusive. For some more common combinations of technologies, the IBM Z DevOps Acceleration Team has produced detailed documentation on the setup and implementation.\n\n\n\n\nThe CI/CD pipeline building block components supply facilities to the development and delivery teams, such as:\n\nIsolation capabilities to reduce coordination efforts in larger groups\nIntegration workflows and traceability from tasks to code changes, binaries, and deliveries\nStandardized development practices across teams and platforms\nAutomation of build, packaging, and deployment tasks\n\nWhen it comes to isolation techniques, we refer to the branching strategies for the version control system. When using Git, branches are created to allow parallel development of the code, with each branch dedicated to a particular purpose, such as stable integration of the team’s code or an individual developer’s work on a hotfix or new feature. Developers can now integrate and share changes with the team through the enterprise’s central Git provider. Accordingly, additional workflows like merge or pull requests for code review and approvals are expected to be used. These concepts are further introduced in the SCM component overview page.\nCompared to a library manager, which relies on concatenation for compilation and linking, Git and its branches provide complete isolation and remove dependencies on the areas under current development by other development teams. The developer therefore works within the scope of the entire application. This also implies that a branch does not represent the contents of an execution environment. The branch is in fact fully decoupled from the environment via the artifact repository, ensuring a complete separation of the build and deployment phases. This decoupling enables developers to adopt previously impossible provisioning practices such as spinning up an isolated test execution environment with the push of a button.\nDefining an application package in a CI/CD pipeline will be different from the largely manual ad-hoc approaches seen in traditional mainframe library managers. With a CI/CD pipeline, the outputs of the build process are preconfigured to automatically be packaged together upon generation. These application packages are the inputs to the deployment manager, and the deployment manager is responsible for the installation of the packages to the execution environment.\n\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Introduction to CI/CD for z/OS"
    ]
  },
  {
    "objectID": "intro-cicd-zos.html#introduction-to-cicd",
    "href": "intro-cicd-zos.html#introduction-to-cicd",
    "title": "Introduction to CI/CD for z/OS",
    "section": "",
    "text": "CI/CD a development process that empowers enterprises to develop and deliver higher quality software more frequently. The “CI” in “CI/CD” stands for “continuous integration”, while the “CD” can stand for either “continuous delivery” and/or “continuous deployment”.\n\n“Continuous integration” means that new code is merged into the shared code base at more frequent intervals (for example, daily or weekly). This allows for more frequent builds of the code base, enabling more frequent automated testing against the application builds, which can help identify integration issues earlier in the development cycle, when they are easier to fix, and subsequently reduce issues closer to release.\n“Continous delivery” is when the changes to an application are automatically tested and uploaded (delivered) to a repository, where it can then be deployed to different environments. This practice can reduce time to deployment due to the integration of continuous testing and development, and can thus reduce the cost of development without compromising quality. Although the deployment itself is automated, manual approval is still required to authorize the deployment.\n“Continuous deployment” is when the changes to an application are automatically deployed to a production environment for use by your customers. It is considered to be one step further than continuous delivery, and might or might not be implemented depending on business needs. With continuous deployment, the developers and automated tests in production-like test environments are trusted enough that the new code is considered approved when it passes the tests. Deployment no longer requires manual approval and is also automated to speed the time to customer value.\n\nCI/CD is powered by a set of tools that automates processes in the software development lifecycle. As a whole, the CI/CD pipeline can enable teams to adopt Agile and DevOps workflows so that application teams can deliver changes more frequently and reliably, and respond more efficiently to feedback from operations. For this reason, CI/CD is considered a best practice in today’s industry standards. IBM’s general DevOps page provides additional information on CI/CD in the context of DevOps.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Introduction to CI/CD for z/OS"
    ]
  },
  {
    "objectID": "intro-cicd-zos.html#applying-cicd-to-zos-applications",
    "href": "intro-cicd-zos.html#applying-cicd-to-zos-applications",
    "title": "Introduction to CI/CD for z/OS",
    "section": "",
    "text": "Legacy z/OS development tools and processes have traditionally made it difficult for z/OS applications to participate in modern CI/CD pipelines. However, the IBM Z DevOps solution allows you to integrate z/OS applications into the same CI/CD pipelines used by today’s distributed applications. That means z/OS teams can now leverage the same modern, industry standard, open source technologies for integrated development environments (IDEs), version control, automation, and more. You can learn more about how IBM supports DevOps for the mainframe at the IBM Z DevOps and hybrid cloud continuous integration pages.\nThe defining components of the IBM Z DevOps solution are Git-based source code management and the IBM Dependency Based Build (DBB) tool. When these components are integrated together with other industry-standard CI/CD pipeline tools, the developer workflow looks something like the Day in the life of a developer described in the following section.\n\n\nWith the IBM Z DevOps CI/CD pipeline in place, an application developer would typically use the following (or similar) workflow when completing a development task (for example, a bugfix or feature enhancement):\nTip: The IBM Z DevOps CI/CD pipeline is based on Git as the source code management tool. If you are new to Git and its concepts (such as branches and pull/merge requests), you can check out the Source code management page to get familiar with the terminology so you can better understand the workflow below.\n\nThe developer will start by cloning or pulling a copy of the application code from a central Git repository down to her local workstation.\nShe can then create a new personal branch of that code for her specific task. This will allow her to work on the task in isolation, in parallel with her team, without having to worry about other development activities disturbing her work.\nOnce she has made her code changes that she is ready to test, she can use IBM’s Dependency Based Build (DBB) tool to build the program so that she can test it individually and verify that her fix works and does not cause regressions.\nOnce she is happy with her changes, she can commit them to her personal branch of code, and then push her personal branch with her code changes to the central Git repository.\nNow, she can open a pull request to have the changes in her personal branch of code be merged into the shared, common branch of code for her team. Her team has set up an automated pipeline to run builds of the code in the pull request, which also include tests and code scans.\n\nThis is also the point at which her team has an approvals process where she can add teammates to review her changes and approve them before merging them into their common branch of code.\n\nOnce her pull request is approved and her changes are merged into the common branch of code, the personal branch of code where she originally did her development work can be deleted, and a full or impact pipeline build can run on the common branch of code to move the changes forward or associate them with a release.\n\n\n\n\nThe developer’s workflow in the previous section is enabled by CI/CD pipeline tooling, the major components of which are summarized in this section. The following image depicts steps in the CI/CD pipeline with two different colors, yellow and green. Yellow steps highlight the steps performed by the developer, while green steps are performed by the pipeline. For the developer, their steps include check-out and check-in of code, but also include triggering of a pipeline job. Developer-level operations end at this point.\nAll subsequent steps, in green, are performed by the pipeline. These steps include building, publishing to an artifact repository, and deploying to an execution environment.\n\n\n\nMain components of a CI/CD pipeline for z/OS applications\n\n\nYou can click on each component in the following list to learn more about it and see common technology options:\n\nIntegrated development environment (IDE): The IDE is what the developer uses to check out and edit her code, as well it check it back into the version control system. Many modern editors have features that enhance development capabilities, such as syntax highlighting, code completion, outline view navigation, and variable lookups, as well as integrations such as debugging and unit testing.\nSource code management (SCM, Version control): The SCM is used to store and manage different versions of source code files, as well as application configuration files, test cases, and more. This is what enables the application development team to do parallel development. The IBM Z DevOps solution is based on Git as the SCM. For more information about Git and why it is paramount to the IBM Z DevOps solution, as well as an explanation of the Git concepts, see the SCM documentation.\nBuild: The build component takes care of understanding dependencies, and then compiling and linking programs to produce the executable binaries such as load modules and DBRMs. When running this component, you can also integrate automated steps for unit testing and code quality inspection (although these are sometimes considered as separate components in the CI/CD pipeline). In the IBM Z DevOps solution, the build is handled by IBM Dependency Based Build (DBB), which has intelligent build capabilities that enable you to perform different types of build to support various steps in your workflow. Some examples of these build types include single-program user builds, full application builds, and impact builds.\nArtifact repository: Once the build component has created the executable binaries, they are packaged together and uploaded into the artifact repository, along with metadata to help trace those binaries back to the source. This component is crucial for decoupling the source code management from the runtime environments, enabling the key DevOps practice of “Build once, deploy many”.\nDeployment manager: The deployment manager is the tool that rolls out the application packages. When it is time to deploy the application, the deployment manager downloads the package from the artifact repository and uploads the contents to the target libraries. If there are other steps to perform, such as installation steps like CICS NEWCOPY or PHASE-IN, or a bind step when DB2 is involved, the deployment manager also handles those. Importantly, it also keeps track of the inventory of execution environments so that you can know what each environment is running.\nPipeline orchestrator: The pipeline orchestrator oversees all the automated processes in the pipeline. This component integrates the steps from the different tools together and ensures they all run in the correct order.\n\nAlthough it might seem CI/CD requires developers to learn and work with a lot of different tools, they are primarily just working with the IDE for code editing, the SCM for version control, and performing some individual user builds. Once development gets to point where they want to integrate their code changes into their team’s shared codebase, the pipeline is largely automated via the pipeline orchestrator. This means that once the CI/CD pipeline is in place, if the developer has to interact with any of the automated components at all, they would mostly just be checking a dashboard or status, performing any intentionally manual approval steps, and/or verifying the results of the pipeline job.\n\n\nYou will find for many of the CI/CD pipeline components, multiple tools are available to perform the functionality. However, to reap the benefits of standardization across the enterprise, we generally recommend that clients pick the option that other parts of their organization (such as distributed development teams) are already using. Popular tooling options for each component in this section are listed on the respective component’s dedicated page, although it should be noted that these lists are not necessarily all-inclusive. For some more common combinations of technologies, the IBM Z DevOps Acceleration Team has produced detailed documentation on the setup and implementation.\n\n\n\n\nThe CI/CD pipeline building block components supply facilities to the development and delivery teams, such as:\n\nIsolation capabilities to reduce coordination efforts in larger groups\nIntegration workflows and traceability from tasks to code changes, binaries, and deliveries\nStandardized development practices across teams and platforms\nAutomation of build, packaging, and deployment tasks\n\nWhen it comes to isolation techniques, we refer to the branching strategies for the version control system. When using Git, branches are created to allow parallel development of the code, with each branch dedicated to a particular purpose, such as stable integration of the team’s code or an individual developer’s work on a hotfix or new feature. Developers can now integrate and share changes with the team through the enterprise’s central Git provider. Accordingly, additional workflows like merge or pull requests for code review and approvals are expected to be used. These concepts are further introduced in the SCM component overview page.\nCompared to a library manager, which relies on concatenation for compilation and linking, Git and its branches provide complete isolation and remove dependencies on the areas under current development by other development teams. The developer therefore works within the scope of the entire application. This also implies that a branch does not represent the contents of an execution environment. The branch is in fact fully decoupled from the environment via the artifact repository, ensuring a complete separation of the build and deployment phases. This decoupling enables developers to adopt previously impossible provisioning practices such as spinning up an isolated test execution environment with the push of a button.\nDefining an application package in a CI/CD pipeline will be different from the largely manual ad-hoc approaches seen in traditional mainframe library managers. With a CI/CD pipeline, the outputs of the build process are preconfigured to automatically be packaged together upon generation. These application packages are the inputs to the deployment manager, and the deployment manager is responsible for the installation of the packages to the execution environment.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Introduction to CI/CD for z/OS"
    ]
  },
  {
    "objectID": "intro-cicd-zos.html#resources",
    "href": "intro-cicd-zos.html#resources",
    "title": "Introduction to CI/CD for z/OS",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Introduction to CI/CD for z/OS"
    ]
  },
  {
    "objectID": "branching-model-supporting-pipeline.html",
    "href": "branching-model-supporting-pipeline.html",
    "title": "Supporting pipeline design and implementation",
    "section": "",
    "text": "This page details the technical implementation of the different continuous integration/continuous delivery (CI/CD) pipeline types used in the Git branching model for mainframe development. If a branching model workflow demands a specific configuration, it will be covered within the same section.\n\n\n\n\n\nBranching diagram of a feature branch\n\n\nWhen developers start working on a new task, they will first create a feature branch. Feature branches are created off the latest code state of the source configuration, whether that is the main branch or an epic or release maintenance branch.\nIf the feature branch was created on the central Git repository, the developers can use the integrated development environment (IDE), a terminal, or another Git interface on their local workstation to clone or pull the new feature branch from the central Git repository. They then switch to the feature branch to implement their changes.\nIDEs supported by IBM allow developers to perform a Dependency Based Build (DBB) User Build to quickly gather feedback on the implemented changes. This feature is expected to be used before the changes are committed and pushed to the central Git server, where a pipeline can process changes automatically. Developers regularly commit and push their changes to their feature branch on the central Git provider.\n\n\nUser Build is a capability provided by IBM-supported IDEs that uploads the modified source code and its dependencies from the local, checked-out Git working tree on the developer’s workstation to a personal directory on z/OS UNIX System Services, and then invokes the build framework to execute the compile and link-edit steps. This capability is available in the following IDEs:\n\nIBM Developer for z/OS\nMicrosoft Visual Studio Code (VS Code) with the IBM Z Open Editor extension\nIBM Wazi for Dev Spaces\n\nThe developer configures the User Build process to point to the central build framework implementation, such as zAppBuild, provided by the Mainframe DevOps Team. The build option --userBuild is passed to the build framework along with the reference to the file the developer would like to build.\nBecause the operation is performed with the credentials of the currently logged-in user, it is recommended for each developer to reuse the high-level qualifier (--hlq) of their personal datasets. It is the developer’s responsibility to regularly clean up the mainframe datasets and sandbox directories on z/OS UNIX System Services that are used for User Build. Automated cleanup of the files can be established based on a defined naming convention for datasets or with a specific storage management policy.\nUser Build is a convenient way to compile and link-edit source code without committing the changes into the Git version control system. Therefore, build outputs of user builds are not assumed to be installed into a runtime environment. To be able to perform simple and rudimentary tests on User Build-generated outputs, the developer should modify the test JCLs to point to the personal libraries used in user builds.\nAlternatively, the setup of a pre-concatenated runtime library can be implemented to perform more tests in the context of a (shared) test runtime environment. A dedicated pre-concatenated library in the runtime system (for example, batch, IMS and CICS) into which the developers can write allows a separation of the modules produced by user builds, and enables regular cleanup of these intermediate versions that are not yet registered in the central Git provider or as a build output in the artifact repository.\nExternal dependencies to other components, such as include files (for example, copybooks or object decks) that are not managed within the application repository but are required for building the application, can either be pulled in via a dataset concatenation or by the usage of Git submodules, depending on the repository organization.\n\n\n\nIt is a common practice to use a feature branch pipeline that builds the codebase of a feature branch and runs automated code inspections using the IDz Code Review feature and/or updates the static code analysis repository such as in IBM Wazi Analyze.\nThis pipeline expands the scope of the build past that of the user build and makes sure all changed and impacted programs are included in the list of artifacts to be produced by leveraging the --impactBuild option of zAppBuild. The developer must make sure to have pushed the feature branch with all their committed changes from their local clone of the repository to the central Git provider so that those changes are available to the feature branch pipeline process.\nThe pipeline configuration requires processing logic to compute a dedicated high-level qualifier to guarantee that build datasets are exclusively used for the provided branch. The computed value is passed into the build command via the --hlq parameter. zAppBuild allocates the datasets automatically.\nThe following screen capture shows the stages included a sample pipeline build for a feature branch.\n\n\n\nScreen capture of pipeline steps for the feature branch\n\n\nThe build leverages the dependency metadata managed by IBM Dependency Based Build via DBB collections, which are consumed by the build framework, zAppBuild. At the first execution of the build process for feature branches, zAppBuild will duplicate this metadata by cloning the related collections for efficiency purposes1. This cloning phase ensures the accuracy of the dependency information for this pipeline build. To be able to clone the collection, zAppBuild needs to understand which collection contains the most accurate information and should be duplicated. As collection names are derived from the name of the branch, it is easy to identify which collection should be cloned. In the zAppBuild configuration, the originating collection reference is defined via the mainBuildBranch2 property.\nDepending on the branching model workflow being used, the mainBuildBranch property might need to be overridden for the feature branch pipeline:\n\nIn the default development process working towards the next planned release based on the head of the main branch, the default configuration for the mainBuildBranch is accurate and does not need to be overridden.\nWhen implementing a fix in context of a release maintenance branch, the mainBuildBranch must be set to the name of the release maintenance branch to correctly clone the dependency information.\nWhen implementing changes on a feature branch in an epic branch context, the mainBuildBranch property must be set to the name of the epic branch.\n\nInstead of manipulating the property file that defines the mainBuildBranch setting and is part of the repository, the pipeline automation can compute the correct setting and pass the overriding property via the override command-line option of zAppBuild.\n\n\n\nToday’s mainframe development workflows can allow developers to install their changes into controlled test environments before these changes get assigned into a release, for instance when the developer would like to prototype/pilot a new feature. On lower environments, there might be multiple CICS regions that developers can use, which provide a level of isolation from other ongoing development work. The pipeline process can be extended to provide a similar functionality as an optional step for the developer.\nThis strategy is supported by feature branch packaging and deployment of a preliminary package. It is implemented as a dedicated pipeline that developers request on demand for their feature branch. The pipeline performs the following actions:\n\nBuild all the changes of the feature branch that were implemented, including their impacts as outlined in Basic Build Pipeline: Build and Test stage, using the commit point at which the feature branch was branched off as the baseline reference for calculating the changes.\nPackage the generated build outputs as outlined in Release Pipeline: Packaging stage.\n\nThe deployment process must ensure that these preliminary packages cannot be deployed into any production environment.\n\n\n\nDiagram showing optional feature branch packaging and deployment of a preliminary package\n\n\nOften, these controlled development test environments are used as shared test environments for multiple application teams. To use the same runtime environment, such as a CICS region, for both prototyping and for testing integrated changes, we recommend separating the preliminary (feature) packages from the planned release packages by separating these types into different libraries. The package for the prototyping workflow is deployed via its dedicated deployment environment model, illustrated in the above diagram as DEV-1-FEATURE-TEST.\nBecause preliminary packages are intended to be short-lived and temporary, they can be deployed to a library via the deployment automation process to a pre-concatenated library. Housekeeping strategies must be established to ensure that either automation routines or developers are cleaning up the preliminary packages when the testing is done.\nThis strategy should be designed with the infrastructure engineering team to prepare the test environments to support this workflow.\n\n\n\nA housekeeping strategy should be implemented when the feature branch is no longer needed and therefore removed from the central Git provider. Successful merging adds commits from one branch to the head of another. Once complete, the branch the commits were merged from can be safely deleted. (Keeping old branches can cause confusion and does not contribute to the traceability of the history.) This housekeeping strategy should include the cleanup of the DBB collections, the build workspace on z/OS UNIX System Services, and the build datasets.\nSpecific scripts can be integrated into the pipeline to delete collections and build groups, or remove unnecessary build datasets. When leveraging GitLab CI/CD as the pipeline orchestrator, the use of GitLab environments helps to automate these steps when a branch is deleted. An implementation sample is provided via the published technical document Integrating IBM z/OS Platform in CI/CD Pipelines with Gitlab. Generally, webhooks and other extensions of the pipeline orchestrator can be leveraged to perform these cleanup activities when a branch is deleted.\n\n\n\n\n\n\n\nBranching diagram with build pipeline on the main branch\n\n\nIt is common practice to build every time the head of the main, epic, or release branch is modified.\nWhen a feature branch is merged into a shared integration branch, a new pipeline is kicked off to build the merged changes in the context of the configuration of the integration branch.\nAdditional steps such as automated code reviews or updates of application discovery repositories can be included in the pipeline process, as shown in the sample pipeline setup in the following screen capture.\n\n\n\nScreen capture of a build pipeline for the main branch\n\n\n\n\nThe purpose of the Build and Test stage of the pipeline for an integration branch is to ensure that the branch can be built and then tested together. It might happen that some features have indirect dependencies on other features planned for the same deliverable. This early point of integration along with the impact build capability of the zAppBuild build framework ensures consistency and transparency for the upcoming deliverable.\nConceptually speaking, the build step of a CI/CD pipeline decouples building from deploying. This is important to ensure that only outputs from successful builds are installed into the test environment, rather than directing the build framework to update the libraries of the test environment directly.\nThe Build and Test stage of the pipeline for the integration branch builds all the incorporated changes that have so far been merged for the deliverable. To identify the list of changes contributing to the next planned release, the release maintenance, or the epic, the build step of the pipeline leverages the --baselineRef option of zAppBuild for incremental builds, which is used to specify a baseline hash or point in the commit history for calculating the list of changes. Using this approach of incremental builds avoids unnecessarily building parts of the application that are unaffected by any of the changes in the commits to the base branch since the last release.\nAdditionally, the pipeline configuration requires a dedicated high-level qualifier to guarantee that build data sets are exclusively used for the provided branch. The value is passed to the zAppBuild command via the --hlq parameter.\nThe option --baselineRef is a sub-parameter of the --impactBuild option in zAppBuild, and sets the base Git hash upon which the git diff command calculates changes for the repository3.\nIn the default workflow with main as the base branch, the baseline reference is defined by the commit hash (or the Git tag) of the previous release (that is, the release currently in production). In the following diagram, the blue dotted line shows the changes calculated from the baseline to the point at which the feature_2 branch is merged in.\n\n\n\nDiagram showing the calculation of changes for the next planned release\n\n\nFor the hotfix workflow, the hotfixes are planned to be implemented from a release maintenance branch whose baseline reference is the commit (or Git tag) that represents the state of the repository for the release. This is also the commit from which the respective release maintenance branch was created, as depicted in the below diagram.\n\n\n\nDiagram showing the calculation of changes in a release fix workflow\n\n\nFor the epic branch workflow, the baseline reference for the build pipeline is the commit (or Release tag) from which the epic branch was created, also referred to as the fork point.\n\n\n\nIn this phase of the development lifecycle for the default workflow implementing and delivering changes for the next planned release, the build typically operates with the compile options to enable testing and debugging of programs. As most organizations restrict the deployment to the production environments with optimized code only, these build artifacts can be seen as temporary and only for initial testing and debugging purposes.\n\nThere are two options to deploy the generated artifacts to the shared development test system - represented by the blue DEV-TEST shape in the above figure.\n(Recommended) Option A: Extend the pipeline with a packaging stage and a deployment stage to create a preliminary package similar to Release Pipeline: Packaging stage. It is traditionally the responsibility of the deployment solution to install the preliminary package into different environments. Doing so in this phase of the workflow will give the necessary traceability to understand which versions are installed in the development and test environment.\nOption B: Use a post-build script to copy the output artifacts from the build libraries to the associated target runtime libraries and manually run the necessary activation steps such as a Db2 bind process or an online refresh. However, even given the temporary nature of the outputs created by this build, this circumvents the formal packaging and deployment process. The major drawback of this approach is a lack of traceability and understanding of what runs on the development and test environment.\nThis step of the build pipeline is also applicable for pipelines for the epic or the release maintenance branches.\n\n\n\nAn optional Analyze stage after building the most current state of the main branch can include steps to perform automated code inspections using the IDz Code Review feature and/or to update the static code analysis repository such as in IBM Wazi Analyze.\nSubmitting a Sonarqube scan at this point of the workflow can also help the development team to keep an eye on the maintainability and serviceability of the application.\n\n\n\n\n\n\n\nDiagram showing the Release Pipeline building a release candidate package for deployment in test environments\n\n\nThe Release Pipeline is leveraged by the development team when they want to create a release candidate package that can be deployed to controlled test environments. The development team manually requests the pipeline to run. The pipeline is not expected to be used for every merge into the main branch.\nThe Release Pipeline differs from the previously-discussed pipelines and includes additional steps: after the stages of building and code scans have successfully completed, the pipeline packages all the incorporated changes of all merged features for this deliverable to create a package.\nThe package can be an intermediate release candidate version that can already be tested in the managed test environments, as outlined in the high-level workflows. When the development team has implemented all the tasks planned for the iteration, this same pipeline is used to produce the package that will be deployed to production.\nThe following diagram outlines the steps of a GitLab pipeline for the Build, Packaging, and Deploy stages.\nThe Deploy stage can only be present in the pipeline for the default workflow (with main) when delivering changes with the next planned release, because the pipeline is unaware of the assigned environments for the epic and release maintenance workflows.\n\n\n\nBuild, package and deploy pipeline\n\n\n\n\nSimilar to the build pipeline outlined in Basic Build Pipeline: Build and Test stage, the Build stage of the Release Pipeline builds all the incorporated changes of all merged features. To identify the list of changes contributing to the deliverable, the Build stage of the pipeline leverages the --baselineRef option of zAppBuild to detect all contributed changes based on rules outlined in Basic Build Pipeline: Build and Test stage.\nFor the main and the release maintenance workflows, this build performs the compilation with the compiler options to produce executables optimized for performance. The pipeline must leverage its dedicated set of PDS libraries to not interfere with the Basic Build Pipeline.\nFor the epic branch workflow, the build can occur with test options, as the package is only deployed into the initiative’s test environment and will be rebuilt when the epic branch is integrated into the main branch.\n\n\n\nThe Packaging stage runs after the Build and Test stage, and creates a package of the generated build outputs (for example, load modules, DBRMs, and JCLs). This package includes the build outputs of all the contributed changes (including the files impacted by the changes) for the deliverable. It represents a release candidate that can be deployed into the various test environments along the existing staging hierarchy. As outlined in the high-level workflows, this can even happen when only a subset of the features for the deliverable is implemented.\nThe name or associated metadata of the package allows the development team to relate the package to the development workflow. Based on naming conventions of the package, different rules need to apply to its lifecycle. Some examples of this are shown below:\n\nUse rel-2.1.0_RC01 for a release candidate package for the next planned release.\nThe package name represents the name of the next planned release. This package can make it to production, as it contains build outputs produced with options for optimization.\nUse rel-2.0.1-patch_RC01 for an urgent fix package of the current production version.\nThis package is allowed to bypass any concurrent development activities and can take shortcuts in the route to production. For example, if it can only be tested on the QA-TEST environment, the developers can bypass lower test environments based on an “emergency” flag of the deployment package.\nUse epic1-prelim_pkg01 for a preliminary package of a long-running epic branch workflow.\nThis package can only be deployed to the assigned test environments available to the initiative, and cannot be deployed to production.\n\nThe Packaging stage not only creates the binaries package, but it also carries information about the source code, such as the Git commit and additional links (including references to the pipeline job), which are helpful for understanding the context of the creation of the package.\nThe DBB community repository contains two sample scripts that implement the Packaging stage. If IBM UrbanCode Deploy (UCD) is used as the deployment solution, the CreateUCDComponentVersion script can be leveraged to create an IBM UrbanCode Deploy component version. Alternatively, if a scripted deployment is being set up, the PackageBuildOutputs script can be used instead to store artifacts in an enterprise binary artifact repository leveraged by IBM Wazi Deploy.\nBoth sample scripts leverage data from the DBB build report to extract and retain the metadata, allowing traceability between the build and deployment activities as outlined above.\n\n\n\nThe deployment process of a release package for the default workflow with main can either be triggered from the CI/CD pipeline or driven through the user interface of the deployment manager. The implementation can vary based on the capabilities offered by the CI/CD orchestrator and the deployment solution. Deployment manager options for z/OS include IBM UrbanCode Deploy (UCD) and IBM Wazi Deploy.\nIBM UrbanCode Deploy provides a rich web-based interface, powerful REST APIs, and a command-line interface. Typically, the pipeline execution requests the deployment of the application package into the defined test environments automatically, after successful completion of the preceding Build and Packaging stages. These requests are performed through the REST APIs provided by UCD. However, if the application team prefers to set up manual triggers for the deployments to the specific environments, this can be performed through UCD’s web interface. In that scenario, the pipeline is primarily used for continuous integration and packaging.\nThe DBB community repository provides a sample DeployUCDComponentVersion script that can be included in a pipeline process to request a UCD application deployment leveraging UCD’s REST APIs.\nIBM Wazi Deploy is a deployment manager for z/OS artifacts and comes with a command-line interface that can be easily invoked from a pipeline orchestrator and does not require a wrapper script. After retrieving the package to deploy from the artifact repository, the wazideploy-generate step generates the deployment instructions (also known as the deployment plan) for the artifacts of the package. This plan is then passed to the wazideploy-deploy step, which installs the contents of the package into the specified runtime environment4.\nThe following screen capture shows the Deploy stage of a sample Release Pipeline.\n\n\n\nScreen capture of the Deploy stage of the Release Pipeline\n\n\nImplementation details of the Deploy stage can vary based on the pipeline orchestrator being used. In a GitLab CI/CD implementation, a pipeline can stay on hold and wait for user input. This allows the pipeline to automatically trigger the deployment of the application package into the first configured environment, and lets the application team decide when to deploy to the next environment through a manual step (for instance, deployment to the Acceptance environment).\nWith Jenkins as the CI/CD orchestrator, it is not common to keep a pipeline in progress over a long time. In this case, the pipeline engineering team might consider the approach of requesting the deployments through the user interface of the deployment manager, or alternatively, they can design and set up a deployment pipeline in Jenkins that can combine the deployment with any automated tests or other automation tasks.\n\n\n\n\nWhen the release candidate package has passed all quality gates and received all the necessary approvals, it is ready to be deployed to the production environment.\nThe release manager takes care of this step of the lifecycle and will leverage the user interface of the deployment manager, such as UCD’s browser-based interface. In the case of a deployment manager solution with a command-line interface such as Wazi Deploy, the user interface of the pipeline orchestrator is used by the release manager to drive the deployment to production. A deployment pipeline definition needs to be configured to roll out the package.\nDeploying to production consists of two tasks:\n\nInvoke the deployment to the production runtime environment through either the deployment manager interface or a deployment pipeline definition.\nTag the commit in the Git server by assigning a Git tag to the commit that was used to build the release package.\n\nMost Git providers allow for the creation of a release to provide a summary of the changes, as well as additional documentation. GitLab and GitHub offer REST APIs to create a new tag/release. This action should be automated as part of the deployment to production.\nAs an example of using Git tags, zAppBuild also declares releases to identify stable versions.\n\n\n\nThis page provides guidance for implementing a Git branching model for mainframe development with IBM Dependency Based Build and zAppBuild.\nThe CI/CD pipeline configurations that were outlined at various stages can be adjusted depending on the application team’s existing and desired development processes and philosophy. Factors that might impact the design of the pipelines and workflow include test strategies, the number of test environments, and potential testing limitations.\nWhen designing a CI/CD pipeline, assessment of current and future requirements in the software delivery lifecycle is key. As CI/CD technologies continue to evolve and automated testing using provisioned test environments becomes more common in mainframe application development teams, the outlined branching strategy can also evolve to maximize the benefits from these advances.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Supporting pipeline design and implementation"
    ]
  },
  {
    "objectID": "branching-model-supporting-pipeline.html#configurations-to-support-working-with-feature-branches",
    "href": "branching-model-supporting-pipeline.html#configurations-to-support-working-with-feature-branches",
    "title": "Supporting pipeline design and implementation",
    "section": "",
    "text": "Branching diagram of a feature branch\n\n\nWhen developers start working on a new task, they will first create a feature branch. Feature branches are created off the latest code state of the source configuration, whether that is the main branch or an epic or release maintenance branch.\nIf the feature branch was created on the central Git repository, the developers can use the integrated development environment (IDE), a terminal, or another Git interface on their local workstation to clone or pull the new feature branch from the central Git repository. They then switch to the feature branch to implement their changes.\nIDEs supported by IBM allow developers to perform a Dependency Based Build (DBB) User Build to quickly gather feedback on the implemented changes. This feature is expected to be used before the changes are committed and pushed to the central Git server, where a pipeline can process changes automatically. Developers regularly commit and push their changes to their feature branch on the central Git provider.\n\n\nUser Build is a capability provided by IBM-supported IDEs that uploads the modified source code and its dependencies from the local, checked-out Git working tree on the developer’s workstation to a personal directory on z/OS UNIX System Services, and then invokes the build framework to execute the compile and link-edit steps. This capability is available in the following IDEs:\n\nIBM Developer for z/OS\nMicrosoft Visual Studio Code (VS Code) with the IBM Z Open Editor extension\nIBM Wazi for Dev Spaces\n\nThe developer configures the User Build process to point to the central build framework implementation, such as zAppBuild, provided by the Mainframe DevOps Team. The build option --userBuild is passed to the build framework along with the reference to the file the developer would like to build.\nBecause the operation is performed with the credentials of the currently logged-in user, it is recommended for each developer to reuse the high-level qualifier (--hlq) of their personal datasets. It is the developer’s responsibility to regularly clean up the mainframe datasets and sandbox directories on z/OS UNIX System Services that are used for User Build. Automated cleanup of the files can be established based on a defined naming convention for datasets or with a specific storage management policy.\nUser Build is a convenient way to compile and link-edit source code without committing the changes into the Git version control system. Therefore, build outputs of user builds are not assumed to be installed into a runtime environment. To be able to perform simple and rudimentary tests on User Build-generated outputs, the developer should modify the test JCLs to point to the personal libraries used in user builds.\nAlternatively, the setup of a pre-concatenated runtime library can be implemented to perform more tests in the context of a (shared) test runtime environment. A dedicated pre-concatenated library in the runtime system (for example, batch, IMS and CICS) into which the developers can write allows a separation of the modules produced by user builds, and enables regular cleanup of these intermediate versions that are not yet registered in the central Git provider or as a build output in the artifact repository.\nExternal dependencies to other components, such as include files (for example, copybooks or object decks) that are not managed within the application repository but are required for building the application, can either be pulled in via a dataset concatenation or by the usage of Git submodules, depending on the repository organization.\n\n\n\nIt is a common practice to use a feature branch pipeline that builds the codebase of a feature branch and runs automated code inspections using the IDz Code Review feature and/or updates the static code analysis repository such as in IBM Wazi Analyze.\nThis pipeline expands the scope of the build past that of the user build and makes sure all changed and impacted programs are included in the list of artifacts to be produced by leveraging the --impactBuild option of zAppBuild. The developer must make sure to have pushed the feature branch with all their committed changes from their local clone of the repository to the central Git provider so that those changes are available to the feature branch pipeline process.\nThe pipeline configuration requires processing logic to compute a dedicated high-level qualifier to guarantee that build datasets are exclusively used for the provided branch. The computed value is passed into the build command via the --hlq parameter. zAppBuild allocates the datasets automatically.\nThe following screen capture shows the stages included a sample pipeline build for a feature branch.\n\n\n\nScreen capture of pipeline steps for the feature branch\n\n\nThe build leverages the dependency metadata managed by IBM Dependency Based Build via DBB collections, which are consumed by the build framework, zAppBuild. At the first execution of the build process for feature branches, zAppBuild will duplicate this metadata by cloning the related collections for efficiency purposes1. This cloning phase ensures the accuracy of the dependency information for this pipeline build. To be able to clone the collection, zAppBuild needs to understand which collection contains the most accurate information and should be duplicated. As collection names are derived from the name of the branch, it is easy to identify which collection should be cloned. In the zAppBuild configuration, the originating collection reference is defined via the mainBuildBranch2 property.\nDepending on the branching model workflow being used, the mainBuildBranch property might need to be overridden for the feature branch pipeline:\n\nIn the default development process working towards the next planned release based on the head of the main branch, the default configuration for the mainBuildBranch is accurate and does not need to be overridden.\nWhen implementing a fix in context of a release maintenance branch, the mainBuildBranch must be set to the name of the release maintenance branch to correctly clone the dependency information.\nWhen implementing changes on a feature branch in an epic branch context, the mainBuildBranch property must be set to the name of the epic branch.\n\nInstead of manipulating the property file that defines the mainBuildBranch setting and is part of the repository, the pipeline automation can compute the correct setting and pass the overriding property via the override command-line option of zAppBuild.\n\n\n\nToday’s mainframe development workflows can allow developers to install their changes into controlled test environments before these changes get assigned into a release, for instance when the developer would like to prototype/pilot a new feature. On lower environments, there might be multiple CICS regions that developers can use, which provide a level of isolation from other ongoing development work. The pipeline process can be extended to provide a similar functionality as an optional step for the developer.\nThis strategy is supported by feature branch packaging and deployment of a preliminary package. It is implemented as a dedicated pipeline that developers request on demand for their feature branch. The pipeline performs the following actions:\n\nBuild all the changes of the feature branch that were implemented, including their impacts as outlined in Basic Build Pipeline: Build and Test stage, using the commit point at which the feature branch was branched off as the baseline reference for calculating the changes.\nPackage the generated build outputs as outlined in Release Pipeline: Packaging stage.\n\nThe deployment process must ensure that these preliminary packages cannot be deployed into any production environment.\n\n\n\nDiagram showing optional feature branch packaging and deployment of a preliminary package\n\n\nOften, these controlled development test environments are used as shared test environments for multiple application teams. To use the same runtime environment, such as a CICS region, for both prototyping and for testing integrated changes, we recommend separating the preliminary (feature) packages from the planned release packages by separating these types into different libraries. The package for the prototyping workflow is deployed via its dedicated deployment environment model, illustrated in the above diagram as DEV-1-FEATURE-TEST.\nBecause preliminary packages are intended to be short-lived and temporary, they can be deployed to a library via the deployment automation process to a pre-concatenated library. Housekeeping strategies must be established to ensure that either automation routines or developers are cleaning up the preliminary packages when the testing is done.\nThis strategy should be designed with the infrastructure engineering team to prepare the test environments to support this workflow.\n\n\n\nA housekeeping strategy should be implemented when the feature branch is no longer needed and therefore removed from the central Git provider. Successful merging adds commits from one branch to the head of another. Once complete, the branch the commits were merged from can be safely deleted. (Keeping old branches can cause confusion and does not contribute to the traceability of the history.) This housekeeping strategy should include the cleanup of the DBB collections, the build workspace on z/OS UNIX System Services, and the build datasets.\nSpecific scripts can be integrated into the pipeline to delete collections and build groups, or remove unnecessary build datasets. When leveraging GitLab CI/CD as the pipeline orchestrator, the use of GitLab environments helps to automate these steps when a branch is deleted. An implementation sample is provided via the published technical document Integrating IBM z/OS Platform in CI/CD Pipelines with Gitlab. Generally, webhooks and other extensions of the pipeline orchestrator can be leveraged to perform these cleanup activities when a branch is deleted.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Supporting pipeline design and implementation"
    ]
  },
  {
    "objectID": "branching-model-supporting-pipeline.html#the-basic-build-pipeline-for-main-epic-and-release-branches",
    "href": "branching-model-supporting-pipeline.html#the-basic-build-pipeline-for-main-epic-and-release-branches",
    "title": "Supporting pipeline design and implementation",
    "section": "",
    "text": "Branching diagram with build pipeline on the main branch\n\n\nIt is common practice to build every time the head of the main, epic, or release branch is modified.\nWhen a feature branch is merged into a shared integration branch, a new pipeline is kicked off to build the merged changes in the context of the configuration of the integration branch.\nAdditional steps such as automated code reviews or updates of application discovery repositories can be included in the pipeline process, as shown in the sample pipeline setup in the following screen capture.\n\n\n\nScreen capture of a build pipeline for the main branch\n\n\n\n\nThe purpose of the Build and Test stage of the pipeline for an integration branch is to ensure that the branch can be built and then tested together. It might happen that some features have indirect dependencies on other features planned for the same deliverable. This early point of integration along with the impact build capability of the zAppBuild build framework ensures consistency and transparency for the upcoming deliverable.\nConceptually speaking, the build step of a CI/CD pipeline decouples building from deploying. This is important to ensure that only outputs from successful builds are installed into the test environment, rather than directing the build framework to update the libraries of the test environment directly.\nThe Build and Test stage of the pipeline for the integration branch builds all the incorporated changes that have so far been merged for the deliverable. To identify the list of changes contributing to the next planned release, the release maintenance, or the epic, the build step of the pipeline leverages the --baselineRef option of zAppBuild for incremental builds, which is used to specify a baseline hash or point in the commit history for calculating the list of changes. Using this approach of incremental builds avoids unnecessarily building parts of the application that are unaffected by any of the changes in the commits to the base branch since the last release.\nAdditionally, the pipeline configuration requires a dedicated high-level qualifier to guarantee that build data sets are exclusively used for the provided branch. The value is passed to the zAppBuild command via the --hlq parameter.\nThe option --baselineRef is a sub-parameter of the --impactBuild option in zAppBuild, and sets the base Git hash upon which the git diff command calculates changes for the repository3.\nIn the default workflow with main as the base branch, the baseline reference is defined by the commit hash (or the Git tag) of the previous release (that is, the release currently in production). In the following diagram, the blue dotted line shows the changes calculated from the baseline to the point at which the feature_2 branch is merged in.\n\n\n\nDiagram showing the calculation of changes for the next planned release\n\n\nFor the hotfix workflow, the hotfixes are planned to be implemented from a release maintenance branch whose baseline reference is the commit (or Git tag) that represents the state of the repository for the release. This is also the commit from which the respective release maintenance branch was created, as depicted in the below diagram.\n\n\n\nDiagram showing the calculation of changes in a release fix workflow\n\n\nFor the epic branch workflow, the baseline reference for the build pipeline is the commit (or Release tag) from which the epic branch was created, also referred to as the fork point.\n\n\n\nIn this phase of the development lifecycle for the default workflow implementing and delivering changes for the next planned release, the build typically operates with the compile options to enable testing and debugging of programs. As most organizations restrict the deployment to the production environments with optimized code only, these build artifacts can be seen as temporary and only for initial testing and debugging purposes.\n\nThere are two options to deploy the generated artifacts to the shared development test system - represented by the blue DEV-TEST shape in the above figure.\n(Recommended) Option A: Extend the pipeline with a packaging stage and a deployment stage to create a preliminary package similar to Release Pipeline: Packaging stage. It is traditionally the responsibility of the deployment solution to install the preliminary package into different environments. Doing so in this phase of the workflow will give the necessary traceability to understand which versions are installed in the development and test environment.\nOption B: Use a post-build script to copy the output artifacts from the build libraries to the associated target runtime libraries and manually run the necessary activation steps such as a Db2 bind process or an online refresh. However, even given the temporary nature of the outputs created by this build, this circumvents the formal packaging and deployment process. The major drawback of this approach is a lack of traceability and understanding of what runs on the development and test environment.\nThis step of the build pipeline is also applicable for pipelines for the epic or the release maintenance branches.\n\n\n\nAn optional Analyze stage after building the most current state of the main branch can include steps to perform automated code inspections using the IDz Code Review feature and/or to update the static code analysis repository such as in IBM Wazi Analyze.\nSubmitting a Sonarqube scan at this point of the workflow can also help the development team to keep an eye on the maintainability and serviceability of the application.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Supporting pipeline design and implementation"
    ]
  },
  {
    "objectID": "branching-model-supporting-pipeline.html#the-release-pipeline-with-build-packaging-and-deploy-stages",
    "href": "branching-model-supporting-pipeline.html#the-release-pipeline-with-build-packaging-and-deploy-stages",
    "title": "Supporting pipeline design and implementation",
    "section": "",
    "text": "Diagram showing the Release Pipeline building a release candidate package for deployment in test environments\n\n\nThe Release Pipeline is leveraged by the development team when they want to create a release candidate package that can be deployed to controlled test environments. The development team manually requests the pipeline to run. The pipeline is not expected to be used for every merge into the main branch.\nThe Release Pipeline differs from the previously-discussed pipelines and includes additional steps: after the stages of building and code scans have successfully completed, the pipeline packages all the incorporated changes of all merged features for this deliverable to create a package.\nThe package can be an intermediate release candidate version that can already be tested in the managed test environments, as outlined in the high-level workflows. When the development team has implemented all the tasks planned for the iteration, this same pipeline is used to produce the package that will be deployed to production.\nThe following diagram outlines the steps of a GitLab pipeline for the Build, Packaging, and Deploy stages.\nThe Deploy stage can only be present in the pipeline for the default workflow (with main) when delivering changes with the next planned release, because the pipeline is unaware of the assigned environments for the epic and release maintenance workflows.\n\n\n\nBuild, package and deploy pipeline\n\n\n\n\nSimilar to the build pipeline outlined in Basic Build Pipeline: Build and Test stage, the Build stage of the Release Pipeline builds all the incorporated changes of all merged features. To identify the list of changes contributing to the deliverable, the Build stage of the pipeline leverages the --baselineRef option of zAppBuild to detect all contributed changes based on rules outlined in Basic Build Pipeline: Build and Test stage.\nFor the main and the release maintenance workflows, this build performs the compilation with the compiler options to produce executables optimized for performance. The pipeline must leverage its dedicated set of PDS libraries to not interfere with the Basic Build Pipeline.\nFor the epic branch workflow, the build can occur with test options, as the package is only deployed into the initiative’s test environment and will be rebuilt when the epic branch is integrated into the main branch.\n\n\n\nThe Packaging stage runs after the Build and Test stage, and creates a package of the generated build outputs (for example, load modules, DBRMs, and JCLs). This package includes the build outputs of all the contributed changes (including the files impacted by the changes) for the deliverable. It represents a release candidate that can be deployed into the various test environments along the existing staging hierarchy. As outlined in the high-level workflows, this can even happen when only a subset of the features for the deliverable is implemented.\nThe name or associated metadata of the package allows the development team to relate the package to the development workflow. Based on naming conventions of the package, different rules need to apply to its lifecycle. Some examples of this are shown below:\n\nUse rel-2.1.0_RC01 for a release candidate package for the next planned release.\nThe package name represents the name of the next planned release. This package can make it to production, as it contains build outputs produced with options for optimization.\nUse rel-2.0.1-patch_RC01 for an urgent fix package of the current production version.\nThis package is allowed to bypass any concurrent development activities and can take shortcuts in the route to production. For example, if it can only be tested on the QA-TEST environment, the developers can bypass lower test environments based on an “emergency” flag of the deployment package.\nUse epic1-prelim_pkg01 for a preliminary package of a long-running epic branch workflow.\nThis package can only be deployed to the assigned test environments available to the initiative, and cannot be deployed to production.\n\nThe Packaging stage not only creates the binaries package, but it also carries information about the source code, such as the Git commit and additional links (including references to the pipeline job), which are helpful for understanding the context of the creation of the package.\nThe DBB community repository contains two sample scripts that implement the Packaging stage. If IBM UrbanCode Deploy (UCD) is used as the deployment solution, the CreateUCDComponentVersion script can be leveraged to create an IBM UrbanCode Deploy component version. Alternatively, if a scripted deployment is being set up, the PackageBuildOutputs script can be used instead to store artifacts in an enterprise binary artifact repository leveraged by IBM Wazi Deploy.\nBoth sample scripts leverage data from the DBB build report to extract and retain the metadata, allowing traceability between the build and deployment activities as outlined above.\n\n\n\nThe deployment process of a release package for the default workflow with main can either be triggered from the CI/CD pipeline or driven through the user interface of the deployment manager. The implementation can vary based on the capabilities offered by the CI/CD orchestrator and the deployment solution. Deployment manager options for z/OS include IBM UrbanCode Deploy (UCD) and IBM Wazi Deploy.\nIBM UrbanCode Deploy provides a rich web-based interface, powerful REST APIs, and a command-line interface. Typically, the pipeline execution requests the deployment of the application package into the defined test environments automatically, after successful completion of the preceding Build and Packaging stages. These requests are performed through the REST APIs provided by UCD. However, if the application team prefers to set up manual triggers for the deployments to the specific environments, this can be performed through UCD’s web interface. In that scenario, the pipeline is primarily used for continuous integration and packaging.\nThe DBB community repository provides a sample DeployUCDComponentVersion script that can be included in a pipeline process to request a UCD application deployment leveraging UCD’s REST APIs.\nIBM Wazi Deploy is a deployment manager for z/OS artifacts and comes with a command-line interface that can be easily invoked from a pipeline orchestrator and does not require a wrapper script. After retrieving the package to deploy from the artifact repository, the wazideploy-generate step generates the deployment instructions (also known as the deployment plan) for the artifacts of the package. This plan is then passed to the wazideploy-deploy step, which installs the contents of the package into the specified runtime environment4.\nThe following screen capture shows the Deploy stage of a sample Release Pipeline.\n\n\n\nScreen capture of the Deploy stage of the Release Pipeline\n\n\nImplementation details of the Deploy stage can vary based on the pipeline orchestrator being used. In a GitLab CI/CD implementation, a pipeline can stay on hold and wait for user input. This allows the pipeline to automatically trigger the deployment of the application package into the first configured environment, and lets the application team decide when to deploy to the next environment through a manual step (for instance, deployment to the Acceptance environment).\nWith Jenkins as the CI/CD orchestrator, it is not common to keep a pipeline in progress over a long time. In this case, the pipeline engineering team might consider the approach of requesting the deployments through the user interface of the deployment manager, or alternatively, they can design and set up a deployment pipeline in Jenkins that can combine the deployment with any automated tests or other automation tasks.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Supporting pipeline design and implementation"
    ]
  },
  {
    "objectID": "branching-model-supporting-pipeline.html#deployment-to-production",
    "href": "branching-model-supporting-pipeline.html#deployment-to-production",
    "title": "Supporting pipeline design and implementation",
    "section": "",
    "text": "When the release candidate package has passed all quality gates and received all the necessary approvals, it is ready to be deployed to the production environment.\nThe release manager takes care of this step of the lifecycle and will leverage the user interface of the deployment manager, such as UCD’s browser-based interface. In the case of a deployment manager solution with a command-line interface such as Wazi Deploy, the user interface of the pipeline orchestrator is used by the release manager to drive the deployment to production. A deployment pipeline definition needs to be configured to roll out the package.\nDeploying to production consists of two tasks:\n\nInvoke the deployment to the production runtime environment through either the deployment manager interface or a deployment pipeline definition.\nTag the commit in the Git server by assigning a Git tag to the commit that was used to build the release package.\n\nMost Git providers allow for the creation of a release to provide a summary of the changes, as well as additional documentation. GitLab and GitHub offer REST APIs to create a new tag/release. This action should be automated as part of the deployment to production.\nAs an example of using Git tags, zAppBuild also declares releases to identify stable versions.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Supporting pipeline design and implementation"
    ]
  },
  {
    "objectID": "branching-model-supporting-pipeline.html#conclusion",
    "href": "branching-model-supporting-pipeline.html#conclusion",
    "title": "Supporting pipeline design and implementation",
    "section": "",
    "text": "This page provides guidance for implementing a Git branching model for mainframe development with IBM Dependency Based Build and zAppBuild.\nThe CI/CD pipeline configurations that were outlined at various stages can be adjusted depending on the application team’s existing and desired development processes and philosophy. Factors that might impact the design of the pipelines and workflow include test strategies, the number of test environments, and potential testing limitations.\nWhen designing a CI/CD pipeline, assessment of current and future requirements in the software delivery lifecycle is key. As CI/CD technologies continue to evolve and automated testing using provisioned test environments becomes more common in mainframe application development teams, the outlined branching strategy can also evolve to maximize the benefits from these advances.",
    "crumbs": [
      "Designing the CI/CD pipeline",
      "Git branching for mainframe development",
      "Supporting pipeline design and implementation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IBM Z DevOps Acceleration Program",
    "section": "",
    "text": "IBM strongly believes that DevOps transformations are anything but plug-and-play; each enterprise brings with it its own culture, IT infrastructure, and mission. Transformation requires an absolute paradigm shift in your organization. It can be tough, we get it. Perhaps many enterprises wish to begin their transformation today but find themselves always putting it off until tomorrow.\nWe wanted to create a better way of bridging the gap between the intention to move towards a DevOps model and the successful transformation of such a move. That’s why we’ve created the DevOps Acceleration Program (DAP) and associated documentation on this website. Our goal is to ensure we are providing prescriptive guidance to our customers on their respective DevOps journeys. The information has been structured in an easily consumable and searchable manner following each step of the journey, from learning about the solution to implementing it, simplifying the navigation of these materials.\n\n\n\n\n\n\n\n\n(Overview)[ibm-z-devops-solution.md]\n\n\n\n\n\nGetting Started\n\n\n\n\n\nComing soon… News and blogs"
  },
  {
    "objectID": "p1.html#agenda",
    "href": "p1.html#agenda",
    "title": "The workflow for  git-based z/OS DevOps",
    "section": "Agenda",
    "text": "Agenda\n\nAims and Assumptions\nChoosing a branching model and workflow\nStarting simple\nScaling up\nIntegration branches\nDelivering changes via a release\nHot-fix for production\nUsing an Epic branch"
  },
  {
    "objectID": "p1.html#scaling",
    "href": "p1.html#scaling",
    "title": "The workflow for  git-based z/OS DevOps",
    "section": "Scaling",
    "text": "Scaling\n\nThe workflow and branching scheme should both scale up and scale down.\n\nSmall teams with simple and infrequent changes will be able to easily understand, adopt, and have a good experience.\nLarge, busy teams with many concurrent activities will be able to plan, track, and execute with maximum agility using the same fundamental principles."
  },
  {
    "objectID": "p1.html#planning",
    "href": "p1.html#planning",
    "title": "The workflow for  git-based z/OS DevOps",
    "section": "Planning",
    "text": "Planning\n\nPlanning and design activities as well as code development aim to align to a regular release cadence.\nThere is no magic answer to managing large numbers of “in-flight” changes, so planning assumptions should aim as much as possible to complete changes quickly, ideally within one release cycle.\n\nDevOps/Agile practices typically encourage that, where possible, development teams should strive to break down larger changes into sets of smaller, incremental deliverables that can each be completed within an iteration. This reduces the number of “in-flight” changes, and allows the team to deliver value (end-to-end functionality) more quickly while still building towards a larger development goal.\n\nWe know it is sometimes unavoidable for work to take longer than one release cycle and we accommodate that as a variant of the base workflow."
  },
  {
    "objectID": "p1.html#starting-simple",
    "href": "p1.html#starting-simple",
    "title": "The workflow for  git-based z/OS DevOps",
    "section": "Starting Simple",
    "text": "Starting Simple\n\n\nEvery change starts in a branch"
  },
  {
    "objectID": "p1.html#starting-simple-1",
    "href": "p1.html#starting-simple-1",
    "title": "The workflow for  git-based z/OS DevOps",
    "section": "Starting Simple",
    "text": "Starting Simple\n\n\nEvery change starts in a branch\nThese branches merge to main\n\n\n\n\n\n\n\nCopyright © 2023 IBM Corporation"
  },
  {
    "objectID": "deployment-manager.html",
    "href": "deployment-manager.html",
    "title": "Deployment manager",
    "section": "",
    "text": "The deployment manager is responsible for understanding the execution environments and maintains an inventory of the environment’s deployed content. It is used to rollout application packages. For many runtimes, copying artifacts is not enough to actually make them executable. There are numerous installation steps to perform. A good example of this would be a CICS NEWCOPY/PHASE-IN, or, when DB2 is involved, a bind against the database of the environment.\n\n\n\nUCD\nWazi Deploy (Python or Ansible)\nAnsible z/OS modules\n\n\n\n\nThis page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Deployment manager"
    ]
  },
  {
    "objectID": "deployment-manager.html#common-options",
    "href": "deployment-manager.html#common-options",
    "title": "Deployment manager",
    "section": "",
    "text": "UCD\nWazi Deploy (Python or Ansible)\nAnsible z/OS modules",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Deployment manager"
    ]
  },
  {
    "objectID": "deployment-manager.html#resources",
    "href": "deployment-manager.html#resources",
    "title": "Deployment manager",
    "section": "",
    "text": "This page contains reformatted excerpts from Packaging and Deployment Strategies in an Open and Modern CI/CD Pipeline focusing on Mainframe Software Development.",
    "crumbs": [
      "Overview",
      "CI/CD for z/OS applications",
      "Deployment manager"
    ]
  }
]